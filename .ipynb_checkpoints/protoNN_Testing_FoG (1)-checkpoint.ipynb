{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwwtn5iEQJ1M"
   },
   "source": [
    "# ProtoNN in Tensorflow\n",
    "\n",
    "This is a simple notebook that illustrates the usage of Tensorflow implementation of ProtoNN. We are using the USPS dataset. Please refer to `fetch_usps.py` for more details on downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.223951Z",
     "start_time": "2018-08-15T13:06:09.303454Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dJBVr2b7QJ1R"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17328\\3165777642.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable_v2_behavior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(r\"D:\\programming\\practice\\research\\protoNN\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxqvfwWQtQ-s"
   },
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cT-KokQSQiS6"
   },
   "outputs": [],
   "source": [
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou1MfKhYtMdT"
   },
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDVo_0JiRSi9"
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAjSVSOFtFmm"
   },
   "source": [
    "# Model Trainer - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp5dEFiZR_sy"
   },
   "outputs": [],
   "source": [
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Z6ym4k_s9pS"
   },
   "source": [
    "# Model Graph - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRPFglKHSbu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEBMKewPQJ1c"
   },
   "source": [
    "# Obtain Data\n",
    "\n",
    "It is assumed that the Daphnet data has already been downloaded,preprocessed and set up in subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"C:\\Users\\Admin\\Desktop\\experiments\"\n",
    "windowLen = 'data'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"C:\\Users\\Admin\\Desktop\\experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data.npy'), np.load(DATA_DIR + '/ttest_data.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u6oX8eJQJ2N"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable `GAMMA`. If `GAMMA` is set to `None`, median heuristic will be used to estimate a good value of $\\gamma$ through the `helper.getGamma()` method. This method also returns the corresponding `W` and `B` matrices which should be used to initialize ProtoNN (as is done here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.279204Z",
     "start_time": "2018-08-15T13:06:10.272880Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UaduZ1vJQJ2P"
   },
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 0.8\n",
    "SPAR_Z = 0.8\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_EPOCHS = 300\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.307632Z",
     "start_time": "2018-08-15T13:06:10.280955Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1567154485603,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "teqlUPhLQJ2W",
    "outputId": "e7e7f7f2-9ddb-448b-9539-65a1a2dc1c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using median heuristic to estimate gamma.\n",
      "Gamma estimate is: 0.008066\n"
     ]
    }
   ],
   "source": [
    "W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJwO4MXatk9G"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.641991Z",
     "start_time": "2018-08-15T13:06:10.309353Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98358,
     "status": "ok",
     "timestamp": 1567154584840,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "MrKAP5_RQJ2b",
    "outputId": "2fb982af-47ae-4867-c5e5-b2ecc2b9dfc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Batch:   0 Loss: 4.09534 Accuracy: 0.00000\n",
      "Epoch:   1 Batch:   0 Loss: 6.19325 Accuracy: 0.00000\n",
      "Epoch:   2 Batch:   0 Loss: 2.39447 Accuracy: 0.00000\n",
      "Epoch:   3 Batch:   0 Loss: 1.50697 Accuracy: 0.00000\n",
      "Epoch:   4 Batch:   0 Loss: 1.07283 Accuracy: 0.00000\n",
      "Epoch:   5 Batch:   0 Loss: 0.90518 Accuracy: 0.00000\n",
      "Epoch:   6 Batch:   0 Loss: 0.83731 Accuracy: 0.00000\n",
      "Epoch:   7 Batch:   0 Loss: 0.78752 Accuracy: 0.06250\n",
      "Epoch:   8 Batch:   0 Loss: 0.74604 Accuracy: 0.18750\n",
      "Epoch:   9 Batch:   0 Loss: 0.71567 Accuracy: 0.40625\n",
      "Test Loss: 2.38225 Accuracy: 0.68021\n",
      "Epoch:  10 Batch:   0 Loss: 0.69319 Accuracy: 0.81250\n",
      "Epoch:  11 Batch:   0 Loss: 0.67203 Accuracy: 0.87500\n",
      "Epoch:  12 Batch:   0 Loss: 0.65746 Accuracy: 0.93750\n",
      "Epoch:  13 Batch:   0 Loss: 0.64073 Accuracy: 0.93750\n",
      "Epoch:  14 Batch:   0 Loss: 0.62586 Accuracy: 0.93750\n",
      "Epoch:  15 Batch:   0 Loss: 0.61133 Accuracy: 0.93750\n",
      "Epoch:  16 Batch:   0 Loss: 0.59735 Accuracy: 0.93750\n",
      "Epoch:  17 Batch:   0 Loss: 0.58330 Accuracy: 0.93750\n",
      "Epoch:  18 Batch:   0 Loss: 0.56563 Accuracy: 0.93750\n",
      "Epoch:  19 Batch:   0 Loss: 0.54577 Accuracy: 0.93750\n",
      "Test Loss: 2.58145 Accuracy: 0.71492\n",
      "Epoch:  20 Batch:   0 Loss: 0.52104 Accuracy: 0.93750\n",
      "Epoch:  21 Batch:   0 Loss: 0.49251 Accuracy: 0.93750\n",
      "Epoch:  22 Batch:   0 Loss: 0.45745 Accuracy: 0.93750\n",
      "Epoch:  23 Batch:   0 Loss: 0.42044 Accuracy: 1.00000\n",
      "Epoch:  24 Batch:   0 Loss: 0.38430 Accuracy: 1.00000\n",
      "Epoch:  25 Batch:   0 Loss: 0.34607 Accuracy: 1.00000\n",
      "Epoch:  26 Batch:   0 Loss: 0.31031 Accuracy: 1.00000\n",
      "Epoch:  27 Batch:   0 Loss: 0.27748 Accuracy: 1.00000\n",
      "Epoch:  28 Batch:   0 Loss: 0.24757 Accuracy: 1.00000\n",
      "Epoch:  29 Batch:   0 Loss: 0.22142 Accuracy: 1.00000\n",
      "Test Loss: 3.00213 Accuracy: 0.71585\n",
      "Epoch:  30 Batch:   0 Loss: 0.19806 Accuracy: 1.00000\n",
      "Epoch:  31 Batch:   0 Loss: 0.17726 Accuracy: 1.00000\n",
      "Epoch:  32 Batch:   0 Loss: 0.15879 Accuracy: 1.00000\n",
      "Epoch:  33 Batch:   0 Loss: 0.14287 Accuracy: 1.00000\n",
      "Epoch:  34 Batch:   0 Loss: 0.13007 Accuracy: 1.00000\n",
      "Epoch:  35 Batch:   0 Loss: 0.11932 Accuracy: 1.00000\n",
      "Epoch:  36 Batch:   0 Loss: 0.11075 Accuracy: 1.00000\n",
      "Epoch:  37 Batch:   0 Loss: 0.10099 Accuracy: 1.00000\n",
      "Epoch:  38 Batch:   0 Loss: 0.09336 Accuracy: 1.00000\n",
      "Epoch:  39 Batch:   0 Loss: 0.08604 Accuracy: 1.00000\n",
      "Test Loss: 3.35241 Accuracy: 0.71963\n",
      "Epoch:  40 Batch:   0 Loss: 0.07990 Accuracy: 1.00000\n",
      "Epoch:  41 Batch:   0 Loss: 0.07430 Accuracy: 1.00000\n",
      "Epoch:  42 Batch:   0 Loss: 0.06918 Accuracy: 1.00000\n",
      "Epoch:  43 Batch:   0 Loss: 0.06470 Accuracy: 1.00000\n",
      "Epoch:  44 Batch:   0 Loss: 0.06128 Accuracy: 1.00000\n",
      "Epoch:  45 Batch:   0 Loss: 0.05921 Accuracy: 1.00000\n",
      "Epoch:  46 Batch:   0 Loss: 0.05800 Accuracy: 1.00000\n",
      "Epoch:  47 Batch:   0 Loss: 0.05687 Accuracy: 1.00000\n",
      "Epoch:  48 Batch:   0 Loss: 0.05588 Accuracy: 1.00000\n",
      "Epoch:  49 Batch:   0 Loss: 0.05492 Accuracy: 1.00000\n",
      "Test Loss: 3.65934 Accuracy: 0.72283\n",
      "Epoch:  50 Batch:   0 Loss: 0.05404 Accuracy: 1.00000\n",
      "Epoch:  51 Batch:   0 Loss: 0.05323 Accuracy: 1.00000\n",
      "Epoch:  52 Batch:   0 Loss: 0.05245 Accuracy: 1.00000\n",
      "Epoch:  53 Batch:   0 Loss: 0.05191 Accuracy: 1.00000\n",
      "Epoch:  54 Batch:   0 Loss: 0.05106 Accuracy: 1.00000\n",
      "Epoch:  55 Batch:   0 Loss: 0.05039 Accuracy: 1.00000\n",
      "Epoch:  56 Batch:   0 Loss: 0.04961 Accuracy: 1.00000\n",
      "Epoch:  57 Batch:   0 Loss: 0.04908 Accuracy: 1.00000\n",
      "Epoch:  58 Batch:   0 Loss: 0.04855 Accuracy: 1.00000\n",
      "Epoch:  59 Batch:   0 Loss: 0.04800 Accuracy: 1.00000\n",
      "Test Loss: 3.92446 Accuracy: 0.72098\n",
      "Epoch:  60 Batch:   0 Loss: 0.04772 Accuracy: 1.00000\n",
      "Epoch:  61 Batch:   0 Loss: 0.04753 Accuracy: 1.00000\n",
      "Epoch:  62 Batch:   0 Loss: 0.04740 Accuracy: 1.00000\n",
      "Epoch:  63 Batch:   0 Loss: 0.04727 Accuracy: 1.00000\n",
      "Epoch:  64 Batch:   0 Loss: 0.04718 Accuracy: 1.00000\n",
      "Epoch:  65 Batch:   0 Loss: 0.04703 Accuracy: 1.00000\n",
      "Epoch:  66 Batch:   0 Loss: 0.04690 Accuracy: 1.00000\n",
      "Epoch:  67 Batch:   0 Loss: 0.04677 Accuracy: 1.00000\n",
      "Epoch:  68 Batch:   0 Loss: 0.04665 Accuracy: 1.00000\n",
      "Epoch:  69 Batch:   0 Loss: 0.04654 Accuracy: 1.00000\n",
      "Test Loss: 4.11349 Accuracy: 0.71930\n",
      "Epoch:  70 Batch:   0 Loss: 0.04646 Accuracy: 1.00000\n",
      "Epoch:  71 Batch:   0 Loss: 0.04640 Accuracy: 1.00000\n",
      "Epoch:  72 Batch:   0 Loss: 0.04637 Accuracy: 1.00000\n",
      "Epoch:  73 Batch:   0 Loss: 0.04631 Accuracy: 1.00000\n",
      "Epoch:  74 Batch:   0 Loss: 0.04624 Accuracy: 1.00000\n",
      "Epoch:  75 Batch:   0 Loss: 0.04614 Accuracy: 1.00000\n",
      "Epoch:  76 Batch:   0 Loss: 0.04605 Accuracy: 1.00000\n",
      "Epoch:  77 Batch:   0 Loss: 0.04595 Accuracy: 1.00000\n",
      "Epoch:  78 Batch:   0 Loss: 0.04595 Accuracy: 1.00000\n",
      "Epoch:  79 Batch:   0 Loss: 0.04575 Accuracy: 1.00000\n",
      "Test Loss: 4.23373 Accuracy: 0.71805\n",
      "Epoch:  80 Batch:   0 Loss: 0.04607 Accuracy: 1.00000\n",
      "Epoch:  81 Batch:   0 Loss: 0.04570 Accuracy: 1.00000\n",
      "Epoch:  82 Batch:   0 Loss: 0.04633 Accuracy: 1.00000\n",
      "Epoch:  83 Batch:   0 Loss: 0.04817 Accuracy: 1.00000\n",
      "Epoch:  84 Batch:   0 Loss: 0.04760 Accuracy: 1.00000\n",
      "Epoch:  85 Batch:   0 Loss: 0.04512 Accuracy: 1.00000\n",
      "Epoch:  86 Batch:   0 Loss: 0.04587 Accuracy: 1.00000\n",
      "Epoch:  87 Batch:   0 Loss: 0.04599 Accuracy: 1.00000\n",
      "Epoch:  88 Batch:   0 Loss: 0.04764 Accuracy: 1.00000\n",
      "Epoch:  89 Batch:   0 Loss: 0.04517 Accuracy: 1.00000\n",
      "Test Loss: 4.32527 Accuracy: 0.71804\n",
      "Epoch:  90 Batch:   0 Loss: 0.04665 Accuracy: 1.00000\n",
      "Epoch:  91 Batch:   0 Loss: 0.04677 Accuracy: 1.00000\n",
      "Epoch:  92 Batch:   0 Loss: 0.04620 Accuracy: 1.00000\n",
      "Epoch:  93 Batch:   0 Loss: 0.04865 Accuracy: 1.00000\n",
      "Epoch:  94 Batch:   0 Loss: 0.04579 Accuracy: 1.00000\n",
      "Epoch:  95 Batch:   0 Loss: 0.04676 Accuracy: 1.00000\n",
      "Epoch:  96 Batch:   0 Loss: 0.04732 Accuracy: 1.00000\n",
      "Epoch:  97 Batch:   0 Loss: 0.04734 Accuracy: 1.00000\n",
      "Epoch:  98 Batch:   0 Loss: 0.04668 Accuracy: 1.00000\n",
      "Epoch:  99 Batch:   0 Loss: 0.04675 Accuracy: 1.00000\n",
      "Test Loss: 4.33254 Accuracy: 0.71687\n",
      "Epoch: 100 Batch:   0 Loss: 0.04681 Accuracy: 1.00000\n",
      "Epoch: 101 Batch:   0 Loss: 0.04682 Accuracy: 1.00000\n",
      "Epoch: 102 Batch:   0 Loss: 0.04671 Accuracy: 1.00000\n",
      "Epoch: 103 Batch:   0 Loss: 0.04778 Accuracy: 1.00000\n",
      "Epoch: 104 Batch:   0 Loss: 0.04741 Accuracy: 1.00000\n",
      "Epoch: 105 Batch:   0 Loss: 0.04993 Accuracy: 1.00000\n",
      "Epoch: 106 Batch:   0 Loss: 0.04748 Accuracy: 1.00000\n",
      "Epoch: 107 Batch:   0 Loss: 0.04722 Accuracy: 1.00000\n",
      "Epoch: 108 Batch:   0 Loss: 0.04721 Accuracy: 1.00000\n",
      "Epoch: 109 Batch:   0 Loss: 0.04816 Accuracy: 1.00000\n",
      "Test Loss: 4.26926 Accuracy: 0.71701\n",
      "Epoch: 110 Batch:   0 Loss: 0.04756 Accuracy: 1.00000\n",
      "Epoch: 111 Batch:   0 Loss: 0.04619 Accuracy: 1.00000\n",
      "Epoch: 112 Batch:   0 Loss: 0.04752 Accuracy: 1.00000\n",
      "Epoch: 113 Batch:   0 Loss: 0.04603 Accuracy: 1.00000\n",
      "Epoch: 114 Batch:   0 Loss: 0.04967 Accuracy: 1.00000\n",
      "Epoch: 115 Batch:   0 Loss: 0.04604 Accuracy: 1.00000\n",
      "Epoch: 116 Batch:   0 Loss: 0.04710 Accuracy: 1.00000\n",
      "Epoch: 117 Batch:   0 Loss: 0.04650 Accuracy: 1.00000\n",
      "Epoch: 118 Batch:   0 Loss: 0.04807 Accuracy: 1.00000\n",
      "Epoch: 119 Batch:   0 Loss: 0.04860 Accuracy: 1.00000\n",
      "Test Loss: 4.19986 Accuracy: 0.71891\n",
      "Epoch: 120 Batch:   0 Loss: 0.04745 Accuracy: 1.00000\n",
      "Epoch: 121 Batch:   0 Loss: 0.04572 Accuracy: 1.00000\n",
      "Epoch: 122 Batch:   0 Loss: 0.04813 Accuracy: 1.00000\n",
      "Epoch: 123 Batch:   0 Loss: 0.04658 Accuracy: 1.00000\n",
      "Epoch: 124 Batch:   0 Loss: 0.04609 Accuracy: 1.00000\n",
      "Epoch: 125 Batch:   0 Loss: 0.04635 Accuracy: 1.00000\n",
      "Epoch: 126 Batch:   0 Loss: 0.04680 Accuracy: 1.00000\n",
      "Epoch: 127 Batch:   0 Loss: 0.04702 Accuracy: 1.00000\n",
      "Epoch: 128 Batch:   0 Loss: 0.04673 Accuracy: 1.00000\n",
      "Epoch: 129 Batch:   0 Loss: 0.04649 Accuracy: 1.00000\n",
      "Test Loss: 4.11959 Accuracy: 0.71402\n",
      "Epoch: 130 Batch:   0 Loss: 0.04640 Accuracy: 1.00000\n",
      "Epoch: 131 Batch:   0 Loss: 0.04641 Accuracy: 1.00000\n",
      "Epoch: 132 Batch:   0 Loss: 0.04644 Accuracy: 1.00000\n",
      "Epoch: 133 Batch:   0 Loss: 0.04790 Accuracy: 1.00000\n",
      "Epoch: 134 Batch:   0 Loss: 0.04667 Accuracy: 1.00000\n",
      "Epoch: 135 Batch:   0 Loss: 0.04971 Accuracy: 1.00000\n",
      "Epoch: 136 Batch:   0 Loss: 0.04629 Accuracy: 1.00000\n",
      "Epoch: 137 Batch:   0 Loss: 0.04640 Accuracy: 1.00000\n",
      "Epoch: 138 Batch:   0 Loss: 0.04529 Accuracy: 1.00000\n",
      "Epoch: 139 Batch:   0 Loss: 0.04758 Accuracy: 1.00000\n",
      "Test Loss: 4.02899 Accuracy: 0.71432\n",
      "Epoch: 140 Batch:   0 Loss: 0.04922 Accuracy: 1.00000\n",
      "Epoch: 141 Batch:   0 Loss: 0.04697 Accuracy: 1.00000\n",
      "Epoch: 142 Batch:   0 Loss: 0.04559 Accuracy: 1.00000\n",
      "Epoch: 143 Batch:   0 Loss: 0.04614 Accuracy: 1.00000\n",
      "Epoch: 144 Batch:   0 Loss: 0.05031 Accuracy: 1.00000\n",
      "Epoch: 145 Batch:   0 Loss: 0.04584 Accuracy: 1.00000\n",
      "Epoch: 146 Batch:   0 Loss: 0.04451 Accuracy: 1.00000\n",
      "Epoch: 147 Batch:   0 Loss: 0.04437 Accuracy: 1.00000\n",
      "Epoch: 148 Batch:   0 Loss: 0.04506 Accuracy: 1.00000\n",
      "Epoch: 149 Batch:   0 Loss: 0.04570 Accuracy: 1.00000\n",
      "Test Loss: 3.89752 Accuracy: 0.71007\n",
      "Epoch: 150 Batch:   0 Loss: 0.04594 Accuracy: 1.00000\n",
      "Epoch: 151 Batch:   0 Loss: 0.04621 Accuracy: 1.00000\n",
      "Epoch: 152 Batch:   0 Loss: 0.04554 Accuracy: 1.00000\n",
      "Epoch: 153 Batch:   0 Loss: 0.04526 Accuracy: 1.00000\n",
      "Epoch: 154 Batch:   0 Loss: 0.04594 Accuracy: 1.00000\n",
      "Epoch: 155 Batch:   0 Loss: 0.04615 Accuracy: 1.00000\n",
      "Epoch: 156 Batch:   0 Loss: 0.04586 Accuracy: 1.00000\n",
      "Epoch: 157 Batch:   0 Loss: 0.04594 Accuracy: 1.00000\n",
      "Epoch: 158 Batch:   0 Loss: 0.04464 Accuracy: 1.00000\n",
      "Epoch: 159 Batch:   0 Loss: 0.04497 Accuracy: 1.00000\n",
      "Test Loss: 3.82474 Accuracy: 0.70927\n",
      "Epoch: 160 Batch:   0 Loss: 0.04444 Accuracy: 1.00000\n",
      "Epoch: 161 Batch:   0 Loss: 0.04656 Accuracy: 1.00000\n",
      "Epoch: 162 Batch:   0 Loss: 0.04731 Accuracy: 1.00000\n",
      "Epoch: 163 Batch:   0 Loss: 0.04549 Accuracy: 1.00000\n",
      "Epoch: 164 Batch:   0 Loss: 0.04400 Accuracy: 1.00000\n",
      "Epoch: 165 Batch:   0 Loss: 0.04475 Accuracy: 1.00000\n",
      "Epoch: 166 Batch:   0 Loss: 0.04596 Accuracy: 1.00000\n",
      "Epoch: 167 Batch:   0 Loss: 0.04490 Accuracy: 1.00000\n",
      "Epoch: 168 Batch:   0 Loss: 0.04435 Accuracy: 1.00000\n",
      "Epoch: 169 Batch:   0 Loss: 0.04404 Accuracy: 1.00000\n",
      "Test Loss: 3.72314 Accuracy: 0.70220\n",
      "Epoch: 170 Batch:   0 Loss: 0.04437 Accuracy: 1.00000\n",
      "Epoch: 171 Batch:   0 Loss: 0.04490 Accuracy: 1.00000\n",
      "Epoch: 172 Batch:   0 Loss: 0.04746 Accuracy: 1.00000\n",
      "Epoch: 173 Batch:   0 Loss: 0.04960 Accuracy: 1.00000\n",
      "Epoch: 174 Batch:   0 Loss: 0.04362 Accuracy: 1.00000\n",
      "Epoch: 175 Batch:   0 Loss: 0.04340 Accuracy: 1.00000\n",
      "Epoch: 176 Batch:   0 Loss: 0.04310 Accuracy: 1.00000\n",
      "Epoch: 177 Batch:   0 Loss: 0.04310 Accuracy: 1.00000\n",
      "Epoch: 178 Batch:   0 Loss: 0.04347 Accuracy: 1.00000\n",
      "Epoch: 179 Batch:   0 Loss: 0.04399 Accuracy: 1.00000\n",
      "Test Loss: 3.63007 Accuracy: 0.70169\n",
      "Epoch: 180 Batch:   0 Loss: 0.04463 Accuracy: 1.00000\n",
      "Epoch: 181 Batch:   0 Loss: 0.04418 Accuracy: 1.00000\n",
      "Epoch: 182 Batch:   0 Loss: 0.04403 Accuracy: 1.00000\n",
      "Epoch: 183 Batch:   0 Loss: 0.04370 Accuracy: 1.00000\n",
      "Epoch: 184 Batch:   0 Loss: 0.04361 Accuracy: 1.00000\n",
      "Epoch: 185 Batch:   0 Loss: 0.04536 Accuracy: 1.00000\n",
      "Epoch: 186 Batch:   0 Loss: 0.04352 Accuracy: 1.00000\n",
      "Epoch: 187 Batch:   0 Loss: 0.04311 Accuracy: 1.00000\n",
      "Epoch: 188 Batch:   0 Loss: 0.04395 Accuracy: 1.00000\n",
      "Epoch: 189 Batch:   0 Loss: 0.04446 Accuracy: 1.00000\n",
      "Test Loss: 3.54981 Accuracy: 0.70395\n",
      "Epoch: 190 Batch:   0 Loss: 0.04472 Accuracy: 1.00000\n",
      "Epoch: 191 Batch:   0 Loss: 0.04351 Accuracy: 1.00000\n",
      "Epoch: 192 Batch:   0 Loss: 0.04323 Accuracy: 1.00000\n",
      "Epoch: 193 Batch:   0 Loss: 0.04305 Accuracy: 1.00000\n",
      "Epoch: 194 Batch:   0 Loss: 0.04324 Accuracy: 1.00000\n",
      "Epoch: 195 Batch:   0 Loss: 0.04304 Accuracy: 1.00000\n",
      "Epoch: 196 Batch:   0 Loss: 0.04435 Accuracy: 1.00000\n",
      "Epoch: 197 Batch:   0 Loss: 0.04548 Accuracy: 1.00000\n",
      "Epoch: 198 Batch:   0 Loss: 0.04284 Accuracy: 1.00000\n",
      "Epoch: 199 Batch:   0 Loss: 0.04276 Accuracy: 1.00000\n",
      "Test Loss: 3.45340 Accuracy: 0.69562\n",
      "Epoch: 200 Batch:   0 Loss: 0.04278 Accuracy: 1.00000\n",
      "Epoch: 201 Batch:   0 Loss: 0.04311 Accuracy: 1.00000\n",
      "Epoch: 202 Batch:   0 Loss: 0.04393 Accuracy: 1.00000\n",
      "Epoch: 203 Batch:   0 Loss: 0.04474 Accuracy: 1.00000\n",
      "Epoch: 204 Batch:   0 Loss: 0.04324 Accuracy: 1.00000\n",
      "Epoch: 205 Batch:   0 Loss: 0.04294 Accuracy: 1.00000\n",
      "Epoch: 206 Batch:   0 Loss: 0.04292 Accuracy: 1.00000\n",
      "Epoch: 207 Batch:   0 Loss: 0.04334 Accuracy: 1.00000\n",
      "Epoch: 208 Batch:   0 Loss: 0.04544 Accuracy: 1.00000\n",
      "Epoch: 209 Batch:   0 Loss: 0.04334 Accuracy: 1.00000\n",
      "Test Loss: 3.39338 Accuracy: 0.69519\n",
      "Epoch: 210 Batch:   0 Loss: 0.04259 Accuracy: 1.00000\n",
      "Epoch: 211 Batch:   0 Loss: 0.04297 Accuracy: 1.00000\n",
      "Epoch: 212 Batch:   0 Loss: 0.04276 Accuracy: 1.00000\n",
      "Epoch: 213 Batch:   0 Loss: 0.04267 Accuracy: 1.00000\n",
      "Epoch: 214 Batch:   0 Loss: 0.04301 Accuracy: 1.00000\n",
      "Epoch: 215 Batch:   0 Loss: 0.04333 Accuracy: 1.00000\n",
      "Epoch: 216 Batch:   0 Loss: 0.04313 Accuracy: 1.00000\n",
      "Epoch: 217 Batch:   0 Loss: 0.04289 Accuracy: 1.00000\n",
      "Epoch: 218 Batch:   0 Loss: 0.04379 Accuracy: 1.00000\n",
      "Epoch: 219 Batch:   0 Loss: 0.04257 Accuracy: 1.00000\n",
      "Test Loss: 3.35019 Accuracy: 0.69533\n",
      "Epoch: 220 Batch:   0 Loss: 0.04237 Accuracy: 1.00000\n",
      "Epoch: 221 Batch:   0 Loss: 0.04311 Accuracy: 1.00000\n",
      "Epoch: 222 Batch:   0 Loss: 0.04299 Accuracy: 1.00000\n",
      "Epoch: 223 Batch:   0 Loss: 0.04239 Accuracy: 1.00000\n",
      "Epoch: 224 Batch:   0 Loss: 0.04239 Accuracy: 1.00000\n",
      "Epoch: 225 Batch:   0 Loss: 0.04257 Accuracy: 1.00000\n",
      "Epoch: 226 Batch:   0 Loss: 0.04321 Accuracy: 1.00000\n",
      "Epoch: 227 Batch:   0 Loss: 0.04319 Accuracy: 1.00000\n",
      "Epoch: 228 Batch:   0 Loss: 0.04270 Accuracy: 1.00000\n",
      "Epoch: 229 Batch:   0 Loss: 0.04230 Accuracy: 1.00000\n",
      "Test Loss: 3.30924 Accuracy: 0.69403\n",
      "Epoch: 230 Batch:   0 Loss: 0.04253 Accuracy: 1.00000\n",
      "Epoch: 231 Batch:   0 Loss: 0.04307 Accuracy: 1.00000\n",
      "Epoch: 232 Batch:   0 Loss: 0.04271 Accuracy: 1.00000\n",
      "Epoch: 233 Batch:   0 Loss: 0.04249 Accuracy: 1.00000\n",
      "Epoch: 234 Batch:   0 Loss: 0.04261 Accuracy: 1.00000\n",
      "Epoch: 235 Batch:   0 Loss: 0.04296 Accuracy: 1.00000\n",
      "Epoch: 236 Batch:   0 Loss: 0.04297 Accuracy: 1.00000\n",
      "Epoch: 237 Batch:   0 Loss: 0.04262 Accuracy: 1.00000\n",
      "Epoch: 238 Batch:   0 Loss: 0.04252 Accuracy: 1.00000\n",
      "Epoch: 239 Batch:   0 Loss: 0.04265 Accuracy: 1.00000\n",
      "Test Loss: 3.25638 Accuracy: 0.69396\n",
      "Epoch: 240 Batch:   0 Loss: 0.04291 Accuracy: 1.00000\n",
      "Epoch: 241 Batch:   0 Loss: 0.04282 Accuracy: 1.00000\n",
      "Epoch: 242 Batch:   0 Loss: 0.04262 Accuracy: 1.00000\n",
      "Epoch: 243 Batch:   0 Loss: 0.04293 Accuracy: 1.00000\n",
      "Epoch: 244 Batch:   0 Loss: 0.04339 Accuracy: 1.00000\n",
      "Epoch: 245 Batch:   0 Loss: 0.04219 Accuracy: 1.00000\n",
      "Epoch: 246 Batch:   0 Loss: 0.04220 Accuracy: 1.00000\n",
      "Epoch: 247 Batch:   0 Loss: 0.04235 Accuracy: 1.00000\n",
      "Epoch: 248 Batch:   0 Loss: 0.04254 Accuracy: 1.00000\n",
      "Epoch: 249 Batch:   0 Loss: 0.04234 Accuracy: 1.00000\n",
      "Test Loss: 3.14772 Accuracy: 0.69790\n",
      "Epoch: 250 Batch:   0 Loss: 0.04233 Accuracy: 1.00000\n",
      "Epoch: 251 Batch:   0 Loss: 0.04221 Accuracy: 1.00000\n",
      "Epoch: 252 Batch:   0 Loss: 0.04234 Accuracy: 1.00000\n",
      "Epoch: 253 Batch:   0 Loss: 0.04269 Accuracy: 1.00000\n",
      "Epoch: 254 Batch:   0 Loss: 0.04389 Accuracy: 1.00000\n",
      "Epoch: 255 Batch:   0 Loss: 0.04293 Accuracy: 1.00000\n",
      "Epoch: 256 Batch:   0 Loss: 0.04212 Accuracy: 1.00000\n",
      "Epoch: 257 Batch:   0 Loss: 0.04230 Accuracy: 1.00000\n",
      "Epoch: 258 Batch:   0 Loss: 0.04208 Accuracy: 1.00000\n",
      "Epoch: 259 Batch:   0 Loss: 0.04191 Accuracy: 1.00000\n",
      "Test Loss: 3.11757 Accuracy: 0.70278\n",
      "Epoch: 260 Batch:   0 Loss: 0.04280 Accuracy: 1.00000\n",
      "Epoch: 261 Batch:   0 Loss: 0.04214 Accuracy: 1.00000\n",
      "Epoch: 262 Batch:   0 Loss: 0.04190 Accuracy: 1.00000\n",
      "Epoch: 263 Batch:   0 Loss: 0.04194 Accuracy: 1.00000\n",
      "Epoch: 264 Batch:   0 Loss: 0.04174 Accuracy: 1.00000\n",
      "Epoch: 265 Batch:   0 Loss: 0.04269 Accuracy: 1.00000\n",
      "Epoch: 266 Batch:   0 Loss: 0.04219 Accuracy: 1.00000\n",
      "Epoch: 267 Batch:   0 Loss: 0.04214 Accuracy: 1.00000\n",
      "Epoch: 268 Batch:   0 Loss: 0.04184 Accuracy: 1.00000\n",
      "Epoch: 269 Batch:   0 Loss: 0.04173 Accuracy: 1.00000\n",
      "Test Loss: 3.02962 Accuracy: 0.69957\n",
      "Epoch: 270 Batch:   0 Loss: 0.04173 Accuracy: 1.00000\n",
      "Epoch: 271 Batch:   0 Loss: 0.04181 Accuracy: 1.00000\n",
      "Epoch: 272 Batch:   0 Loss: 0.04203 Accuracy: 1.00000\n",
      "Epoch: 273 Batch:   0 Loss: 0.04215 Accuracy: 1.00000\n",
      "Epoch: 274 Batch:   0 Loss: 0.04201 Accuracy: 1.00000\n",
      "Epoch: 275 Batch:   0 Loss: 0.04243 Accuracy: 1.00000\n",
      "Epoch: 276 Batch:   0 Loss: 0.04319 Accuracy: 1.00000\n",
      "Epoch: 277 Batch:   0 Loss: 0.04198 Accuracy: 1.00000\n",
      "Epoch: 278 Batch:   0 Loss: 0.04200 Accuracy: 1.00000\n",
      "Epoch: 279 Batch:   0 Loss: 0.04193 Accuracy: 1.00000\n",
      "Test Loss: 3.00669 Accuracy: 0.70067\n",
      "Epoch: 280 Batch:   0 Loss: 0.04224 Accuracy: 1.00000\n",
      "Epoch: 281 Batch:   0 Loss: 0.04227 Accuracy: 1.00000\n",
      "Epoch: 282 Batch:   0 Loss: 0.04234 Accuracy: 1.00000\n",
      "Epoch: 283 Batch:   0 Loss: 0.04222 Accuracy: 1.00000\n",
      "Epoch: 284 Batch:   0 Loss: 0.04215 Accuracy: 1.00000\n",
      "Epoch: 285 Batch:   0 Loss: 0.04233 Accuracy: 1.00000\n",
      "Epoch: 286 Batch:   0 Loss: 0.04248 Accuracy: 1.00000\n",
      "Epoch: 287 Batch:   0 Loss: 0.04249 Accuracy: 1.00000\n",
      "Epoch: 288 Batch:   0 Loss: 0.04249 Accuracy: 1.00000\n",
      "Epoch: 289 Batch:   0 Loss: 0.04248 Accuracy: 1.00000\n",
      "Test Loss: 2.95343 Accuracy: 0.70000\n",
      "Epoch: 290 Batch:   0 Loss: 0.04259 Accuracy: 1.00000\n",
      "Epoch: 291 Batch:   0 Loss: 0.04306 Accuracy: 1.00000\n",
      "Epoch: 292 Batch:   0 Loss: 0.04334 Accuracy: 1.00000\n",
      "Epoch: 293 Batch:   0 Loss: 0.04247 Accuracy: 1.00000\n",
      "Epoch: 294 Batch:   0 Loss: 0.04255 Accuracy: 1.00000\n",
      "Epoch: 295 Batch:   0 Loss: 0.04241 Accuracy: 1.00000\n",
      "Epoch: 296 Batch:   0 Loss: 0.04248 Accuracy: 1.00000\n",
      "Epoch: 297 Batch:   0 Loss: 0.04263 Accuracy: 1.00000\n",
      "Epoch: 298 Batch:   0 Loss: 0.04277 Accuracy: 1.00000\n",
      "Epoch: 299 Batch:   0 Loss: 0.04273 Accuracy: 1.00000\n",
      "Test Loss: 2.85898 Accuracy: 0.69905\n"
     ]
    }
   ],
   "source": [
    "# Setup input and train protoNN\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uypXmz1QJ2h"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.671507Z",
     "start_time": "2018-08-15T13:07:22.645050Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96103,
     "status": "ok",
     "timestamp": 1567154584845,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "VYxefAD3QJ2j",
    "outputId": "f8b711a1-bae4-4bf5-9a62-935838844601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.6988769\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91542,
     "status": "ok",
     "timestamp": 1567154584848,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "yJM9puhcQJ2t",
    "outputId": "d8e743e0-1068-4681-e3bc-3b650ab66a3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3949 2907]\n",
      " [1222 5634]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.76368   0.57599   0.65669      6856\n",
      "           1    0.65964   0.82176   0.73183      6856\n",
      "\n",
      "    accuracy                        0.69888     13712\n",
      "   macro avg    0.71166   0.69888   0.69426     13712\n",
      "weighted avg    0.71166   0.69888   0.69426     13712\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "protoNN_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ProtoNN",
   "language": "python",
   "name": "protonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
