{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwwtn5iEQJ1M"
   },
   "source": [
    "# ProtoNN in Tensorflow\n",
    "\n",
    "This is a simple notebook that illustrates the usage of Tensorflow implementation of ProtoNN. We are using the USPS dataset. Please refer to `fetch_usps.py` for more details on downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.223951Z",
     "start_time": "2018-08-15T13:06:09.303454Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dJBVr2b7QJ1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(r\"D:\\programming\\practice\\research\\protoNN\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxqvfwWQtQ-s"
   },
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cT-KokQSQiS6"
   },
   "outputs": [],
   "source": [
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou1MfKhYtMdT"
   },
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDVo_0JiRSi9"
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAjSVSOFtFmm"
   },
   "source": [
    "# Model Trainer - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp5dEFiZR_sy"
   },
   "outputs": [],
   "source": [
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Z6ym4k_s9pS"
   },
   "source": [
    "# Model Graph - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRPFglKHSbu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEBMKewPQJ1c"
   },
   "source": [
    "# Obtain Data\n",
    "\n",
    "It is assumed that the Daphnet data has already been downloaded,preprocessed and set up in subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "windowLen = 'data'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data.npy'), np.load(DATA_DIR + '/ttest_data.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u6oX8eJQJ2N"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable `GAMMA`. If `GAMMA` is set to `None`, median heuristic will be used to estimate a good value of $\\gamma$ through the `helper.getGamma()` method. This method also returns the corresponding `W` and `B` matrices which should be used to initialize ProtoNN (as is done here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.279204Z",
     "start_time": "2018-08-15T13:06:10.272880Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UaduZ1vJQJ2P"
   },
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 0.8\n",
    "SPAR_Z = 0.8\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 600\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.007586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.307632Z",
     "start_time": "2018-08-15T13:06:10.280955Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1567154485603,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "teqlUPhLQJ2W",
    "outputId": "e7e7f7f2-9ddb-448b-9539-65a1a2dc1c03"
   },
   "outputs": [],
   "source": [
    "W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJwO4MXatk9G"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.641991Z",
     "start_time": "2018-08-15T13:06:10.309353Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98358,
     "status": "ok",
     "timestamp": 1567154584840,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "MrKAP5_RQJ2b",
    "outputId": "2fb982af-47ae-4867-c5e5-b2ecc2b9dfc4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Batch:   0 Loss: 6.57128 Accuracy: 0.00000\n",
      "Epoch:   1 Batch:   0 Loss: 2.66670 Accuracy: 0.00000\n",
      "Epoch:   2 Batch:   0 Loss: 3.32305 Accuracy: 0.00000\n",
      "Epoch:   3 Batch:   0 Loss: 3.54816 Accuracy: 0.00000\n",
      "Epoch:   4 Batch:   0 Loss: 3.63148 Accuracy: 0.00000\n",
      "Epoch:   5 Batch:   0 Loss: 3.65622 Accuracy: 0.00000\n",
      "Epoch:   6 Batch:   0 Loss: 3.64175 Accuracy: 0.00000\n",
      "Epoch:   7 Batch:   0 Loss: 3.58378 Accuracy: 0.00000\n",
      "Epoch:   8 Batch:   0 Loss: 3.47416 Accuracy: 0.00000\n",
      "Epoch:   9 Batch:   0 Loss: 3.31540 Accuracy: 0.00000\n",
      "Test Loss: 1.68696 Accuracy: 0.50020\n",
      "Epoch:  10 Batch:   0 Loss: 3.11747 Accuracy: 0.00000\n",
      "Epoch:  11 Batch:   0 Loss: 2.89448 Accuracy: 0.00000\n",
      "Epoch:  12 Batch:   0 Loss: 2.66599 Accuracy: 0.00000\n",
      "Epoch:  13 Batch:   0 Loss: 2.44510 Accuracy: 0.00000\n",
      "Epoch:  14 Batch:   0 Loss: 2.24200 Accuracy: 0.00000\n",
      "Epoch:  15 Batch:   0 Loss: 2.06063 Accuracy: 0.00000\n",
      "Epoch:  16 Batch:   0 Loss: 1.90067 Accuracy: 0.00000\n",
      "Epoch:  17 Batch:   0 Loss: 1.76035 Accuracy: 0.00000\n",
      "Epoch:  18 Batch:   0 Loss: 1.63581 Accuracy: 0.00000\n",
      "Epoch:  19 Batch:   0 Loss: 1.52315 Accuracy: 0.00000\n",
      "Test Loss: 1.59826 Accuracy: 0.51256\n",
      "Epoch:  20 Batch:   0 Loss: 1.41913 Accuracy: 0.00000\n",
      "Epoch:  21 Batch:   0 Loss: 1.32220 Accuracy: 0.00000\n",
      "Epoch:  22 Batch:   0 Loss: 1.23268 Accuracy: 0.00000\n",
      "Epoch:  23 Batch:   0 Loss: 1.15106 Accuracy: 0.00000\n",
      "Epoch:  24 Batch:   0 Loss: 1.07738 Accuracy: 0.00000\n",
      "Epoch:  25 Batch:   0 Loss: 1.01121 Accuracy: 0.00000\n",
      "Epoch:  26 Batch:   0 Loss: 0.95181 Accuracy: 0.00000\n",
      "Epoch:  27 Batch:   0 Loss: 0.89831 Accuracy: 0.03125\n",
      "Epoch:  28 Batch:   0 Loss: 0.84993 Accuracy: 0.12500\n",
      "Epoch:  29 Batch:   0 Loss: 0.80595 Accuracy: 0.15625\n",
      "Test Loss: 1.37510 Accuracy: 0.53929\n",
      "Epoch:  30 Batch:   0 Loss: 0.76579 Accuracy: 0.40625\n",
      "Epoch:  31 Batch:   0 Loss: 0.72897 Accuracy: 0.43750\n",
      "Epoch:  32 Batch:   0 Loss: 0.69508 Accuracy: 0.50000\n",
      "Epoch:  33 Batch:   0 Loss: 0.66380 Accuracy: 0.75000\n",
      "Epoch:  34 Batch:   0 Loss: 0.63485 Accuracy: 0.81250\n",
      "Epoch:  35 Batch:   0 Loss: 0.60799 Accuracy: 0.87500\n",
      "Epoch:  36 Batch:   0 Loss: 0.58304 Accuracy: 0.90625\n",
      "Epoch:  37 Batch:   0 Loss: 0.55982 Accuracy: 0.93750\n",
      "Epoch:  38 Batch:   0 Loss: 0.53821 Accuracy: 0.96875\n",
      "Epoch:  39 Batch:   0 Loss: 0.51807 Accuracy: 0.96875\n",
      "Test Loss: 1.16940 Accuracy: 0.58209\n",
      "Epoch:  40 Batch:   0 Loss: 0.49931 Accuracy: 0.96875\n",
      "Epoch:  41 Batch:   0 Loss: 0.48185 Accuracy: 1.00000\n",
      "Epoch:  42 Batch:   0 Loss: 0.46559 Accuracy: 1.00000\n",
      "Epoch:  43 Batch:   0 Loss: 0.45047 Accuracy: 1.00000\n",
      "Epoch:  44 Batch:   0 Loss: 0.43640 Accuracy: 1.00000\n",
      "Epoch:  45 Batch:   0 Loss: 0.42331 Accuracy: 1.00000\n",
      "Epoch:  46 Batch:   0 Loss: 0.41114 Accuracy: 1.00000\n",
      "Epoch:  47 Batch:   0 Loss: 0.39979 Accuracy: 1.00000\n",
      "Epoch:  48 Batch:   0 Loss: 0.38923 Accuracy: 1.00000\n",
      "Epoch:  49 Batch:   0 Loss: 0.37942 Accuracy: 1.00000\n",
      "Test Loss: 1.02695 Accuracy: 0.65051\n",
      "Epoch:  50 Batch:   0 Loss: 0.37030 Accuracy: 1.00000\n",
      "Epoch:  51 Batch:   0 Loss: 0.36182 Accuracy: 1.00000\n",
      "Epoch:  52 Batch:   0 Loss: 0.35393 Accuracy: 1.00000\n",
      "Epoch:  53 Batch:   0 Loss: 0.34659 Accuracy: 1.00000\n",
      "Epoch:  54 Batch:   0 Loss: 0.33975 Accuracy: 1.00000\n",
      "Epoch:  55 Batch:   0 Loss: 0.33336 Accuracy: 1.00000\n",
      "Epoch:  56 Batch:   0 Loss: 0.32740 Accuracy: 1.00000\n",
      "Epoch:  57 Batch:   0 Loss: 0.32182 Accuracy: 1.00000\n",
      "Epoch:  58 Batch:   0 Loss: 0.31660 Accuracy: 1.00000\n",
      "Epoch:  59 Batch:   0 Loss: 0.31171 Accuracy: 1.00000\n",
      "Test Loss: 0.93551 Accuracy: 0.69402\n",
      "Epoch:  60 Batch:   0 Loss: 0.30712 Accuracy: 1.00000\n",
      "Epoch:  61 Batch:   0 Loss: 0.30280 Accuracy: 1.00000\n",
      "Epoch:  62 Batch:   0 Loss: 0.29875 Accuracy: 1.00000\n",
      "Epoch:  63 Batch:   0 Loss: 0.29493 Accuracy: 1.00000\n",
      "Epoch:  64 Batch:   0 Loss: 0.29134 Accuracy: 1.00000\n",
      "Epoch:  65 Batch:   0 Loss: 0.28796 Accuracy: 1.00000\n",
      "Epoch:  66 Batch:   0 Loss: 0.28476 Accuracy: 1.00000\n",
      "Epoch:  67 Batch:   0 Loss: 0.28174 Accuracy: 1.00000\n",
      "Epoch:  68 Batch:   0 Loss: 0.27889 Accuracy: 1.00000\n",
      "Epoch:  69 Batch:   0 Loss: 0.27618 Accuracy: 1.00000\n",
      "Test Loss: 0.87686 Accuracy: 0.71632\n",
      "Epoch:  70 Batch:   0 Loss: 0.27360 Accuracy: 1.00000\n",
      "Epoch:  71 Batch:   0 Loss: 0.27114 Accuracy: 1.00000\n",
      "Epoch:  72 Batch:   0 Loss: 0.26881 Accuracy: 1.00000\n",
      "Epoch:  73 Batch:   0 Loss: 0.26658 Accuracy: 1.00000\n",
      "Epoch:  74 Batch:   0 Loss: 0.26445 Accuracy: 1.00000\n",
      "Epoch:  75 Batch:   0 Loss: 0.26240 Accuracy: 1.00000\n",
      "Epoch:  76 Batch:   0 Loss: 0.26044 Accuracy: 1.00000\n",
      "Epoch:  77 Batch:   0 Loss: 0.25855 Accuracy: 1.00000\n",
      "Epoch:  78 Batch:   0 Loss: 0.25672 Accuracy: 1.00000\n",
      "Epoch:  79 Batch:   0 Loss: 0.25496 Accuracy: 1.00000\n",
      "Test Loss: 0.83824 Accuracy: 0.72687\n",
      "Epoch:  80 Batch:   0 Loss: 0.25324 Accuracy: 1.00000\n",
      "Epoch:  81 Batch:   0 Loss: 0.25158 Accuracy: 1.00000\n",
      "Epoch:  82 Batch:   0 Loss: 0.24996 Accuracy: 1.00000\n",
      "Epoch:  83 Batch:   0 Loss: 0.24837 Accuracy: 1.00000\n",
      "Epoch:  84 Batch:   0 Loss: 0.24682 Accuracy: 1.00000\n",
      "Epoch:  85 Batch:   0 Loss: 0.24531 Accuracy: 1.00000\n",
      "Epoch:  86 Batch:   0 Loss: 0.24382 Accuracy: 1.00000\n",
      "Epoch:  87 Batch:   0 Loss: 0.24236 Accuracy: 1.00000\n",
      "Epoch:  88 Batch:   0 Loss: 0.24091 Accuracy: 1.00000\n",
      "Epoch:  89 Batch:   0 Loss: 0.23949 Accuracy: 1.00000\n",
      "Test Loss: 0.81212 Accuracy: 0.73541\n",
      "Epoch:  90 Batch:   0 Loss: 0.23809 Accuracy: 1.00000\n",
      "Epoch:  91 Batch:   0 Loss: 0.23671 Accuracy: 1.00000\n",
      "Epoch:  92 Batch:   0 Loss: 0.23535 Accuracy: 1.00000\n",
      "Epoch:  93 Batch:   0 Loss: 0.23400 Accuracy: 1.00000\n",
      "Epoch:  94 Batch:   0 Loss: 0.23266 Accuracy: 1.00000\n",
      "Epoch:  95 Batch:   0 Loss: 0.23134 Accuracy: 1.00000\n",
      "Epoch:  96 Batch:   0 Loss: 0.23003 Accuracy: 1.00000\n",
      "Epoch:  97 Batch:   0 Loss: 0.22873 Accuracy: 1.00000\n",
      "Epoch:  98 Batch:   0 Loss: 0.22745 Accuracy: 1.00000\n",
      "Epoch:  99 Batch:   0 Loss: 0.22618 Accuracy: 1.00000\n",
      "Test Loss: 0.79437 Accuracy: 0.74103\n",
      "Epoch: 100 Batch:   0 Loss: 0.22492 Accuracy: 1.00000\n",
      "Epoch: 101 Batch:   0 Loss: 0.22368 Accuracy: 1.00000\n",
      "Epoch: 102 Batch:   0 Loss: 0.22244 Accuracy: 1.00000\n",
      "Epoch: 103 Batch:   0 Loss: 0.22122 Accuracy: 1.00000\n",
      "Epoch: 104 Batch:   0 Loss: 0.22001 Accuracy: 1.00000\n",
      "Epoch: 105 Batch:   0 Loss: 0.21881 Accuracy: 1.00000\n",
      "Epoch: 106 Batch:   0 Loss: 0.21762 Accuracy: 1.00000\n",
      "Epoch: 107 Batch:   0 Loss: 0.21644 Accuracy: 1.00000\n",
      "Epoch: 108 Batch:   0 Loss: 0.21527 Accuracy: 1.00000\n",
      "Epoch: 109 Batch:   0 Loss: 0.21412 Accuracy: 1.00000\n",
      "Test Loss: 0.78289 Accuracy: 0.74404\n",
      "Epoch: 110 Batch:   0 Loss: 0.21298 Accuracy: 1.00000\n",
      "Epoch: 111 Batch:   0 Loss: 0.21185 Accuracy: 1.00000\n",
      "Epoch: 112 Batch:   0 Loss: 0.21073 Accuracy: 1.00000\n",
      "Epoch: 113 Batch:   0 Loss: 0.20962 Accuracy: 1.00000\n",
      "Epoch: 114 Batch:   0 Loss: 0.20852 Accuracy: 1.00000\n",
      "Epoch: 115 Batch:   0 Loss: 0.20743 Accuracy: 1.00000\n",
      "Epoch: 116 Batch:   0 Loss: 0.20636 Accuracy: 1.00000\n",
      "Epoch: 117 Batch:   0 Loss: 0.20529 Accuracy: 1.00000\n",
      "Epoch: 118 Batch:   0 Loss: 0.20424 Accuracy: 1.00000\n",
      "Epoch: 119 Batch:   0 Loss: 0.20319 Accuracy: 1.00000\n",
      "Test Loss: 0.77592 Accuracy: 0.74675\n",
      "Epoch: 120 Batch:   0 Loss: 0.20215 Accuracy: 1.00000\n",
      "Epoch: 121 Batch:   0 Loss: 0.20113 Accuracy: 1.00000\n",
      "Epoch: 122 Batch:   0 Loss: 0.20011 Accuracy: 1.00000\n",
      "Epoch: 123 Batch:   0 Loss: 0.19910 Accuracy: 1.00000\n",
      "Epoch: 124 Batch:   0 Loss: 0.19809 Accuracy: 1.00000\n",
      "Epoch: 125 Batch:   0 Loss: 0.19710 Accuracy: 1.00000\n",
      "Epoch: 126 Batch:   0 Loss: 0.19610 Accuracy: 1.00000\n",
      "Epoch: 127 Batch:   0 Loss: 0.19512 Accuracy: 1.00000\n",
      "Epoch: 128 Batch:   0 Loss: 0.19414 Accuracy: 1.00000\n",
      "Epoch: 129 Batch:   0 Loss: 0.19317 Accuracy: 1.00000\n",
      "Test Loss: 0.77149 Accuracy: 0.74947\n",
      "Epoch: 130 Batch:   0 Loss: 0.19220 Accuracy: 1.00000\n",
      "Epoch: 131 Batch:   0 Loss: 0.19122 Accuracy: 1.00000\n",
      "Epoch: 132 Batch:   0 Loss: 0.19024 Accuracy: 1.00000\n",
      "Epoch: 133 Batch:   0 Loss: 0.18924 Accuracy: 1.00000\n",
      "Epoch: 134 Batch:   0 Loss: 0.18809 Accuracy: 1.00000\n",
      "Epoch: 135 Batch:   0 Loss: 0.18687 Accuracy: 1.00000\n",
      "Epoch: 136 Batch:   0 Loss: 0.18561 Accuracy: 1.00000\n",
      "Epoch: 137 Batch:   0 Loss: 0.18506 Accuracy: 1.00000\n",
      "Epoch: 138 Batch:   0 Loss: 0.18420 Accuracy: 1.00000\n",
      "Epoch: 139 Batch:   0 Loss: 0.18326 Accuracy: 1.00000\n",
      "Test Loss: 0.76316 Accuracy: 0.75429\n",
      "Epoch: 140 Batch:   0 Loss: 0.18228 Accuracy: 1.00000\n",
      "Epoch: 141 Batch:   0 Loss: 0.18137 Accuracy: 1.00000\n",
      "Epoch: 142 Batch:   0 Loss: 0.18058 Accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143 Batch:   0 Loss: 0.17974 Accuracy: 1.00000\n",
      "Epoch: 144 Batch:   0 Loss: 0.17887 Accuracy: 1.00000\n",
      "Epoch: 145 Batch:   0 Loss: 0.17798 Accuracy: 1.00000\n",
      "Epoch: 146 Batch:   0 Loss: 0.17709 Accuracy: 1.00000\n",
      "Epoch: 147 Batch:   0 Loss: 0.17620 Accuracy: 1.00000\n",
      "Epoch: 148 Batch:   0 Loss: 0.17530 Accuracy: 1.00000\n",
      "Epoch: 149 Batch:   0 Loss: 0.17440 Accuracy: 1.00000\n",
      "Test Loss: 0.76373 Accuracy: 0.75609\n",
      "Epoch: 150 Batch:   0 Loss: 0.17350 Accuracy: 1.00000\n",
      "Epoch: 151 Batch:   0 Loss: 0.17261 Accuracy: 1.00000\n",
      "Epoch: 152 Batch:   0 Loss: 0.17171 Accuracy: 1.00000\n",
      "Epoch: 153 Batch:   0 Loss: 0.17082 Accuracy: 1.00000\n",
      "Epoch: 154 Batch:   0 Loss: 0.16993 Accuracy: 1.00000\n",
      "Epoch: 155 Batch:   0 Loss: 0.16904 Accuracy: 1.00000\n",
      "Epoch: 156 Batch:   0 Loss: 0.16815 Accuracy: 1.00000\n",
      "Epoch: 157 Batch:   0 Loss: 0.16726 Accuracy: 1.00000\n",
      "Epoch: 158 Batch:   0 Loss: 0.16637 Accuracy: 1.00000\n",
      "Epoch: 159 Batch:   0 Loss: 0.16549 Accuracy: 1.00000\n",
      "Test Loss: 0.76619 Accuracy: 0.75680\n",
      "Epoch: 160 Batch:   0 Loss: 0.16461 Accuracy: 1.00000\n",
      "Epoch: 161 Batch:   0 Loss: 0.16373 Accuracy: 1.00000\n",
      "Epoch: 162 Batch:   0 Loss: 0.16285 Accuracy: 1.00000\n",
      "Epoch: 163 Batch:   0 Loss: 0.16198 Accuracy: 1.00000\n",
      "Epoch: 164 Batch:   0 Loss: 0.16111 Accuracy: 1.00000\n",
      "Epoch: 165 Batch:   0 Loss: 0.16024 Accuracy: 1.00000\n",
      "Epoch: 166 Batch:   0 Loss: 0.15905 Accuracy: 1.00000\n",
      "Epoch: 167 Batch:   0 Loss: 0.15845 Accuracy: 1.00000\n",
      "Epoch: 168 Batch:   0 Loss: 0.15762 Accuracy: 1.00000\n",
      "Epoch: 169 Batch:   0 Loss: 0.15676 Accuracy: 1.00000\n",
      "Test Loss: 0.76954 Accuracy: 0.75720\n",
      "Epoch: 170 Batch:   0 Loss: 0.15590 Accuracy: 1.00000\n",
      "Epoch: 171 Batch:   0 Loss: 0.15504 Accuracy: 1.00000\n",
      "Epoch: 172 Batch:   0 Loss: 0.15402 Accuracy: 1.00000\n",
      "Epoch: 173 Batch:   0 Loss: 0.15331 Accuracy: 1.00000\n",
      "Epoch: 174 Batch:   0 Loss: 0.15248 Accuracy: 1.00000\n",
      "Epoch: 175 Batch:   0 Loss: 0.15163 Accuracy: 1.00000\n",
      "Epoch: 176 Batch:   0 Loss: 0.15079 Accuracy: 1.00000\n",
      "Epoch: 177 Batch:   0 Loss: 0.14995 Accuracy: 1.00000\n",
      "Epoch: 178 Batch:   0 Loss: 0.14911 Accuracy: 1.00000\n",
      "Epoch: 179 Batch:   0 Loss: 0.14827 Accuracy: 1.00000\n",
      "Test Loss: 0.77369 Accuracy: 0.75820\n",
      "Epoch: 180 Batch:   0 Loss: 0.14744 Accuracy: 1.00000\n",
      "Epoch: 181 Batch:   0 Loss: 0.14661 Accuracy: 1.00000\n",
      "Epoch: 182 Batch:   0 Loss: 0.14579 Accuracy: 1.00000\n",
      "Epoch: 183 Batch:   0 Loss: 0.14496 Accuracy: 1.00000\n",
      "Epoch: 184 Batch:   0 Loss: 0.14415 Accuracy: 1.00000\n",
      "Epoch: 185 Batch:   0 Loss: 0.14333 Accuracy: 1.00000\n",
      "Epoch: 186 Batch:   0 Loss: 0.14252 Accuracy: 1.00000\n",
      "Epoch: 187 Batch:   0 Loss: 0.14171 Accuracy: 1.00000\n",
      "Epoch: 188 Batch:   0 Loss: 0.14090 Accuracy: 1.00000\n",
      "Epoch: 189 Batch:   0 Loss: 0.14010 Accuracy: 1.00000\n",
      "Test Loss: 0.77859 Accuracy: 0.75800\n",
      "Epoch: 190 Batch:   0 Loss: 0.13930 Accuracy: 1.00000\n",
      "Epoch: 191 Batch:   0 Loss: 0.13851 Accuracy: 1.00000\n",
      "Epoch: 192 Batch:   0 Loss: 0.13772 Accuracy: 1.00000\n",
      "Epoch: 193 Batch:   0 Loss: 0.13693 Accuracy: 1.00000\n",
      "Epoch: 194 Batch:   0 Loss: 0.13604 Accuracy: 1.00000\n",
      "Epoch: 195 Batch:   0 Loss: 0.13512 Accuracy: 1.00000\n",
      "Epoch: 196 Batch:   0 Loss: 0.13431 Accuracy: 1.00000\n",
      "Epoch: 197 Batch:   0 Loss: 0.13336 Accuracy: 1.00000\n",
      "Epoch: 198 Batch:   0 Loss: 0.13262 Accuracy: 1.00000\n",
      "Epoch: 199 Batch:   0 Loss: 0.13189 Accuracy: 1.00000\n",
      "Test Loss: 0.78353 Accuracy: 0.75900\n",
      "Epoch: 200 Batch:   0 Loss: 0.13116 Accuracy: 1.00000\n",
      "Epoch: 201 Batch:   0 Loss: 0.13043 Accuracy: 1.00000\n",
      "Epoch: 202 Batch:   0 Loss: 0.12970 Accuracy: 1.00000\n",
      "Epoch: 203 Batch:   0 Loss: 0.12896 Accuracy: 1.00000\n",
      "Epoch: 204 Batch:   0 Loss: 0.12823 Accuracy: 1.00000\n",
      "Epoch: 205 Batch:   0 Loss: 0.12751 Accuracy: 1.00000\n",
      "Epoch: 206 Batch:   0 Loss: 0.12698 Accuracy: 1.00000\n",
      "Epoch: 207 Batch:   0 Loss: 0.12608 Accuracy: 1.00000\n",
      "Epoch: 208 Batch:   0 Loss: 0.12534 Accuracy: 1.00000\n",
      "Epoch: 209 Batch:   0 Loss: 0.12462 Accuracy: 1.00000\n",
      "Test Loss: 0.78968 Accuracy: 0.76011\n",
      "Epoch: 210 Batch:   0 Loss: 0.12393 Accuracy: 1.00000\n",
      "Epoch: 211 Batch:   0 Loss: 0.12324 Accuracy: 1.00000\n",
      "Epoch: 212 Batch:   0 Loss: 0.12257 Accuracy: 1.00000\n",
      "Epoch: 213 Batch:   0 Loss: 0.12189 Accuracy: 1.00000\n",
      "Epoch: 214 Batch:   0 Loss: 0.12122 Accuracy: 1.00000\n",
      "Epoch: 215 Batch:   0 Loss: 0.12055 Accuracy: 1.00000\n",
      "Epoch: 216 Batch:   0 Loss: 0.11987 Accuracy: 1.00000\n",
      "Epoch: 217 Batch:   0 Loss: 0.11919 Accuracy: 1.00000\n",
      "Epoch: 218 Batch:   0 Loss: 0.11851 Accuracy: 1.00000\n",
      "Epoch: 219 Batch:   0 Loss: 0.11784 Accuracy: 1.00000\n",
      "Test Loss: 0.79576 Accuracy: 0.76101\n",
      "Epoch: 220 Batch:   0 Loss: 0.11716 Accuracy: 1.00000\n",
      "Epoch: 221 Batch:   0 Loss: 0.11649 Accuracy: 1.00000\n",
      "Epoch: 222 Batch:   0 Loss: 0.11583 Accuracy: 1.00000\n",
      "Epoch: 223 Batch:   0 Loss: 0.11516 Accuracy: 1.00000\n",
      "Epoch: 224 Batch:   0 Loss: 0.11450 Accuracy: 1.00000\n",
      "Epoch: 225 Batch:   0 Loss: 0.11385 Accuracy: 1.00000\n",
      "Epoch: 226 Batch:   0 Loss: 0.11319 Accuracy: 1.00000\n",
      "Epoch: 227 Batch:   0 Loss: 0.11254 Accuracy: 1.00000\n",
      "Epoch: 228 Batch:   0 Loss: 0.11190 Accuracy: 1.00000\n",
      "Epoch: 229 Batch:   0 Loss: 0.11126 Accuracy: 1.00000\n",
      "Test Loss: 0.80229 Accuracy: 0.76171\n",
      "Epoch: 230 Batch:   0 Loss: 0.11061 Accuracy: 1.00000\n",
      "Epoch: 231 Batch:   0 Loss: 0.11017 Accuracy: 1.00000\n",
      "Epoch: 232 Batch:   0 Loss: 0.10955 Accuracy: 1.00000\n",
      "Epoch: 233 Batch:   0 Loss: 0.10891 Accuracy: 1.00000\n",
      "Epoch: 234 Batch:   0 Loss: 0.10827 Accuracy: 1.00000\n",
      "Epoch: 235 Batch:   0 Loss: 0.10764 Accuracy: 1.00000\n",
      "Epoch: 236 Batch:   0 Loss: 0.10702 Accuracy: 1.00000\n",
      "Epoch: 237 Batch:   0 Loss: 0.10641 Accuracy: 1.00000\n",
      "Epoch: 238 Batch:   0 Loss: 0.10580 Accuracy: 1.00000\n",
      "Epoch: 239 Batch:   0 Loss: 0.10519 Accuracy: 1.00000\n",
      "Test Loss: 0.80980 Accuracy: 0.76171\n",
      "Epoch: 240 Batch:   0 Loss: 0.10459 Accuracy: 1.00000\n",
      "Epoch: 241 Batch:   0 Loss: 0.10400 Accuracy: 1.00000\n",
      "Epoch: 242 Batch:   0 Loss: 0.10341 Accuracy: 1.00000\n",
      "Epoch: 243 Batch:   0 Loss: 0.10282 Accuracy: 1.00000\n",
      "Epoch: 244 Batch:   0 Loss: 0.10224 Accuracy: 1.00000\n",
      "Epoch: 245 Batch:   0 Loss: 0.10167 Accuracy: 1.00000\n",
      "Epoch: 246 Batch:   0 Loss: 0.10109 Accuracy: 1.00000\n",
      "Epoch: 247 Batch:   0 Loss: 0.10053 Accuracy: 1.00000\n",
      "Epoch: 248 Batch:   0 Loss: 0.09996 Accuracy: 1.00000\n",
      "Epoch: 249 Batch:   0 Loss: 0.09940 Accuracy: 1.00000\n",
      "Test Loss: 0.81776 Accuracy: 0.76131\n",
      "Epoch: 250 Batch:   0 Loss: 0.09885 Accuracy: 1.00000\n",
      "Epoch: 251 Batch:   0 Loss: 0.09830 Accuracy: 1.00000\n",
      "Epoch: 252 Batch:   0 Loss: 0.09775 Accuracy: 1.00000\n",
      "Epoch: 253 Batch:   0 Loss: 0.09721 Accuracy: 1.00000\n",
      "Epoch: 254 Batch:   0 Loss: 0.09668 Accuracy: 1.00000\n",
      "Epoch: 255 Batch:   0 Loss: 0.09614 Accuracy: 1.00000\n",
      "Epoch: 256 Batch:   0 Loss: 0.09562 Accuracy: 1.00000\n",
      "Epoch: 257 Batch:   0 Loss: 0.09509 Accuracy: 1.00000\n",
      "Epoch: 258 Batch:   0 Loss: 0.09450 Accuracy: 1.00000\n",
      "Epoch: 259 Batch:   0 Loss: 0.09388 Accuracy: 1.00000\n",
      "Test Loss: 0.82611 Accuracy: 0.76060\n",
      "Epoch: 260 Batch:   0 Loss: 0.09338 Accuracy: 1.00000\n",
      "Epoch: 261 Batch:   0 Loss: 0.09289 Accuracy: 1.00000\n",
      "Epoch: 262 Batch:   0 Loss: 0.09240 Accuracy: 1.00000\n",
      "Epoch: 263 Batch:   0 Loss: 0.09191 Accuracy: 1.00000\n",
      "Epoch: 264 Batch:   0 Loss: 0.09143 Accuracy: 1.00000\n",
      "Epoch: 265 Batch:   0 Loss: 0.09094 Accuracy: 1.00000\n",
      "Epoch: 266 Batch:   0 Loss: 0.09046 Accuracy: 1.00000\n",
      "Epoch: 267 Batch:   0 Loss: 0.08999 Accuracy: 1.00000\n",
      "Epoch: 268 Batch:   0 Loss: 0.08951 Accuracy: 1.00000\n",
      "Epoch: 269 Batch:   0 Loss: 0.08905 Accuracy: 1.00000\n",
      "Test Loss: 0.83529 Accuracy: 0.76070\n",
      "Epoch: 270 Batch:   0 Loss: 0.08858 Accuracy: 1.00000\n",
      "Epoch: 271 Batch:   0 Loss: 0.08812 Accuracy: 1.00000\n",
      "Epoch: 272 Batch:   0 Loss: 0.08766 Accuracy: 1.00000\n",
      "Epoch: 273 Batch:   0 Loss: 0.08721 Accuracy: 1.00000\n",
      "Epoch: 274 Batch:   0 Loss: 0.08676 Accuracy: 1.00000\n",
      "Epoch: 275 Batch:   0 Loss: 0.08632 Accuracy: 1.00000\n",
      "Epoch: 276 Batch:   0 Loss: 0.08597 Accuracy: 1.00000\n",
      "Epoch: 277 Batch:   0 Loss: 0.08543 Accuracy: 1.00000\n",
      "Epoch: 278 Batch:   0 Loss: 0.08499 Accuracy: 1.00000\n",
      "Epoch: 279 Batch:   0 Loss: 0.08456 Accuracy: 1.00000\n",
      "Test Loss: 0.84504 Accuracy: 0.76020\n",
      "Epoch: 280 Batch:   0 Loss: 0.08414 Accuracy: 1.00000\n",
      "Epoch: 281 Batch:   0 Loss: 0.08372 Accuracy: 1.00000\n",
      "Epoch: 282 Batch:   0 Loss: 0.08331 Accuracy: 1.00000\n",
      "Epoch: 283 Batch:   0 Loss: 0.08289 Accuracy: 1.00000\n",
      "Epoch: 284 Batch:   0 Loss: 0.08248 Accuracy: 1.00000\n",
      "Epoch: 285 Batch:   0 Loss: 0.08215 Accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286 Batch:   0 Loss: 0.08167 Accuracy: 1.00000\n",
      "Epoch: 287 Batch:   0 Loss: 0.08127 Accuracy: 1.00000\n",
      "Epoch: 288 Batch:   0 Loss: 0.08088 Accuracy: 1.00000\n",
      "Epoch: 289 Batch:   0 Loss: 0.08049 Accuracy: 1.00000\n",
      "Test Loss: 0.85526 Accuracy: 0.75859\n",
      "Epoch: 290 Batch:   0 Loss: 0.08011 Accuracy: 1.00000\n",
      "Epoch: 291 Batch:   0 Loss: 0.07972 Accuracy: 1.00000\n",
      "Epoch: 292 Batch:   0 Loss: 0.07934 Accuracy: 1.00000\n",
      "Epoch: 293 Batch:   0 Loss: 0.07897 Accuracy: 1.00000\n",
      "Epoch: 294 Batch:   0 Loss: 0.07860 Accuracy: 1.00000\n",
      "Epoch: 295 Batch:   0 Loss: 0.07823 Accuracy: 1.00000\n",
      "Epoch: 296 Batch:   0 Loss: 0.07787 Accuracy: 1.00000\n",
      "Epoch: 297 Batch:   0 Loss: 0.07750 Accuracy: 1.00000\n",
      "Epoch: 298 Batch:   0 Loss: 0.07715 Accuracy: 1.00000\n",
      "Epoch: 299 Batch:   0 Loss: 0.07679 Accuracy: 1.00000\n",
      "Test Loss: 0.86588 Accuracy: 0.75969\n",
      "Epoch: 300 Batch:   0 Loss: 0.07644 Accuracy: 1.00000\n",
      "Epoch: 301 Batch:   0 Loss: 0.07610 Accuracy: 1.00000\n",
      "Epoch: 302 Batch:   0 Loss: 0.07575 Accuracy: 1.00000\n",
      "Epoch: 303 Batch:   0 Loss: 0.07541 Accuracy: 1.00000\n",
      "Epoch: 304 Batch:   0 Loss: 0.07507 Accuracy: 1.00000\n",
      "Epoch: 305 Batch:   0 Loss: 0.07474 Accuracy: 1.00000\n",
      "Epoch: 306 Batch:   0 Loss: 0.07441 Accuracy: 1.00000\n",
      "Epoch: 307 Batch:   0 Loss: 0.07408 Accuracy: 1.00000\n",
      "Epoch: 308 Batch:   0 Loss: 0.07376 Accuracy: 1.00000\n",
      "Epoch: 309 Batch:   0 Loss: 0.07344 Accuracy: 1.00000\n",
      "Test Loss: 0.87684 Accuracy: 0.75959\n",
      "Epoch: 310 Batch:   0 Loss: 0.07312 Accuracy: 1.00000\n",
      "Epoch: 311 Batch:   0 Loss: 0.07280 Accuracy: 1.00000\n",
      "Epoch: 312 Batch:   0 Loss: 0.07249 Accuracy: 1.00000\n",
      "Epoch: 313 Batch:   0 Loss: 0.07218 Accuracy: 1.00000\n",
      "Epoch: 314 Batch:   0 Loss: 0.07188 Accuracy: 1.00000\n",
      "Epoch: 315 Batch:   0 Loss: 0.07157 Accuracy: 1.00000\n",
      "Epoch: 316 Batch:   0 Loss: 0.07127 Accuracy: 1.00000\n",
      "Epoch: 317 Batch:   0 Loss: 0.07098 Accuracy: 1.00000\n",
      "Epoch: 318 Batch:   0 Loss: 0.07068 Accuracy: 1.00000\n",
      "Epoch: 319 Batch:   0 Loss: 0.07039 Accuracy: 1.00000\n",
      "Test Loss: 0.88810 Accuracy: 0.76029\n",
      "Epoch: 320 Batch:   0 Loss: 0.07010 Accuracy: 1.00000\n",
      "Epoch: 321 Batch:   0 Loss: 0.06982 Accuracy: 1.00000\n",
      "Epoch: 322 Batch:   0 Loss: 0.06954 Accuracy: 1.00000\n",
      "Epoch: 323 Batch:   0 Loss: 0.06926 Accuracy: 1.00000\n",
      "Epoch: 324 Batch:   0 Loss: 0.06898 Accuracy: 1.00000\n",
      "Epoch: 325 Batch:   0 Loss: 0.06871 Accuracy: 1.00000\n",
      "Epoch: 326 Batch:   0 Loss: 0.06844 Accuracy: 1.00000\n",
      "Epoch: 327 Batch:   0 Loss: 0.06817 Accuracy: 1.00000\n",
      "Epoch: 328 Batch:   0 Loss: 0.06790 Accuracy: 1.00000\n",
      "Epoch: 329 Batch:   0 Loss: 0.06764 Accuracy: 1.00000\n",
      "Test Loss: 0.89959 Accuracy: 0.76040\n",
      "Epoch: 330 Batch:   0 Loss: 0.06738 Accuracy: 1.00000\n",
      "Epoch: 331 Batch:   0 Loss: 0.06712 Accuracy: 1.00000\n",
      "Epoch: 332 Batch:   0 Loss: 0.06686 Accuracy: 1.00000\n",
      "Epoch: 333 Batch:   0 Loss: 0.06661 Accuracy: 1.00000\n",
      "Epoch: 334 Batch:   0 Loss: 0.06636 Accuracy: 1.00000\n",
      "Epoch: 335 Batch:   0 Loss: 0.06611 Accuracy: 1.00000\n",
      "Epoch: 336 Batch:   0 Loss: 0.06587 Accuracy: 1.00000\n",
      "Epoch: 337 Batch:   0 Loss: 0.06562 Accuracy: 1.00000\n",
      "Epoch: 338 Batch:   0 Loss: 0.06538 Accuracy: 1.00000\n",
      "Epoch: 339 Batch:   0 Loss: 0.06514 Accuracy: 1.00000\n",
      "Test Loss: 0.91129 Accuracy: 0.76079\n",
      "Epoch: 340 Batch:   0 Loss: 0.06491 Accuracy: 1.00000\n",
      "Epoch: 341 Batch:   0 Loss: 0.06467 Accuracy: 1.00000\n",
      "Epoch: 342 Batch:   0 Loss: 0.06444 Accuracy: 1.00000\n",
      "Epoch: 343 Batch:   0 Loss: 0.06421 Accuracy: 1.00000\n",
      "Epoch: 344 Batch:   0 Loss: 0.06399 Accuracy: 1.00000\n",
      "Epoch: 345 Batch:   0 Loss: 0.06376 Accuracy: 1.00000\n",
      "Epoch: 346 Batch:   0 Loss: 0.06354 Accuracy: 1.00000\n",
      "Epoch: 347 Batch:   0 Loss: 0.06332 Accuracy: 1.00000\n",
      "Epoch: 348 Batch:   0 Loss: 0.06310 Accuracy: 1.00000\n",
      "Epoch: 349 Batch:   0 Loss: 0.06288 Accuracy: 1.00000\n",
      "Test Loss: 0.92314 Accuracy: 0.76089\n",
      "Epoch: 350 Batch:   0 Loss: 0.06267 Accuracy: 1.00000\n",
      "Epoch: 351 Batch:   0 Loss: 0.06246 Accuracy: 1.00000\n",
      "Epoch: 352 Batch:   0 Loss: 0.06225 Accuracy: 1.00000\n",
      "Epoch: 353 Batch:   0 Loss: 0.06204 Accuracy: 1.00000\n",
      "Epoch: 354 Batch:   0 Loss: 0.06184 Accuracy: 1.00000\n",
      "Epoch: 355 Batch:   0 Loss: 0.06163 Accuracy: 1.00000\n",
      "Epoch: 356 Batch:   0 Loss: 0.06143 Accuracy: 1.00000\n",
      "Epoch: 357 Batch:   0 Loss: 0.06123 Accuracy: 1.00000\n",
      "Epoch: 358 Batch:   0 Loss: 0.06103 Accuracy: 1.00000\n",
      "Epoch: 359 Batch:   0 Loss: 0.06084 Accuracy: 1.00000\n",
      "Test Loss: 0.93513 Accuracy: 0.76110\n",
      "Epoch: 360 Batch:   0 Loss: 0.06065 Accuracy: 1.00000\n",
      "Epoch: 361 Batch:   0 Loss: 0.06045 Accuracy: 1.00000\n",
      "Epoch: 362 Batch:   0 Loss: 0.06026 Accuracy: 1.00000\n",
      "Epoch: 363 Batch:   0 Loss: 0.06008 Accuracy: 1.00000\n",
      "Epoch: 364 Batch:   0 Loss: 0.05989 Accuracy: 1.00000\n",
      "Epoch: 365 Batch:   0 Loss: 0.05971 Accuracy: 1.00000\n",
      "Epoch: 366 Batch:   0 Loss: 0.05952 Accuracy: 1.00000\n",
      "Epoch: 367 Batch:   0 Loss: 0.05934 Accuracy: 1.00000\n",
      "Epoch: 368 Batch:   0 Loss: 0.05917 Accuracy: 1.00000\n",
      "Epoch: 369 Batch:   0 Loss: 0.05899 Accuracy: 1.00000\n",
      "Test Loss: 0.94721 Accuracy: 0.76169\n",
      "Epoch: 370 Batch:   0 Loss: 0.05881 Accuracy: 1.00000\n",
      "Epoch: 371 Batch:   0 Loss: 0.05864 Accuracy: 1.00000\n",
      "Epoch: 372 Batch:   0 Loss: 0.05847 Accuracy: 1.00000\n",
      "Epoch: 373 Batch:   0 Loss: 0.05830 Accuracy: 1.00000\n",
      "Epoch: 374 Batch:   0 Loss: 0.05813 Accuracy: 1.00000\n",
      "Epoch: 375 Batch:   0 Loss: 0.05796 Accuracy: 1.00000\n",
      "Epoch: 376 Batch:   0 Loss: 0.05780 Accuracy: 1.00000\n",
      "Epoch: 377 Batch:   0 Loss: 0.05763 Accuracy: 1.00000\n",
      "Epoch: 378 Batch:   0 Loss: 0.05747 Accuracy: 1.00000\n",
      "Epoch: 379 Batch:   0 Loss: 0.05731 Accuracy: 1.00000\n",
      "Test Loss: 0.95936 Accuracy: 0.76230\n",
      "Epoch: 380 Batch:   0 Loss: 0.05715 Accuracy: 1.00000\n",
      "Epoch: 381 Batch:   0 Loss: 0.05700 Accuracy: 1.00000\n",
      "Epoch: 382 Batch:   0 Loss: 0.05684 Accuracy: 1.00000\n",
      "Epoch: 383 Batch:   0 Loss: 0.05669 Accuracy: 1.00000\n",
      "Epoch: 384 Batch:   0 Loss: 0.05653 Accuracy: 1.00000\n",
      "Epoch: 385 Batch:   0 Loss: 0.05638 Accuracy: 1.00000\n",
      "Epoch: 386 Batch:   0 Loss: 0.05623 Accuracy: 1.00000\n",
      "Epoch: 387 Batch:   0 Loss: 0.05608 Accuracy: 1.00000\n",
      "Epoch: 388 Batch:   0 Loss: 0.05594 Accuracy: 1.00000\n",
      "Epoch: 389 Batch:   0 Loss: 0.05579 Accuracy: 1.00000\n",
      "Test Loss: 0.97154 Accuracy: 0.76169\n",
      "Epoch: 390 Batch:   0 Loss: 0.05565 Accuracy: 1.00000\n",
      "Epoch: 391 Batch:   0 Loss: 0.05551 Accuracy: 1.00000\n",
      "Epoch: 392 Batch:   0 Loss: 0.05536 Accuracy: 1.00000\n",
      "Epoch: 393 Batch:   0 Loss: 0.05523 Accuracy: 1.00000\n",
      "Epoch: 394 Batch:   0 Loss: 0.05509 Accuracy: 1.00000\n",
      "Epoch: 395 Batch:   0 Loss: 0.05495 Accuracy: 1.00000\n",
      "Epoch: 396 Batch:   0 Loss: 0.05481 Accuracy: 1.00000\n",
      "Epoch: 397 Batch:   0 Loss: 0.05468 Accuracy: 1.00000\n",
      "Epoch: 398 Batch:   0 Loss: 0.05455 Accuracy: 1.00000\n",
      "Epoch: 399 Batch:   0 Loss: 0.05442 Accuracy: 1.00000\n",
      "Test Loss: 0.98374 Accuracy: 0.76190\n",
      "Epoch: 400 Batch:   0 Loss: 0.05429 Accuracy: 1.00000\n",
      "Epoch: 401 Batch:   0 Loss: 0.05416 Accuracy: 1.00000\n",
      "Epoch: 402 Batch:   0 Loss: 0.05403 Accuracy: 1.00000\n",
      "Epoch: 403 Batch:   0 Loss: 0.05390 Accuracy: 1.00000\n",
      "Epoch: 404 Batch:   0 Loss: 0.05378 Accuracy: 1.00000\n",
      "Epoch: 405 Batch:   0 Loss: 0.05365 Accuracy: 1.00000\n",
      "Epoch: 406 Batch:   0 Loss: 0.05353 Accuracy: 1.00000\n",
      "Epoch: 407 Batch:   0 Loss: 0.05341 Accuracy: 1.00000\n",
      "Epoch: 408 Batch:   0 Loss: 0.05329 Accuracy: 1.00000\n",
      "Epoch: 409 Batch:   0 Loss: 0.05317 Accuracy: 1.00000\n",
      "Test Loss: 0.99593 Accuracy: 0.76280\n",
      "Epoch: 410 Batch:   0 Loss: 0.05305 Accuracy: 1.00000\n",
      "Epoch: 411 Batch:   0 Loss: 0.05293 Accuracy: 1.00000\n",
      "Epoch: 412 Batch:   0 Loss: 0.05282 Accuracy: 1.00000\n",
      "Epoch: 413 Batch:   0 Loss: 0.05270 Accuracy: 1.00000\n",
      "Epoch: 414 Batch:   0 Loss: 0.05259 Accuracy: 1.00000\n",
      "Epoch: 415 Batch:   0 Loss: 0.05248 Accuracy: 1.00000\n",
      "Epoch: 416 Batch:   0 Loss: 0.05237 Accuracy: 1.00000\n",
      "Epoch: 417 Batch:   0 Loss: 0.05226 Accuracy: 1.00000\n",
      "Epoch: 418 Batch:   0 Loss: 0.05215 Accuracy: 1.00000\n",
      "Epoch: 419 Batch:   0 Loss: 0.05204 Accuracy: 1.00000\n",
      "Test Loss: 1.00808 Accuracy: 0.76270\n",
      "Epoch: 420 Batch:   0 Loss: 0.05193 Accuracy: 1.00000\n",
      "Epoch: 421 Batch:   0 Loss: 0.05182 Accuracy: 1.00000\n",
      "Epoch: 422 Batch:   0 Loss: 0.05172 Accuracy: 1.00000\n",
      "Epoch: 423 Batch:   0 Loss: 0.05162 Accuracy: 1.00000\n",
      "Epoch: 424 Batch:   0 Loss: 0.05151 Accuracy: 1.00000\n",
      "Epoch: 425 Batch:   0 Loss: 0.05141 Accuracy: 1.00000\n",
      "Epoch: 426 Batch:   0 Loss: 0.05131 Accuracy: 1.00000\n",
      "Epoch: 427 Batch:   0 Loss: 0.05121 Accuracy: 1.00000\n",
      "Epoch: 428 Batch:   0 Loss: 0.05111 Accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429 Batch:   0 Loss: 0.05101 Accuracy: 1.00000\n",
      "Test Loss: 1.02016 Accuracy: 0.76280\n",
      "Epoch: 430 Batch:   0 Loss: 0.05092 Accuracy: 1.00000\n",
      "Epoch: 431 Batch:   0 Loss: 0.05082 Accuracy: 1.00000\n",
      "Epoch: 432 Batch:   0 Loss: 0.05072 Accuracy: 1.00000\n",
      "Epoch: 433 Batch:   0 Loss: 0.05063 Accuracy: 1.00000\n",
      "Epoch: 434 Batch:   0 Loss: 0.05054 Accuracy: 1.00000\n",
      "Epoch: 435 Batch:   0 Loss: 0.05044 Accuracy: 1.00000\n",
      "Epoch: 436 Batch:   0 Loss: 0.05035 Accuracy: 1.00000\n",
      "Epoch: 437 Batch:   0 Loss: 0.05026 Accuracy: 1.00000\n",
      "Epoch: 438 Batch:   0 Loss: 0.05017 Accuracy: 1.00000\n",
      "Epoch: 439 Batch:   0 Loss: 0.05008 Accuracy: 1.00000\n",
      "Test Loss: 1.03217 Accuracy: 0.76239\n",
      "Epoch: 440 Batch:   0 Loss: 0.04999 Accuracy: 1.00000\n",
      "Epoch: 441 Batch:   0 Loss: 0.04991 Accuracy: 1.00000\n",
      "Epoch: 442 Batch:   0 Loss: 0.04982 Accuracy: 1.00000\n",
      "Epoch: 443 Batch:   0 Loss: 0.04973 Accuracy: 1.00000\n",
      "Epoch: 444 Batch:   0 Loss: 0.04965 Accuracy: 1.00000\n",
      "Epoch: 445 Batch:   0 Loss: 0.04956 Accuracy: 1.00000\n",
      "Epoch: 446 Batch:   0 Loss: 0.04948 Accuracy: 1.00000\n",
      "Epoch: 447 Batch:   0 Loss: 0.04940 Accuracy: 1.00000\n",
      "Epoch: 448 Batch:   0 Loss: 0.04932 Accuracy: 1.00000\n",
      "Epoch: 449 Batch:   0 Loss: 0.04924 Accuracy: 1.00000\n",
      "Test Loss: 1.04408 Accuracy: 0.76149\n",
      "Epoch: 450 Batch:   0 Loss: 0.04916 Accuracy: 1.00000\n",
      "Epoch: 451 Batch:   0 Loss: 0.04908 Accuracy: 1.00000\n",
      "Epoch: 452 Batch:   0 Loss: 0.04900 Accuracy: 1.00000\n",
      "Epoch: 453 Batch:   0 Loss: 0.04892 Accuracy: 1.00000\n",
      "Epoch: 454 Batch:   0 Loss: 0.04884 Accuracy: 1.00000\n",
      "Epoch: 455 Batch:   0 Loss: 0.04877 Accuracy: 1.00000\n",
      "Epoch: 456 Batch:   0 Loss: 0.04869 Accuracy: 1.00000\n",
      "Epoch: 457 Batch:   0 Loss: 0.04862 Accuracy: 1.00000\n",
      "Epoch: 458 Batch:   0 Loss: 0.04854 Accuracy: 1.00000\n",
      "Epoch: 459 Batch:   0 Loss: 0.04847 Accuracy: 1.00000\n",
      "Test Loss: 1.05588 Accuracy: 0.76109\n",
      "Epoch: 460 Batch:   0 Loss: 0.04840 Accuracy: 1.00000\n",
      "Epoch: 461 Batch:   0 Loss: 0.04832 Accuracy: 1.00000\n",
      "Epoch: 462 Batch:   0 Loss: 0.04825 Accuracy: 1.00000\n",
      "Epoch: 463 Batch:   0 Loss: 0.04818 Accuracy: 1.00000\n",
      "Epoch: 464 Batch:   0 Loss: 0.04811 Accuracy: 1.00000\n",
      "Epoch: 465 Batch:   0 Loss: 0.04804 Accuracy: 1.00000\n",
      "Epoch: 466 Batch:   0 Loss: 0.04797 Accuracy: 1.00000\n",
      "Epoch: 467 Batch:   0 Loss: 0.04791 Accuracy: 1.00000\n",
      "Epoch: 468 Batch:   0 Loss: 0.04784 Accuracy: 1.00000\n",
      "Epoch: 469 Batch:   0 Loss: 0.04777 Accuracy: 1.00000\n",
      "Test Loss: 1.06755 Accuracy: 0.76129\n",
      "Epoch: 470 Batch:   0 Loss: 0.04770 Accuracy: 1.00000\n",
      "Epoch: 471 Batch:   0 Loss: 0.04764 Accuracy: 1.00000\n",
      "Epoch: 472 Batch:   0 Loss: 0.04757 Accuracy: 1.00000\n",
      "Epoch: 473 Batch:   0 Loss: 0.04751 Accuracy: 1.00000\n",
      "Epoch: 474 Batch:   0 Loss: 0.04745 Accuracy: 1.00000\n",
      "Epoch: 475 Batch:   0 Loss: 0.04738 Accuracy: 1.00000\n",
      "Epoch: 476 Batch:   0 Loss: 0.04732 Accuracy: 1.00000\n",
      "Epoch: 477 Batch:   0 Loss: 0.04726 Accuracy: 1.00000\n",
      "Epoch: 478 Batch:   0 Loss: 0.04720 Accuracy: 1.00000\n",
      "Epoch: 479 Batch:   0 Loss: 0.04714 Accuracy: 1.00000\n",
      "Test Loss: 1.07909 Accuracy: 0.76048\n",
      "Epoch: 480 Batch:   0 Loss: 0.04707 Accuracy: 1.00000\n",
      "Epoch: 481 Batch:   0 Loss: 0.04701 Accuracy: 1.00000\n",
      "Epoch: 482 Batch:   0 Loss: 0.04696 Accuracy: 1.00000\n",
      "Epoch: 483 Batch:   0 Loss: 0.04690 Accuracy: 1.00000\n",
      "Epoch: 484 Batch:   0 Loss: 0.04684 Accuracy: 1.00000\n",
      "Epoch: 485 Batch:   0 Loss: 0.04678 Accuracy: 1.00000\n",
      "Epoch: 486 Batch:   0 Loss: 0.04672 Accuracy: 1.00000\n",
      "Epoch: 487 Batch:   0 Loss: 0.04667 Accuracy: 1.00000\n",
      "Epoch: 488 Batch:   0 Loss: 0.04661 Accuracy: 1.00000\n",
      "Epoch: 489 Batch:   0 Loss: 0.04656 Accuracy: 1.00000\n",
      "Test Loss: 1.09048 Accuracy: 0.76028\n",
      "Epoch: 490 Batch:   0 Loss: 0.04650 Accuracy: 1.00000\n",
      "Epoch: 491 Batch:   0 Loss: 0.04645 Accuracy: 1.00000\n",
      "Epoch: 492 Batch:   0 Loss: 0.04639 Accuracy: 1.00000\n",
      "Epoch: 493 Batch:   0 Loss: 0.04634 Accuracy: 1.00000\n",
      "Epoch: 494 Batch:   0 Loss: 0.04629 Accuracy: 1.00000\n",
      "Epoch: 495 Batch:   0 Loss: 0.04623 Accuracy: 1.00000\n",
      "Epoch: 496 Batch:   0 Loss: 0.04618 Accuracy: 1.00000\n",
      "Epoch: 497 Batch:   0 Loss: 0.04613 Accuracy: 1.00000\n",
      "Epoch: 498 Batch:   0 Loss: 0.04608 Accuracy: 1.00000\n",
      "Epoch: 499 Batch:   0 Loss: 0.04603 Accuracy: 1.00000\n",
      "Test Loss: 1.10172 Accuracy: 0.76048\n",
      "Epoch: 500 Batch:   0 Loss: 0.04598 Accuracy: 1.00000\n",
      "Epoch: 501 Batch:   0 Loss: 0.04593 Accuracy: 1.00000\n",
      "Epoch: 502 Batch:   0 Loss: 0.04588 Accuracy: 1.00000\n",
      "Epoch: 503 Batch:   0 Loss: 0.04583 Accuracy: 1.00000\n",
      "Epoch: 504 Batch:   0 Loss: 0.04578 Accuracy: 1.00000\n",
      "Epoch: 505 Batch:   0 Loss: 0.04573 Accuracy: 1.00000\n",
      "Epoch: 506 Batch:   0 Loss: 0.04569 Accuracy: 1.00000\n",
      "Epoch: 507 Batch:   0 Loss: 0.04564 Accuracy: 1.00000\n",
      "Epoch: 508 Batch:   0 Loss: 0.04559 Accuracy: 1.00000\n",
      "Epoch: 509 Batch:   0 Loss: 0.04555 Accuracy: 1.00000\n",
      "Test Loss: 1.11279 Accuracy: 0.76038\n",
      "Epoch: 510 Batch:   0 Loss: 0.04550 Accuracy: 1.00000\n",
      "Epoch: 511 Batch:   0 Loss: 0.04545 Accuracy: 1.00000\n",
      "Epoch: 512 Batch:   0 Loss: 0.04541 Accuracy: 1.00000\n",
      "Epoch: 513 Batch:   0 Loss: 0.04536 Accuracy: 1.00000\n",
      "Epoch: 514 Batch:   0 Loss: 0.04532 Accuracy: 1.00000\n",
      "Epoch: 515 Batch:   0 Loss: 0.04528 Accuracy: 1.00000\n",
      "Epoch: 516 Batch:   0 Loss: 0.04523 Accuracy: 1.00000\n",
      "Epoch: 517 Batch:   0 Loss: 0.04519 Accuracy: 1.00000\n",
      "Epoch: 518 Batch:   0 Loss: 0.04515 Accuracy: 1.00000\n",
      "Epoch: 519 Batch:   0 Loss: 0.04510 Accuracy: 1.00000\n",
      "Test Loss: 1.12370 Accuracy: 0.76018\n",
      "Epoch: 520 Batch:   0 Loss: 0.04506 Accuracy: 1.00000\n",
      "Epoch: 521 Batch:   0 Loss: 0.04502 Accuracy: 1.00000\n",
      "Epoch: 522 Batch:   0 Loss: 0.04498 Accuracy: 1.00000\n",
      "Epoch: 523 Batch:   0 Loss: 0.04494 Accuracy: 1.00000\n",
      "Epoch: 524 Batch:   0 Loss: 0.04490 Accuracy: 1.00000\n",
      "Epoch: 525 Batch:   0 Loss: 0.04486 Accuracy: 1.00000\n",
      "Epoch: 526 Batch:   0 Loss: 0.04482 Accuracy: 1.00000\n",
      "Epoch: 527 Batch:   0 Loss: 0.04478 Accuracy: 1.00000\n",
      "Epoch: 528 Batch:   0 Loss: 0.04474 Accuracy: 1.00000\n",
      "Epoch: 529 Batch:   0 Loss: 0.04470 Accuracy: 1.00000\n",
      "Test Loss: 1.13443 Accuracy: 0.75998\n",
      "Epoch: 530 Batch:   0 Loss: 0.04466 Accuracy: 1.00000\n",
      "Epoch: 531 Batch:   0 Loss: 0.04462 Accuracy: 1.00000\n",
      "Epoch: 532 Batch:   0 Loss: 0.04459 Accuracy: 1.00000\n",
      "Epoch: 533 Batch:   0 Loss: 0.04455 Accuracy: 1.00000\n",
      "Epoch: 534 Batch:   0 Loss: 0.04451 Accuracy: 1.00000\n",
      "Epoch: 535 Batch:   0 Loss: 0.04447 Accuracy: 1.00000\n",
      "Epoch: 536 Batch:   0 Loss: 0.04444 Accuracy: 1.00000\n",
      "Epoch: 537 Batch:   0 Loss: 0.04440 Accuracy: 1.00000\n",
      "Epoch: 538 Batch:   0 Loss: 0.04437 Accuracy: 1.00000\n",
      "Epoch: 539 Batch:   0 Loss: 0.04433 Accuracy: 1.00000\n",
      "Test Loss: 1.14498 Accuracy: 0.75947\n",
      "Epoch: 540 Batch:   0 Loss: 0.04430 Accuracy: 1.00000\n",
      "Epoch: 541 Batch:   0 Loss: 0.04426 Accuracy: 1.00000\n",
      "Epoch: 542 Batch:   0 Loss: 0.04423 Accuracy: 1.00000\n",
      "Epoch: 543 Batch:   0 Loss: 0.04419 Accuracy: 1.00000\n",
      "Epoch: 544 Batch:   0 Loss: 0.04416 Accuracy: 1.00000\n",
      "Epoch: 545 Batch:   0 Loss: 0.04412 Accuracy: 1.00000\n",
      "Epoch: 546 Batch:   0 Loss: 0.04409 Accuracy: 1.00000\n",
      "Epoch: 547 Batch:   0 Loss: 0.04406 Accuracy: 1.00000\n",
      "Epoch: 548 Batch:   0 Loss: 0.04402 Accuracy: 1.00000\n",
      "Epoch: 549 Batch:   0 Loss: 0.04399 Accuracy: 1.00000\n",
      "Test Loss: 1.15533 Accuracy: 0.75917\n",
      "Epoch: 550 Batch:   0 Loss: 0.04396 Accuracy: 1.00000\n",
      "Epoch: 551 Batch:   0 Loss: 0.04393 Accuracy: 1.00000\n",
      "Epoch: 552 Batch:   0 Loss: 0.04390 Accuracy: 1.00000\n",
      "Epoch: 553 Batch:   0 Loss: 0.04386 Accuracy: 1.00000\n",
      "Epoch: 554 Batch:   0 Loss: 0.04383 Accuracy: 1.00000\n",
      "Epoch: 555 Batch:   0 Loss: 0.04380 Accuracy: 1.00000\n",
      "Epoch: 556 Batch:   0 Loss: 0.04377 Accuracy: 1.00000\n",
      "Epoch: 557 Batch:   0 Loss: 0.04374 Accuracy: 1.00000\n",
      "Epoch: 558 Batch:   0 Loss: 0.04371 Accuracy: 1.00000\n",
      "Epoch: 559 Batch:   0 Loss: 0.04368 Accuracy: 1.00000\n",
      "Test Loss: 1.16549 Accuracy: 0.75867\n",
      "Epoch: 560 Batch:   0 Loss: 0.04365 Accuracy: 1.00000\n",
      "Epoch: 561 Batch:   0 Loss: 0.04362 Accuracy: 1.00000\n",
      "Epoch: 562 Batch:   0 Loss: 0.04359 Accuracy: 1.00000\n",
      "Epoch: 563 Batch:   0 Loss: 0.04356 Accuracy: 1.00000\n",
      "Epoch: 564 Batch:   0 Loss: 0.04353 Accuracy: 1.00000\n",
      "Epoch: 565 Batch:   0 Loss: 0.04351 Accuracy: 1.00000\n",
      "Epoch: 566 Batch:   0 Loss: 0.04348 Accuracy: 1.00000\n",
      "Epoch: 567 Batch:   0 Loss: 0.04345 Accuracy: 1.00000\n",
      "Epoch: 568 Batch:   0 Loss: 0.04342 Accuracy: 1.00000\n",
      "Epoch: 569 Batch:   0 Loss: 0.04339 Accuracy: 1.00000\n",
      "Test Loss: 1.17543 Accuracy: 0.75806\n",
      "Epoch: 570 Batch:   0 Loss: 0.04337 Accuracy: 1.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 571 Batch:   0 Loss: 0.04334 Accuracy: 1.00000\n",
      "Epoch: 572 Batch:   0 Loss: 0.04331 Accuracy: 1.00000\n",
      "Epoch: 573 Batch:   0 Loss: 0.04329 Accuracy: 1.00000\n",
      "Epoch: 574 Batch:   0 Loss: 0.04326 Accuracy: 1.00000\n",
      "Epoch: 575 Batch:   0 Loss: 0.04323 Accuracy: 1.00000\n",
      "Epoch: 576 Batch:   0 Loss: 0.04321 Accuracy: 1.00000\n",
      "Epoch: 577 Batch:   0 Loss: 0.04318 Accuracy: 1.00000\n",
      "Epoch: 578 Batch:   0 Loss: 0.04316 Accuracy: 1.00000\n",
      "Epoch: 579 Batch:   0 Loss: 0.04313 Accuracy: 1.00000\n",
      "Test Loss: 1.18517 Accuracy: 0.75796\n",
      "Epoch: 580 Batch:   0 Loss: 0.04311 Accuracy: 1.00000\n",
      "Epoch: 581 Batch:   0 Loss: 0.04308 Accuracy: 1.00000\n",
      "Epoch: 582 Batch:   0 Loss: 0.04306 Accuracy: 1.00000\n",
      "Epoch: 583 Batch:   0 Loss: 0.04303 Accuracy: 1.00000\n",
      "Epoch: 584 Batch:   0 Loss: 0.04301 Accuracy: 1.00000\n",
      "Epoch: 585 Batch:   0 Loss: 0.04298 Accuracy: 1.00000\n",
      "Epoch: 586 Batch:   0 Loss: 0.04296 Accuracy: 1.00000\n",
      "Epoch: 587 Batch:   0 Loss: 0.04293 Accuracy: 1.00000\n",
      "Epoch: 588 Batch:   0 Loss: 0.04291 Accuracy: 1.00000\n",
      "Epoch: 589 Batch:   0 Loss: 0.04289 Accuracy: 1.00000\n",
      "Test Loss: 1.19470 Accuracy: 0.75837\n",
      "Epoch: 590 Batch:   0 Loss: 0.04286 Accuracy: 1.00000\n",
      "Epoch: 591 Batch:   0 Loss: 0.04284 Accuracy: 1.00000\n",
      "Epoch: 592 Batch:   0 Loss: 0.04282 Accuracy: 1.00000\n",
      "Epoch: 593 Batch:   0 Loss: 0.04280 Accuracy: 1.00000\n",
      "Epoch: 594 Batch:   0 Loss: 0.04277 Accuracy: 1.00000\n",
      "Epoch: 595 Batch:   0 Loss: 0.04275 Accuracy: 1.00000\n",
      "Epoch: 596 Batch:   0 Loss: 0.04273 Accuracy: 1.00000\n",
      "Epoch: 597 Batch:   0 Loss: 0.04271 Accuracy: 1.00000\n",
      "Epoch: 598 Batch:   0 Loss: 0.04269 Accuracy: 1.00000\n",
      "Epoch: 599 Batch:   0 Loss: 0.04266 Accuracy: 1.00000\n",
      "Test Loss: 1.20402 Accuracy: 0.75756\n"
     ]
    }
   ],
   "source": [
    "# Setup input and train protoNN\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uypXmz1QJ2h"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.671507Z",
     "start_time": "2018-08-15T13:07:22.645050Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96103,
     "status": "ok",
     "timestamp": 1567154584845,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "VYxefAD3QJ2j",
    "outputId": "f8b711a1-bae4-4bf5-9a62-935838844601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.7575392\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91542,
     "status": "ok",
     "timestamp": 1567154584848,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "yJM9puhcQJ2t",
    "outputId": "d8e743e0-1068-4681-e3bc-3b650ab66a3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3533 1441]\n",
      " [ 971 4003]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.78441   0.71029   0.74552      4974\n",
      "           1    0.73530   0.80478   0.76848      4974\n",
      "\n",
      "    accuracy                        0.75754      9948\n",
      "   macro avg    0.75986   0.75754   0.75700      9948\n",
      "weighted avg    0.75986   0.75754   0.75700      9948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8047848813831926"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7102935263369522"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = (TP + TN) / (TP + TN + FP + FN) \n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)\n",
    "# sensitivity = TP / (TP + FN) \n",
    "# specificity = TN / (TN + FP) \n",
    "# positive predictive value (PPV) = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "protoNN_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ProtoNN",
   "language": "python",
   "name": "protonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
