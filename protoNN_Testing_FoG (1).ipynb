{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwwtn5iEQJ1M"
   },
   "source": [
    "# ProtoNN in Tensorflow\n",
    "\n",
    "This is a simple notebook that illustrates the usage of Tensorflow implementation of ProtoNN. We are using the USPS dataset. Please refer to `fetch_usps.py` for more details on downloading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.223951Z",
     "start_time": "2018-08-15T13:06:09.303454Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dJBVr2b7QJ1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nadit\\anaconda3\\envs\\ProtoNN\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "#sys.path.insert(0, '../../')\n",
    "# from edgeml.trainer.protoNNTrainer import ProtoNNTrainer\n",
    "# from edgeml.graph.protoNN import ProtoNN\n",
    "# import edgeml.utils as utils\n",
    "# import helpermethods as helper\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "sys.path.append(r\"D:\\programming\\practice\\research\\protoNN\\EdgeML\\examples\\tf\\ProtoNN\")\n",
    "import helpermethods as helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HYPERPARAMETER\n",
    "hyper = {'REG_W': 2.0012967740413277e-06,\n",
    " 'REG_B': 2.0861588167171992e-05,\n",
    " 'REG_Z': 2.0478935633536105e-05,\n",
    " 'SPAR_W': 0.8367487762320901,\n",
    " 'SPAR_B': 0.9791350117492328,\n",
    " 'SPAR_Z': 0.9505125522850648,\n",
    " 'LEARNING_RATE': 0.00012654644856451654,\n",
    " 'NUM_EPOCHS': 355}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mxqvfwWQtQ-s"
   },
   "source": [
    "# Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cT-KokQSQiS6"
   },
   "outputs": [],
   "source": [
    "#helper methods\n",
    "sys.path.insert(0, '../')\n",
    "import argparse\n",
    "\n",
    "\n",
    "def getModelSize(matrixList, sparcityList, expected=True, bytesPerVar=4):\n",
    "    '''\n",
    "    expected: Expected size according to the parameters set. The number of\n",
    "        zeros could actually be more than that is required to satisfy the\n",
    "        sparsity constraint.\n",
    "    '''\n",
    "    nnzList, sizeList, isSparseList = [], [], []\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        assert A.ndim == 2\n",
    "        assert s >= 0\n",
    "        assert s <= 1\n",
    "        nnz, size, sparse = countnnZ(A, s, bytesPerVar=bytesPerVar)\n",
    "        nnzList.append(nnz)\n",
    "        sizeList.append(size)\n",
    "        hasSparse = (hasSparse or sparse)\n",
    "\n",
    "    totalnnZ = np.sum(nnzList)\n",
    "    totalSize = np.sum(sizeList)\n",
    "    if expected:\n",
    "        return totalnnZ, totalSize, hasSparse\n",
    "    numNonZero = 0\n",
    "    totalSize = 0\n",
    "    hasSparse = False\n",
    "    for i in range(len(matrixList)):\n",
    "        A, s = matrixList[i], sparcityList[i]\n",
    "        numNonZero_ = np.count_nonzero(A)\n",
    "        numNonZero += numNonZero_\n",
    "        hasSparse = (hasSparse or (s < 0.5))\n",
    "        if s <= 0.5:\n",
    "            totalSize += numNonZero_ * 2 * bytesPerVar\n",
    "        else:\n",
    "            totalSize += A.size * bytesPerVar\n",
    "    return numNonZero, totalSize, hasSparse\n",
    "\n",
    "\n",
    "def getGamma(gammaInit, projectionDim, dataDim, numPrototypes, x_train):\n",
    "    if gammaInit is None:\n",
    "        print(\"Using median heuristic to estimate gamma.\")\n",
    "        gamma, W, B = medianHeuristic(x_train, projectionDim,\n",
    "                                            numPrototypes)\n",
    "        print(\"Gamma estimate is: %f\" % gamma)\n",
    "        return W, B, gamma\n",
    "    return None, None, gammaInit\n",
    "\n",
    "\n",
    "def preprocessData(dataDir,w):\n",
    "    '''\n",
    "    Loads data from the dataDir and does some initial preprocessing\n",
    "    steps. Data is assumed to be contained in two files,\n",
    "    train.npy and test.npy. Each containing a 2D numpy array of dimension\n",
    "    [numberOfExamples, numberOfFeatures + 1]. The first column of each\n",
    "    matrix is assumed to contain label information.\n",
    "\n",
    "    For an N-Class problem, we assume the labels are integers from 0 through\n",
    "    N-1.\n",
    "    '''\n",
    "    # Uncomment for usual training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "    # Uncomment for time domain training data\n",
    "    train = np.load(dataDir + '/ttrain_'+str(w)+'.npy')\n",
    "    test = np.load(dataDir + '/ttest_'+str(w)+'.npy')\n",
    "    # Uncomment for 1 sensordrop training data\n",
    "    # train = np.load(dataDir + '/train_'+str(w)+'.npy')\n",
    "    # test = np.load(dataDir + '/test_'+str(w)+'.npy')\n",
    "\n",
    "    dataDimension = int(train.shape[1]) - 1\n",
    "    x_train = train[:, 1:dataDimension + 1]\n",
    "    y_train_ = train[:, 0]\n",
    "    x_test = test[:, 1:dataDimension + 1]\n",
    "    y_test_ = test[:, 0]\n",
    "\n",
    "    numClasses = max(y_train_) - min(y_train_) + 1\n",
    "    numClasses = max(numClasses, max(y_test_) - min(y_test_) + 1)\n",
    "    numClasses = int(numClasses)\n",
    "\n",
    "    # mean-var\n",
    "    mean = np.mean(x_train, 0)\n",
    "    std = np.std(x_train, 0)\n",
    "    std[std[:] < 0.000001] = 1\n",
    "    x_train = (x_train - mean) / std\n",
    "    x_test = (x_test - mean) / std\n",
    "\n",
    "    # one hot y-train\n",
    "    lab = y_train_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_train.shape[0], numClasses))\n",
    "    lab_[np.arange(x_train.shape[0]), lab] = 1\n",
    "    y_train = lab_\n",
    "\n",
    "    # one hot y-test\n",
    "    lab = y_test_.astype('uint8')\n",
    "    lab = np.array(lab) - min(lab)\n",
    "    lab_ = np.zeros((x_test.shape[0], numClasses))\n",
    "    lab_[np.arange(x_test.shape[0]), lab] = 1\n",
    "    y_test = lab_\n",
    "\n",
    "    return dataDimension, numClasses, x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "def getProtoNNArgs():\n",
    "    def checkIntPos(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkIntNneg(value):\n",
    "        ivalue = int(value)\n",
    "        if ivalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg int value\" % value)\n",
    "        return ivalue\n",
    "\n",
    "    def checkFloatNneg(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue < 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid non-neg float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    def checkFloatPos(value):\n",
    "        fvalue = float(value)\n",
    "        if fvalue <= 0:\n",
    "            raise argparse.ArgumentTypeError(\n",
    "                \"%s is an invalid positive float value\" % value)\n",
    "        return fvalue\n",
    "\n",
    "    '''\n",
    "    Parse protoNN commandline arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Hyperparameters for ProtoNN Algorithm')\n",
    "\n",
    "    msg = 'Data directory containing train and test data. The '\n",
    "    msg += 'data is assumed to be saved as 2-D numpy matrices with '\n",
    "    msg += 'names `train.npy` and `test.npy`, of dimensions\\n'\n",
    "    msg += '\\t[numberOfInstances, numberOfFeatures + 1].\\n'\n",
    "    msg += 'The first column of each file is assumed to contain label information.'\n",
    "    msg += ' For a N-class problem, labels are assumed to be integers from 0 to'\n",
    "    msg += ' N-1 (inclusive).'\n",
    "    parser.add_argument('-d', '--data-dir', required=True, help=msg)\n",
    "    parser.add_argument('-l', '--projection-dim', type=checkIntPos, default=10,\n",
    "                        help='Projection Dimension.')\n",
    "    parser.add_argument('-p', '--num-prototypes', type=checkIntPos, default=20,\n",
    "                        help='Number of prototypes.')\n",
    "    parser.add_argument('-g', '--gamma', type=checkFloatPos, default=None,\n",
    "                        help='Gamma for Gaussian kernel. If not provided, ' +\n",
    "                        'median heuristic will be used to estimate gamma.')\n",
    "\n",
    "    parser.add_argument('-e', '--epochs', type=checkIntPos, default=100,\n",
    "                        help='Total training epochs.')\n",
    "    parser.add_argument('-b', '--batch-size', type=checkIntPos, default=32,\n",
    "                        help='Batch size for each pass.')\n",
    "    parser.add_argument('-r', '--learning-rate', type=checkFloatPos,\n",
    "                        default=0.001,\n",
    "                        help='Initial Learning rate for ADAM Optimizer.')\n",
    "\n",
    "    parser.add_argument('-rW', type=float, default=0.000,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter W ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rB', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        ' parameter B ' + '(default = 0.0).')\n",
    "    parser.add_argument('-rZ', type=float, default=0.00,\n",
    "                        help='Coefficient for l2 regularizer for predictor' +\n",
    "                        'parameter Z ' +\n",
    "                        '(default = 0.0).')\n",
    "\n",
    "    parser.add_argument('-sW', type=float, default=1.000,\n",
    "                        help='Sparsity constraint for predictor parameter W ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sB', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter B ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-sZ', type=float, default=1.00,\n",
    "                        help='Sparsity constraint for predictor parameter Z ' +\n",
    "                        '(default = 1.0, i.e. dense matrix).')\n",
    "    parser.add_argument('-pS', '--print-step', type=int, default=200,\n",
    "                        help='The number of update steps between print ' +\n",
    "                        'calls to console.')\n",
    "    parser.add_argument('-vS', '--val-step', type=int, default=3,\n",
    "                        help='The number of epochs between validation' +\n",
    "                        'performance evaluation')\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ou1MfKhYtMdT"
   },
   "source": [
    "# Utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jDVo_0JiRSi9"
   },
   "outputs": [],
   "source": [
    "#utils\n",
    "import scipy.cluster\n",
    "import scipy.spatial\n",
    "import os\n",
    "\n",
    "\n",
    "def medianHeuristic(data, projectionDimension, numPrototypes, W_init=None):\n",
    "    '''\n",
    "    This method can be used to estimate gamma for ProtoNN. An approximation to\n",
    "    median heuristic is used here.\n",
    "    1. First the data is collapsed into the projectionDimension by W_init. If\n",
    "    W_init is not provided, it is initialized from a random normal(0, 1). Hence\n",
    "    data normalization is essential.\n",
    "    2. Prototype are computed by running a  k-means clustering on the projected\n",
    "    data.\n",
    "    3. The median distance is then estimated by calculating median distance\n",
    "    between prototypes and projected data points.\n",
    "\n",
    "    data needs to be [-1, numFeats]\n",
    "    If using this method to initialize gamma, please use the W and B as well.\n",
    "\n",
    "    TODO: Return estimate of Z (prototype labels) based on cluster centroids\n",
    "    andand labels\n",
    "\n",
    "    TODO: Clustering fails due to singularity error if projecting upwards\n",
    "\n",
    "    W [dxd_cap]\n",
    "    B [d_cap, m]\n",
    "    returns gamma, W, B\n",
    "    '''\n",
    "    assert data.ndim == 2\n",
    "    X = data\n",
    "    featDim = data.shape[1]\n",
    "    if projectionDimension > featDim:\n",
    "        print(\"Warning: Projection dimension > feature dimension. Gamma\")\n",
    "        print(\"\\t estimation due to median heuristic could fail.\")\n",
    "        print(\"\\tTo retain the projection dataDimension, provide\")\n",
    "        print(\"\\ta value for gamma.\")\n",
    "\n",
    "    if W_init is None:\n",
    "        W_init = np.random.normal(size=[featDim, projectionDimension])\n",
    "    W = W_init\n",
    "    XW = np.matmul(X, W)\n",
    "    assert XW.shape[1] == projectionDimension\n",
    "    assert XW.shape[0] == len(X)\n",
    "    # Requires [N x d_cap] data matrix of N observations of d_cap-dimension and\n",
    "    # the number of centroids m. Returns, [n x d_cap] centroids and\n",
    "    # elementwise center information.\n",
    "    B, centers = scipy.cluster.vq.kmeans2(XW, numPrototypes)\n",
    "    # Requires two matrices. Number of observations x dimension of observation\n",
    "    # space. Distances[i,j] is the distance between XW[i] and B[j]\n",
    "    distances = scipy.spatial.distance.cdist(XW, B, metric='euclidean')\n",
    "    distances = np.reshape(distances, [-1])\n",
    "    gamma = np.median(distances)\n",
    "    gamma = 1 / (2.5 * gamma)\n",
    "    return gamma.astype('float32'), W.astype('float32'), B.T.astype('float32')\n",
    "\n",
    "\n",
    "def multiClassHingeLoss(logits, label, batch_th):\n",
    "    '''\n",
    "    MultiClassHingeLoss to match C++ Version - No TF internal version\n",
    "    '''\n",
    "    flatLogits = tf.reshape(logits, [-1, ])\n",
    "    label_ = tf.argmax(label, 1)\n",
    "\n",
    "    correctId = tf.range(0, batch_th) * label.shape[1] + label_\n",
    "    correctLogit = tf.gather(flatLogits, correctId)\n",
    "\n",
    "    maxLabel = tf.argmax(logits, 1)\n",
    "    top2, _ = tf.nn.top_k(logits, k=2, sorted=True)\n",
    "\n",
    "    wrongMaxLogit = tf.where(\n",
    "        tf.equal(maxLabel, label_), top2[:, 1], top2[:, 0])\n",
    "\n",
    "    return tf.reduce_mean(tf.nn.relu(1. + wrongMaxLogit - correctLogit))\n",
    "\n",
    "\n",
    "def crossEntropyLoss(logits, label):\n",
    "    '''\n",
    "    Cross Entropy loss for MultiClass case in joint training for\n",
    "    faster convergence\n",
    "    '''\n",
    "    return tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                   labels=tf.stop_gradient(label)))\n",
    "\n",
    "\n",
    "def mean_absolute_error(logits, label):\n",
    "    '''\n",
    "    Function to compute the mean absolute error.\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.abs(tf.subtract(logits, label)))\n",
    "\n",
    "\n",
    "def hardThreshold(A, s):\n",
    "    '''\n",
    "    Hard thresholding function on Tensor A with sparsity s\n",
    "    '''\n",
    "    A_ = np.copy(A)\n",
    "    A_ = A_.ravel()\n",
    "    if len(A_) > 0:\n",
    "        th = np.percentile(np.abs(A_), (1 - s) * 100.0, interpolation='higher')\n",
    "        A_[np.abs(A_) < th] = 0.0\n",
    "    A_ = A_.reshape(A.shape)\n",
    "    return A_\n",
    "\n",
    "\n",
    "def copySupport(src, dest):\n",
    "    '''\n",
    "    copy support of src tensor to dest tensor\n",
    "    '''\n",
    "    support = np.nonzero(src)\n",
    "    dest_ = dest\n",
    "    dest = np.zeros(dest_.shape)\n",
    "    dest[support] = dest_[support]\n",
    "    return dest\n",
    "\n",
    "\n",
    "def countnnZ(A, s, bytesPerVar=4):\n",
    "    '''\n",
    "    Returns # of non-zeros and representative size of the tensor\n",
    "    Uses dense for s >= 0.5 - 4 byte\n",
    "    Else uses sparse - 8 byte\n",
    "    '''\n",
    "    params = 1\n",
    "    hasSparse = False\n",
    "    for i in range(0, len(A.shape)):\n",
    "        params *= int(A.shape[i])\n",
    "    if s < 0.5:\n",
    "        nnZ = np.ceil(params * s)\n",
    "        hasSparse = True\n",
    "        return nnZ, nnZ * 2 * bytesPerVar, hasSparse\n",
    "    else:\n",
    "        nnZ = params\n",
    "        return nnZ, nnZ * bytesPerVar, hasSparse\n",
    "\n",
    "\n",
    "def getConfusionMatrix(predicted, target, numClasses):\n",
    "    '''\n",
    "    Returns a confusion matrix for a multiclass classification\n",
    "    problem. `predicted` is a 1-D array of integers representing\n",
    "    the predicted classes and `target` is the target classes.\n",
    "\n",
    "    confusion[i][j]: Number of elements of class j\n",
    "        predicted as class i\n",
    "    Labels are assumed to be in range(0, numClasses)\n",
    "    Use`printFormattedConfusionMatrix` to echo the confusion matrix\n",
    "    in a user friendly form.\n",
    "    '''\n",
    "    assert(predicted.ndim == 1)\n",
    "    assert(target.ndim == 1)\n",
    "    arr = np.zeros([numClasses, numClasses])\n",
    "\n",
    "    for i in range(len(predicted)):\n",
    "        arr[predicted[i]][target[i]] += 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "def printFormattedConfusionMatrix(matrix):\n",
    "    '''\n",
    "    Given a 2D confusion matrix, prints it in a human readable way.\n",
    "    The confusion matrix is expected to be a 2D numpy array with\n",
    "    square dimensions\n",
    "    '''\n",
    "    assert(matrix.ndim == 2)\n",
    "    assert(matrix.shape[0] == matrix.shape[1])\n",
    "    RECALL = 'Recall'\n",
    "    PRECISION = 'PRECISION'\n",
    "    print(\"|%s|\" % ('True->'), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%7d|\" % i, end='')\n",
    "    print(\"%s|\" % 'Precision')\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "\n",
    "    precisionlist = np.sum(matrix, axis=1)\n",
    "    recalllist = np.sum(matrix, axis=0)\n",
    "    precisionlist = [matrix[i][i] / x if x !=\n",
    "                     0 else -1 for i, x in enumerate(precisionlist)]\n",
    "    recalllist = [matrix[i][i] / x if x !=\n",
    "                  0 else -1 for i, x in enumerate(recalllist)]\n",
    "    for i in range(matrix.shape[0]):\n",
    "        # len recall = 6\n",
    "        print(\"|%6d|\" % (i), end='')\n",
    "        for j in range(matrix.shape[0]):\n",
    "            print(\"%7d|\" % (matrix[i][j]), end='')\n",
    "        print(\"%s\" % (\" \" * (len(PRECISION) - 7)), end='')\n",
    "        if precisionlist[i] != -1:\n",
    "            print(\"%1.5f|\" % precisionlist[i])\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\")\n",
    "\n",
    "    print(\"|%s|\" % ('-' * len(RECALL)), end='')\n",
    "    for i in range(matrix.shape[0]):\n",
    "        print(\"%s|\" % ('-' * 7), end='')\n",
    "    print(\"%s|\" % ('-' * len(PRECISION)))\n",
    "    print(\"|%s|\" % ('Recall'), end='')\n",
    "\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if recalllist[i] != -1:\n",
    "            print(\"%1.5f|\" % (recalllist[i]), end='')\n",
    "        else:\n",
    "            print(\"%7s|\" % \"nan\", end='')\n",
    "\n",
    "    print('%s|' % (' ' * len(PRECISION)))\n",
    "\n",
    "\n",
    "def getPrecisionRecall(cmatrix, label=1):\n",
    "    trueP = cmatrix[label][label]\n",
    "    denom = np.sum(cmatrix, axis=0)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    recall = trueP / denom\n",
    "    denom = np.sum(cmatrix, axis=1)[label]\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    precision = trueP / denom\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    precision = np.sum(precisionlist__)\n",
    "    precision /= len(precisionlist__)\n",
    "    recall = np.sum(recalllist__)\n",
    "    recall /= len(recalllist__)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMicroPrecisionRecall(cmatrix):\n",
    "    # TP + FP\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    # TP + FN\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    num = 0.0\n",
    "    for i in range(len(cmatrix)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    precision = num / np.sum(precisionlist)\n",
    "    recall = num / np.sum(recalllist)\n",
    "    return precision, recall\n",
    "\n",
    "\n",
    "def getMacroMicroFScore(cmatrix):\n",
    "    '''\n",
    "    Returns macro and micro f-scores.\n",
    "    Refer: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.104.8244&rep=rep1&type=pdf\n",
    "    '''\n",
    "    precisionlist = np.sum(cmatrix, axis=1)\n",
    "    recalllist = np.sum(cmatrix, axis=0)\n",
    "    precisionlist__ = [cmatrix[i][i] / x if x !=\n",
    "                       0 else 0 for i, x in enumerate(precisionlist)]\n",
    "    recalllist__ = [cmatrix[i][i] / x if x !=\n",
    "                    0 else 0 for i, x in enumerate(recalllist)]\n",
    "    macro = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        denom = precisionlist__[i] + recalllist__[i]\n",
    "        numer = precisionlist__[i] * recalllist__[i] * 2\n",
    "        if denom == 0:\n",
    "            denom = 1\n",
    "        macro += numer / denom\n",
    "    macro /= len(precisionlist)\n",
    "\n",
    "    num = 0.0\n",
    "    for i in range(len(precisionlist)):\n",
    "        num += cmatrix[i][i]\n",
    "\n",
    "    denom1 = np.sum(precisionlist)\n",
    "    denom2 = np.sum(recalllist)\n",
    "    pi = num / denom1\n",
    "    rho = num / denom2\n",
    "    denom = pi + rho\n",
    "    if denom == 0:\n",
    "        denom = 1\n",
    "    micro = 2 * pi * rho / denom\n",
    "    return macro, micro\n",
    "\n",
    "\n",
    "class GraphManager:\n",
    "    '''\n",
    "    Manages saving and restoring graphs. Designed to be used with EMI-RNN\n",
    "    though is general enough to be useful otherwise as well.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def checkpointModel(self, saver, sess, modelPrefix,\n",
    "                        globalStep=1000, redirFile=None):\n",
    "        saver.save(sess, modelPrefix, global_step=globalStep)\n",
    "        print('Model saved to %s, global_step %d' % (modelPrefix, globalStep),\n",
    "              file=redirFile)\n",
    "\n",
    "    def loadCheckpoint(self, sess, modelPrefix, globalStep,\n",
    "                       redirFile=None):\n",
    "        metaname = modelPrefix + '-%d.meta' % globalStep\n",
    "        basename = os.path.basename(metaname)\n",
    "        fileList = os.listdir(os.path.dirname(modelPrefix))\n",
    "        fileList = [x for x in fileList if x.startswith(basename)]\n",
    "        assert len(fileList) > 0, 'Checkpoint file not found'\n",
    "        msg = 'Too many or too few checkpoint files for globalStep: %d' % globalStep\n",
    "        assert len(fileList) is 1, msg\n",
    "        chkpt = basename + '/' + fileList[0]\n",
    "        saver = tf.train.import_meta_graph(metaname)\n",
    "        metaname = metaname[:-5]\n",
    "        saver.restore(sess, metaname)\n",
    "        graph = tf.get_default_graph()\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DAjSVSOFtFmm"
   },
   "source": [
    "# Model Trainer - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bp5dEFiZR_sy"
   },
   "outputs": [],
   "source": [
    "#Trainer\n",
    "class ProtoNNTrainer:\n",
    "    def __init__(self, protoNNObj, regW, regB, regZ,\n",
    "                 sparcityW, sparcityB, sparcityZ,\n",
    "                 learningRate, X, Y, lossType='l2'):\n",
    "        '''\n",
    "        A wrapper for the various techniques used for training ProtoNN. This\n",
    "        subsumes both the responsibility of loss graph construction and\n",
    "        performing training. The original training routine that is part of the\n",
    "        C++ implementation of EdgeML used iterative hard thresholding (IHT),\n",
    "        gamma estimation through median heuristic and other tricks for\n",
    "        training ProtoNN. This module implements the same in Tensorflow\n",
    "        and python.\n",
    "\n",
    "        protoNNObj: An instance of ProtoNN class defining the forward\n",
    "            computation graph. The loss functions and training routines will be\n",
    "            attached to this instance.\n",
    "        regW, regB, regZ: Regularization constants for W, B, and\n",
    "            Z matrices of protoNN.\n",
    "        sparcityW, sparcityB, sparcityZ: Sparsity constraints\n",
    "            for W, B and Z matrices. A value between 0 (exclusive) and 1\n",
    "            (inclusive) is expected. A value of 1 indicates dense training.\n",
    "        learningRate: Initial learning rate for ADAM optimizer.\n",
    "        X, Y : Placeholders for data and labels.\n",
    "            X [-1, featureDimension]\n",
    "            Y [-1, num Labels]\n",
    "        lossType: ['l2', 'xentropy']\n",
    "        '''\n",
    "        self.protoNNObj = protoNNObj\n",
    "        self.__regW = regW\n",
    "        self.__regB = regB\n",
    "        self.__regZ = regZ\n",
    "        self.__sW = sparcityW\n",
    "        self.__sB = sparcityB\n",
    "        self.__sZ = sparcityZ\n",
    "        self.__lR = learningRate\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.sparseTraining = True\n",
    "        if (sparcityW == 1.0) and (sparcityB == 1.0) and (sparcityZ == 1.0):\n",
    "            self.sparseTraining = False\n",
    "            print(\"Sparse training disabled.\", file=sys.stderr)\n",
    "        # Define placeholders for sparse training\n",
    "        self.W_th = None\n",
    "        self.B_th = None\n",
    "        self.Z_th = None\n",
    "        self.__lossType = lossType\n",
    "        self.__validInit = False\n",
    "        self.__validInit = self.__validateInit()\n",
    "        self.__protoNNOut = protoNNObj(X, Y)\n",
    "        self.loss = self.__lossGraph()\n",
    "        self.trainStep = self.__trainGraph()\n",
    "        self.__hthOp = self.__getHardThresholdOp()\n",
    "        self.accuracy = protoNNObj.getAccuracyOp()\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        msg = \"Sparsity value should be between\"\n",
    "        msg += \" 0 and 1 (both inclusive).\"\n",
    "        assert self.__sW >= 0. and self.__sW <= 1., 'W:' + msg\n",
    "        assert self.__sB >= 0. and self.__sB <= 1., 'B:' + msg\n",
    "        assert self.__sZ >= 0. and self.__sZ <= 1., 'Z:' + msg\n",
    "        d, dcap, m, L, _ = self.protoNNObj.getHyperParams()\n",
    "        msg = 'Y should be of dimension [-1, num labels/classes]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.Y.shape)) == 2, msg\n",
    "        assert (self.Y.shape[1] == L), msg\n",
    "        msg = 'X should be of dimension [-1, featureDimension]'\n",
    "        msg += ' specified as part of ProtoNN object.'\n",
    "        assert (len(self.X.shape) == 2), msg\n",
    "        assert (self.X.shape[1] == d), msg\n",
    "        self.__validInit = True\n",
    "        msg = 'Values can be \\'l2\\', or \\'xentropy\\''\n",
    "        if self.__lossType not in ['l2', 'xentropy']:\n",
    "            raise ValueError(msg)\n",
    "        return True\n",
    "\n",
    "    def __lossGraph(self):\n",
    "        pnnOut = self.__protoNNOut\n",
    "        l1, l2, l3 = self.__regW, self.__regB, self.__regZ\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        if self.__lossType == 'l2':\n",
    "            with tf.name_scope('protonn-l2-loss'):\n",
    "                loss_0 = tf.nn.l2_loss(self.Y - pnnOut)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        elif self.__lossType == 'xentropy':\n",
    "            with tf.name_scope('protonn-xentropy-loss'):\n",
    "                loss_0 = tf.nn.softmax_cross_entropy_with_logits_v2(logits=pnnOut,\n",
    "                                                         labels=tf.stop_gradient(self.Y))\n",
    "                loss_0 = tf.reduce_mean(loss_0)\n",
    "                reg = l1 * tf.nn.l2_loss(W) + l2 * tf.nn.l2_loss(B)\n",
    "                reg += l3 * tf.nn.l2_loss(Z)\n",
    "                loss = loss_0 + reg\n",
    "        return loss\n",
    "\n",
    "    def __trainGraph(self):\n",
    "        with tf.name_scope('protonn-gradient-adam'):\n",
    "            trainStep = tf.train.AdamOptimizer(self.__lR)\n",
    "            trainStep = trainStep.minimize(self.loss)\n",
    "        return trainStep\n",
    "\n",
    "    def __getHardThresholdOp(self):\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        self.W_th = tf.placeholder(tf.float32, name='W_th')\n",
    "        self.B_th = tf.placeholder(tf.float32, name='B_th')\n",
    "        self.Z_th = tf.placeholder(tf.float32, name='Z_th')\n",
    "        with tf.name_scope('hard-threshold-assignments'):\n",
    "            hard_thrsd_W = W.assign(self.W_th)\n",
    "            hard_thrsd_B = B.assign(self.B_th)\n",
    "            hard_thrsd_Z = Z.assign(self.Z_th)\n",
    "            hard_thrsd_op = tf.group(hard_thrsd_W, hard_thrsd_B, hard_thrsd_Z)\n",
    "        return hard_thrsd_op\n",
    "\n",
    "    def train(self, batchSize, totalEpochs, sess,\n",
    "              x_train, x_val, y_train, y_val, noInit=False,\n",
    "              redirFile=None, printStep=10, valStep=3):\n",
    "        '''\n",
    "        Performs dense training of ProtoNN followed by iterative hard\n",
    "        thresholding to enforce sparsity constraints.\n",
    "\n",
    "        batchSize: Batch size per update\n",
    "        totalEpochs: The number of epochs to run training for. One epoch is\n",
    "            defined as one pass over the entire training data.\n",
    "        sess: The Tensorflow session to use for running various graph\n",
    "            operators.\n",
    "        x_train, x_val, y_train, y_val: The numpy array containing train and\n",
    "            validation data. x data is assumed to in of shape [-1,\n",
    "            featureDimension] while y should have shape [-1, numberLabels].\n",
    "        noInit: By default, all the tensors of the computation graph are\n",
    "        initialized at the start of the training session. Set noInit=False to\n",
    "        disable this behaviour.\n",
    "        printStep: Number of batches between echoing of loss and train accuracy.\n",
    "        valStep: Number of epochs between evolutions on validation set.\n",
    "        '''\n",
    "        d, d_cap, m, L, gamma = self.protoNNObj.getHyperParams()\n",
    "        assert batchSize >= 1, 'Batch size should be positive integer'\n",
    "        assert totalEpochs >= 1, 'Total epochs should be positive integer'\n",
    "        assert x_train.ndim == 2, 'Expected training data to be of rank 2'\n",
    "        assert x_train.shape[1] == d, 'Expected x_train to be [-1, %d]' % d\n",
    "        assert x_val.ndim == 2, 'Expected validation data to be of rank 2'\n",
    "        assert x_val.shape[1] == d, 'Expected x_val to be [-1, %d]' % d\n",
    "        assert y_train.ndim == 2, 'Expected training labels to be of rank 2'\n",
    "        assert y_train.shape[1] == L, 'Expected y_train to be [-1, %d]' % L\n",
    "        assert y_val.ndim == 2, 'Expected validation labels to be of rank 2'\n",
    "        assert y_val.shape[1] == L, 'Expected y_val to be [-1, %d]' % L\n",
    "\n",
    "        # Numpy will throw asserts for arrays\n",
    "        if sess is None:\n",
    "            raise ValueError('sess must be valid Tensorflow session.')\n",
    "\n",
    "        trainNumBatches = int(np.ceil(len(x_train) / batchSize))\n",
    "        valNumBatches = int(np.ceil(len(x_val) / batchSize))\n",
    "        x_train_batches = np.array_split(x_train, trainNumBatches)\n",
    "        y_train_batches = np.array_split(y_train, trainNumBatches)\n",
    "        x_val_batches = np.array_split(x_val, valNumBatches)\n",
    "        y_val_batches = np.array_split(y_val, valNumBatches)\n",
    "        if not noInit:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        X, Y = self.X, self.Y\n",
    "        W, B, Z, _ = self.protoNNObj.getModelMatrices()\n",
    "        for epoch in range(totalEpochs):\n",
    "            for i in range(len(x_train_batches)):\n",
    "                batch_x = x_train_batches[i]\n",
    "                batch_y = y_train_batches[i]\n",
    "                feed_dict = {\n",
    "                    X: batch_x,\n",
    "                    Y: batch_y\n",
    "                }\n",
    "                sess.run(self.trainStep, feed_dict=feed_dict)\n",
    "                if i % printStep == 0:\n",
    "                    loss, acc = sess.run([self.loss, self.accuracy],\n",
    "                                         feed_dict=feed_dict)\n",
    "                    msg = \"Epoch: %3d Batch: %3d\" % (epoch, i)\n",
    "                    msg += \" Loss: %3.5f Accuracy: %2.5f\" % (loss, acc)\n",
    "                    print(msg, file=redirFile)\n",
    "\n",
    "            # Perform Hard thresholding\n",
    "            if self.sparseTraining:\n",
    "                W_, B_, Z_ = sess.run([W, B, Z])\n",
    "                fd_thrsd = {\n",
    "                    self.W_th: hardThreshold(W_, self.__sW),\n",
    "                    self.B_th: hardThreshold(B_, self.__sB),\n",
    "                    self.Z_th: hardThreshold(Z_, self.__sZ)\n",
    "                }\n",
    "                sess.run(self.__hthOp, feed_dict=fd_thrsd)\n",
    "\n",
    "            if (epoch + 1) % valStep  == 0:\n",
    "                acc = 0.0\n",
    "                loss = 0.0\n",
    "                for j in range(len(x_val_batches)):\n",
    "                    batch_x = x_val_batches[j]\n",
    "                    batch_y = y_val_batches[j]\n",
    "                    feed_dict = {\n",
    "                        X: batch_x,\n",
    "                        Y: batch_y\n",
    "                    }\n",
    "                    acc_, loss_ = sess.run([self.accuracy, self.loss],\n",
    "                                           feed_dict=feed_dict)\n",
    "                    acc += acc_\n",
    "                    loss += loss_\n",
    "                acc /= len(y_val_batches)\n",
    "                loss /= len(y_val_batches)\n",
    "                print(\"Test Loss: %2.5f Accuracy: %2.5f\" % (loss, acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8Z6ym4k_s9pS"
   },
   "source": [
    "# Model Graph - ProtoNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRPFglKHSbu-"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ProtoNN:\n",
    "    def __init__(self, inputDimension, projectionDimension, numPrototypes,\n",
    "                 numOutputLabels, gamma,\n",
    "                 W = None, B = None, Z = None):\n",
    "        '''\n",
    "        Forward computation graph for ProtoNN.\n",
    "\n",
    "        inputDimension: Input data dimension or feature dimension.\n",
    "        projectionDimension: hyperparameter\n",
    "        numPrototypes: hyperparameter\n",
    "        numOutputLabels: The number of output labels or classes\n",
    "        W, B, Z: Numpy matrices that can be used to initialize\n",
    "            projection matrix(W), prototype matrix (B) and prototype labels\n",
    "            matrix (B).\n",
    "            Expected Dimensions:\n",
    "                W   inputDimension (d) x projectionDimension (d_cap)\n",
    "                B   projectionDimension (d_cap) x numPrototypes (m)\n",
    "                Z   numOutputLabels (L) x numPrototypes (m)\n",
    "        '''\n",
    "        with tf.name_scope('protoNN') as ns:\n",
    "            self.__nscope = ns\n",
    "        self.__d = inputDimension\n",
    "        self.__d_cap = projectionDimension\n",
    "        self.__m = numPrototypes\n",
    "        self.__L = numOutputLabels\n",
    "\n",
    "        self.__inW = W\n",
    "        self.__inB = B\n",
    "        self.__inZ = Z\n",
    "        self.__inGamma = gamma\n",
    "        self.W, self.B, self.Z = None, None, None\n",
    "        self.gamma = None\n",
    "\n",
    "        self.__validInit = False\n",
    "        self.__initWBZ()\n",
    "        self.__initGamma()\n",
    "        self.__validateInit()\n",
    "        self.protoNNOut = None\n",
    "        self.predictions = None\n",
    "        self.accuracy = None\n",
    "\n",
    "    def __validateInit(self):\n",
    "        self.__validInit = False\n",
    "        errmsg = \"Dimensions mismatch! Should be W[d, d_cap]\"\n",
    "        errmsg += \", B[d_cap, m] and Z[L, m]\"\n",
    "        d, d_cap, m, L, _ = self.getHyperParams()\n",
    "        assert self.W.shape[0] == d, errmsg\n",
    "        assert self.W.shape[1] == d_cap, errmsg\n",
    "        assert self.B.shape[0] == d_cap, errmsg\n",
    "        assert self.B.shape[1] == m, errmsg\n",
    "        assert self.Z.shape[0] == L, errmsg\n",
    "        assert self.Z.shape[1] == m, errmsg\n",
    "        self.__validInit = True\n",
    "\n",
    "    def __initWBZ(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            W = self.__inW\n",
    "            if W is None:\n",
    "                W = tf.random_normal_initializer()\n",
    "                W = W([self.__d, self.__d_cap])\n",
    "            self.W = tf.Variable(W, name='W', dtype=tf.float32)\n",
    "\n",
    "            B = self.__inB\n",
    "            if B is None:\n",
    "                B = tf.random_uniform_initializer()\n",
    "                B = B([self.__d_cap, self.__m])\n",
    "            self.B = tf.Variable(B, name='B', dtype=tf.float32)\n",
    "\n",
    "            Z = self.__inZ\n",
    "            if Z is None:\n",
    "                Z = tf.random_normal_initializer()\n",
    "                Z = Z([self.__L, self.__m])\n",
    "            Z = tf.Variable(Z, name='Z', dtype=tf.float32)\n",
    "            self.Z = Z\n",
    "        return self.W, self.B, self.Z\n",
    "\n",
    "    def __initGamma(self):\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            gamma = self.__inGamma\n",
    "            self.gamma = tf.constant(gamma, name='gamma')\n",
    "\n",
    "    def getHyperParams(self):\n",
    "        '''\n",
    "        Returns the model hyperparameters:\n",
    "            [inputDimension, projectionDimension,\n",
    "            numPrototypes, numOutputLabels, gamma]\n",
    "        '''\n",
    "        d = self.__d\n",
    "        dcap = self.__d_cap\n",
    "        m = self.__m\n",
    "        L = self.__L\n",
    "        return d, dcap, m, L, self.gamma\n",
    "\n",
    "    def getModelMatrices(self):\n",
    "        '''\n",
    "        Returns Tensorflow tensors of the model matrices, which\n",
    "        can then be evaluated to obtain corresponding numpy arrays.\n",
    "\n",
    "        These can then be exported as part of other implementations of\n",
    "        ProtonNN, for instance a C++ implementation or pure python\n",
    "        implementation.\n",
    "        Returns\n",
    "            [ProjectionMatrix (W), prototypeMatrix (B),\n",
    "             prototypeLabelsMatrix (Z), gamma]\n",
    "        '''\n",
    "        return self.W, self.B, self.Z, self.gamma\n",
    "\n",
    "    def __call__(self, X, Y=None):\n",
    "        '''\n",
    "        This method is responsible for construction of the forward computation\n",
    "        graph. The end point of the computation graph, or in other words the\n",
    "        output operator for the forward computation is returned. Additionally,\n",
    "        if the argument Y is provided, a classification accuracy operator with\n",
    "        Y as target will also be created. For this, Y is assumed to in one-hot\n",
    "        encoded format and the class with the maximum prediction score is\n",
    "        compared to the encoded class in Y.  This accuracy operator is returned\n",
    "        by getAccuracyOp() method. If a different accuracyOp is required, it\n",
    "        can be defined by overriding the createAccOp(protoNNScoresOut, Y)\n",
    "        method.\n",
    "\n",
    "        X: Input tensor or placeholder of shape [-1, inputDimension]\n",
    "        Y: Optional tensor or placeholder for targets (labels or classes).\n",
    "            Expected shape is [-1, numOutputLabels].\n",
    "        returns: The forward computation outputs, self.protoNNOut\n",
    "        '''\n",
    "        # This should never execute\n",
    "        assert self.__validInit is True, \"Initialization failed!\"\n",
    "        if self.protoNNOut is not None:\n",
    "            return self.protoNNOut\n",
    "\n",
    "        W, B, Z, gamma = self.W, self.B, self.Z, self.gamma\n",
    "        with tf.name_scope(self.__nscope):\n",
    "            WX = tf.matmul(X, W)\n",
    "            # Convert WX to tensor so that broadcasting can work\n",
    "            dim = [-1, WX.shape.as_list()[1], 1]\n",
    "            WX = tf.reshape(WX, dim)\n",
    "            dim = [1, B.shape.as_list()[0], -1]\n",
    "            B = tf.reshape(B, dim)\n",
    "            l2sim = B - WX\n",
    "            l2sim = tf.pow(l2sim, 2)\n",
    "            l2sim = tf.reduce_sum(l2sim, 1, keepdims=True)\n",
    "            self.l2sim = l2sim\n",
    "            gammal2sim = (-1 * gamma * gamma) * l2sim\n",
    "            M = tf.exp(gammal2sim)\n",
    "            dim = [1] + Z.shape.as_list()\n",
    "            Z = tf.reshape(Z, dim)\n",
    "            y = tf.multiply(Z, M)\n",
    "            y = tf.reduce_sum(y, 2, name='protoNNScoreOut')\n",
    "            self.protoNNOut = y\n",
    "            self.predictions = tf.argmax(y, 1, name='protoNNPredictions')\n",
    "            if Y is not None:\n",
    "                self.createAccOp(self.protoNNOut, Y)\n",
    "        return y\n",
    "\n",
    "    def createAccOp(self, outputs, target):\n",
    "        '''\n",
    "        Define an accuracy operation on ProtoNN's output scores and targets.\n",
    "        Here a simple classification accuracy operator is defined. More\n",
    "        complicated operators (for multiple label problems and so forth) can be\n",
    "        defined by overriding this method\n",
    "        '''\n",
    "        assert self.predictions is not None\n",
    "        target = tf.argmax(target, 1)\n",
    "        correctPrediction = tf.equal(self.predictions, target)\n",
    "        acc = tf.reduce_mean(tf.cast(correctPrediction, tf.float32),\n",
    "                             name='protoNNAccuracy')\n",
    "        self.accuracy = acc\n",
    "\n",
    "    def getPredictionsOp(self):\n",
    "        '''\n",
    "        The predictions operator is defined as argmax(protoNNScores) for each\n",
    "        prediction.\n",
    "        '''\n",
    "        return self.predictions\n",
    "\n",
    "    def getAccuracyOp(self):\n",
    "        '''\n",
    "        returns accuracyOp as defined by createAccOp. It defaults to\n",
    "        multi-class classification accuracy.\n",
    "        '''\n",
    "        msg = \"Accuracy operator not defined in graph. Did you provide Y as an\"\n",
    "        msg += \" argument to _call_?\"\n",
    "        assert self.accuracy is not None, msg\n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uEBMKewPQJ1c"
   },
   "source": [
    "# Obtain Data\n",
    "\n",
    "It is assumed that the Daphnet data has already been downloaded,preprocessed and set up in subdirectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Dimension:  423\n",
      "Num classes:  2\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "windowLen = 'data'\n",
    "out = preprocessData(DATA_DIR,windowLen)\n",
    "dataDimension = out[0]\n",
    "numClasses = out[1]\n",
    "x_train, y_train = out[2], out[3]\n",
    "x_test, y_test = out[4], out[5]\n",
    "print(\"Feature Dimension: \", dataDimension)\n",
    "print(\"Num classes: \", numClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = r\"./experiments\"\n",
    "train, test = np.load(DATA_DIR + '/ttrain_data.npy'), np.load(DATA_DIR + '/ttest_data.npy')\n",
    "x_train, y_train = train[:, 1:], train[:, 0]\n",
    "x_test, y_test = test[:, 1:], test[:, 0]\n",
    "\n",
    "numClasses = max(y_train) - min(y_train) + 1\n",
    "numClasses = max(numClasses, max(y_test) - min(y_test) + 1)\n",
    "numClasses = int(numClasses)\n",
    "\n",
    "y_train = helper.to_onehot(y_train, numClasses)\n",
    "y_test = helper.to_onehot(y_test, numClasses)\n",
    "dataDimension = x_train.shape[1]\n",
    "numClasses = y_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u6oX8eJQJ2N"
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "Note that ProtoNN is very sensitive to the value of the hyperparameter $\\gamma$, here stored in valiable `GAMMA`. If `GAMMA` is set to `None`, median heuristic will be used to estimate a good value of $\\gamma$ through the `helper.getGamma()` method. This method also returns the corresponding `W` and `B` matrices which should be used to initialize ProtoNN (as is done here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.279204Z",
     "start_time": "2018-08-15T13:06:10.272880Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UaduZ1vJQJ2P"
   },
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = 0.000005\n",
    "REG_B = 0.0\n",
    "REG_Z = 0.00005\n",
    "SPAR_W = 1.0\n",
    "SPAR_B = 0.8\n",
    "SPAR_Z = 0.8\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 600\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.007586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:06:10.307632Z",
     "start_time": "2018-08-15T13:06:10.280955Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1003,
     "status": "ok",
     "timestamp": 1567154485603,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "teqlUPhLQJ2W",
    "outputId": "e7e7f7f2-9ddb-448b-9539-65a1a2dc1c03"
   },
   "outputs": [],
   "source": [
    "W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.007586"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from functools import partial\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "def objective(trial,x_train, x_test, y_train, y_test):\n",
    "    W, B, gamma = getGamma(GAMMA, PROJECTION_DIM, dataDimension,\n",
    "                       NUM_PROTOTYPES, x_train)\n",
    "    # Inside the optimization function, you use the 'trial' object to suggest hyperparameters\n",
    "    REG_W = trial.suggest_float('REG_W', 2e-6, 5e-6)\n",
    "    REG_B = trial.suggest_float('REG_B', 0.0, 0.01)\n",
    "    REG_Z = trial.suggest_float('REG_Z', 2e-5, 5e-5)\n",
    "    SPAR_W = trial.suggest_float('SPAR_W', 0.5, 1.0)\n",
    "    SPAR_B = trial.suggest_float('SPAR_B', 0.5, 1.0)\n",
    "    SPAR_Z = trial.suggest_float('SPAR_Z', 0.5, 1.0)\n",
    "    LEARNING_RATE = trial.suggest_float('LEARNING_RATE', 1e-4, 1e-3)\n",
    "    NUM_EPOCHS = trial.suggest_int('NUM_EPOCHS', 200, 600)\n",
    "\n",
    "    # Set the suggested hyperparameters in the trainer\n",
    "    protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "    trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "    # Call your ProtoNN trainer function or use it as needed\n",
    "    sess = tf.Session()\n",
    "\n",
    "    trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,printStep=600, valStep=10)\n",
    "    acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "    pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "    # W, B, Z are tensorflow graph nodes\n",
    "    W, B, Z, _ = protoNN.getModelMatrices()\n",
    "    matrixList = sess.run([W, B, Z])\n",
    "    sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "    nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "    y_test = np.argmax(y_test,axis=1)\n",
    "    sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "    specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "    weight_sensitivity = 0.3\n",
    "    weight_specificity = 0.7\n",
    "    weighted_avg = (weight_sensitivity * sensitivity + weight_specificity * specificity) / (weight_sensitivity + weight_specificity)\n",
    "    return weighted_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "study = optuna.create_study(direction='maximize')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "op_fun = partial(objective,x_train=x_train, x_test=x_test, y_train=y_train, y_test=y_test)\n",
    "study.optimize(op_fun,n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = study.best_params['REG_W']\n",
    "REG_B = study.best_params['REG_B']\n",
    "REG_Z = study.best_params['REG_Z']\n",
    "SPAR_W = study.best_params['SPAR_W']\n",
    "SPAR_B = study.best_params['SPAR_B']\n",
    "SPAR_Z = study.best_params['SPAR_Z']\n",
    "LEARNING_RATE = study.best_params['LEARNING_RATE']\n",
    "NUM_EPOCHS = study.best_params['NUM_EPOCHS']\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECTION_DIM = 5 #d^\n",
    "NUM_PROTOTYPES = 40 #m\n",
    "REG_W = hyper['REG_W']\n",
    "REG_B = hyper['REG_B']\n",
    "REG_Z = hyper['REG_Z']\n",
    "SPAR_W = hyper['SPAR_W']\n",
    "SPAR_B = hyper['SPAR_B']\n",
    "SPAR_Z = hyper['SPAR_Z']\n",
    "LEARNING_RATE = hyper['LEARNING_RATE']\n",
    "NUM_EPOCHS = 600\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJwO4MXatk9G"
   },
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.641991Z",
     "start_time": "2018-08-15T13:06:10.309353Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 98358,
     "status": "ok",
     "timestamp": 1567154584840,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "MrKAP5_RQJ2b",
    "outputId": "2fb982af-47ae-4867-c5e5-b2ecc2b9dfc4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   0 Batch:   0 Loss: 0.00493 Accuracy: 1.00000\n",
      "Epoch:   1 Batch:   0 Loss: 0.00955 Accuracy: 1.00000\n",
      "Epoch:   2 Batch:   0 Loss: 0.04387 Accuracy: 1.00000\n",
      "Epoch:   3 Batch:   0 Loss: 0.20178 Accuracy: 1.00000\n",
      "Epoch:   4 Batch:   0 Loss: 0.51887 Accuracy: 1.00000\n",
      "Epoch:   5 Batch:   0 Loss: 0.78418 Accuracy: 0.00000\n",
      "Epoch:   6 Batch:   0 Loss: 0.91766 Accuracy: 0.00000\n",
      "Epoch:   7 Batch:   0 Loss: 0.97359 Accuracy: 0.00000\n",
      "Epoch:   8 Batch:   0 Loss: 0.99591 Accuracy: 0.00000\n",
      "Epoch:   9 Batch:   0 Loss: 1.00462 Accuracy: 0.00000\n",
      "Test Loss: 0.72631 Accuracy: 0.50017\n",
      "Epoch:  10 Batch:   0 Loss: 1.00750 Accuracy: 0.00000\n",
      "Epoch:  11 Batch:   0 Loss: 1.00754 Accuracy: 0.00000\n",
      "Epoch:  12 Batch:   0 Loss: 1.00587 Accuracy: 0.00000\n",
      "Epoch:  13 Batch:   0 Loss: 1.00301 Accuracy: 0.00000\n",
      "Epoch:  14 Batch:   0 Loss: 0.99928 Accuracy: 0.00000\n",
      "Epoch:  15 Batch:   0 Loss: 0.99486 Accuracy: 0.00000\n",
      "Epoch:  16 Batch:   0 Loss: 0.98989 Accuracy: 0.00000\n",
      "Epoch:  17 Batch:   0 Loss: 0.98447 Accuracy: 0.00000\n",
      "Epoch:  18 Batch:   0 Loss: 0.97877 Accuracy: 0.00000\n",
      "Epoch:  19 Batch:   0 Loss: 0.97274 Accuracy: 0.00000\n",
      "Test Loss: 0.70668 Accuracy: 0.50051\n",
      "Epoch:  20 Batch:   0 Loss: 0.96665 Accuracy: 0.00000\n",
      "Epoch:  21 Batch:   0 Loss: 0.96041 Accuracy: 0.00000\n",
      "Epoch:  22 Batch:   0 Loss: 0.95399 Accuracy: 0.00000\n",
      "Epoch:  23 Batch:   0 Loss: 0.94745 Accuracy: 0.00000\n",
      "Epoch:  24 Batch:   0 Loss: 0.94092 Accuracy: 0.00000\n",
      "Epoch:  25 Batch:   0 Loss: 0.93420 Accuracy: 0.00000\n",
      "Epoch:  26 Batch:   0 Loss: 0.92739 Accuracy: 0.00000\n",
      "Epoch:  27 Batch:   0 Loss: 0.92045 Accuracy: 0.00000\n",
      "Epoch:  28 Batch:   0 Loss: 0.91344 Accuracy: 0.00000\n",
      "Epoch:  29 Batch:   0 Loss: 0.90634 Accuracy: 0.00000\n",
      "Test Loss: 0.67796 Accuracy: 0.51799\n",
      "Epoch:  30 Batch:   0 Loss: 0.89916 Accuracy: 0.00000\n",
      "Epoch:  31 Batch:   0 Loss: 0.89185 Accuracy: 0.00000\n",
      "Epoch:  32 Batch:   0 Loss: 0.88446 Accuracy: 0.00000\n",
      "Epoch:  33 Batch:   0 Loss: 0.87701 Accuracy: 0.00000\n",
      "Epoch:  34 Batch:   0 Loss: 0.86941 Accuracy: 0.00000\n",
      "Epoch:  35 Batch:   0 Loss: 0.86172 Accuracy: 0.00000\n",
      "Epoch:  36 Batch:   0 Loss: 0.85394 Accuracy: 0.00000\n",
      "Epoch:  37 Batch:   0 Loss: 0.84609 Accuracy: 0.00000\n",
      "Epoch:  38 Batch:   0 Loss: 0.83819 Accuracy: 0.00000\n",
      "Epoch:  39 Batch:   0 Loss: 0.83019 Accuracy: 0.00000\n",
      "Test Loss: 0.64310 Accuracy: 0.60769\n",
      "Epoch:  40 Batch:   0 Loss: 0.82207 Accuracy: 0.00000\n",
      "Epoch:  41 Batch:   0 Loss: 0.81382 Accuracy: 0.00000\n",
      "Epoch:  42 Batch:   0 Loss: 0.80556 Accuracy: 0.00000\n",
      "Epoch:  43 Batch:   0 Loss: 0.79726 Accuracy: 0.00000\n",
      "Epoch:  44 Batch:   0 Loss: 0.78888 Accuracy: 0.00000\n",
      "Epoch:  45 Batch:   0 Loss: 0.78045 Accuracy: 0.00000\n",
      "Epoch:  46 Batch:   0 Loss: 0.77199 Accuracy: 0.09375\n",
      "Epoch:  47 Batch:   0 Loss: 0.76342 Accuracy: 0.15625\n",
      "Epoch:  48 Batch:   0 Loss: 0.75488 Accuracy: 0.21875\n",
      "Epoch:  49 Batch:   0 Loss: 0.74629 Accuracy: 0.28125\n",
      "Test Loss: 0.60868 Accuracy: 0.66292\n",
      "Epoch:  50 Batch:   0 Loss: 0.73768 Accuracy: 0.31250\n",
      "Epoch:  51 Batch:   0 Loss: 0.72904 Accuracy: 0.34375\n",
      "Epoch:  52 Batch:   0 Loss: 0.72036 Accuracy: 0.43750\n",
      "Epoch:  53 Batch:   0 Loss: 0.71166 Accuracy: 0.46875\n",
      "Epoch:  54 Batch:   0 Loss: 0.70296 Accuracy: 0.62500\n",
      "Epoch:  55 Batch:   0 Loss: 0.69422 Accuracy: 0.62500\n",
      "Epoch:  56 Batch:   0 Loss: 0.68555 Accuracy: 0.71875\n",
      "Epoch:  57 Batch:   0 Loss: 0.67689 Accuracy: 0.71875\n",
      "Epoch:  58 Batch:   0 Loss: 0.66826 Accuracy: 0.78125\n",
      "Epoch:  59 Batch:   0 Loss: 0.65964 Accuracy: 0.81250\n",
      "Test Loss: 0.57896 Accuracy: 0.71292\n",
      "Epoch:  60 Batch:   0 Loss: 0.65106 Accuracy: 0.84375\n",
      "Epoch:  61 Batch:   0 Loss: 0.64250 Accuracy: 0.87500\n",
      "Epoch:  62 Batch:   0 Loss: 0.63403 Accuracy: 0.93750\n",
      "Epoch:  63 Batch:   0 Loss: 0.62565 Accuracy: 0.93750\n",
      "Epoch:  64 Batch:   0 Loss: 0.61725 Accuracy: 0.93750\n",
      "Epoch:  65 Batch:   0 Loss: 0.60901 Accuracy: 0.93750\n",
      "Epoch:  66 Batch:   0 Loss: 0.60083 Accuracy: 0.93750\n",
      "Epoch:  67 Batch:   0 Loss: 0.59273 Accuracy: 0.93750\n",
      "Epoch:  68 Batch:   0 Loss: 0.58469 Accuracy: 0.93750\n",
      "Epoch:  69 Batch:   0 Loss: 0.57675 Accuracy: 0.93750\n",
      "Test Loss: 0.55363 Accuracy: 0.75658\n",
      "Epoch:  70 Batch:   0 Loss: 0.56893 Accuracy: 0.93750\n",
      "Epoch:  71 Batch:   0 Loss: 0.56119 Accuracy: 0.96875\n",
      "Epoch:  72 Batch:   0 Loss: 0.55359 Accuracy: 0.96875\n",
      "Epoch:  73 Batch:   0 Loss: 0.54615 Accuracy: 0.96875\n",
      "Epoch:  74 Batch:   0 Loss: 0.53877 Accuracy: 0.96875\n",
      "Epoch:  75 Batch:   0 Loss: 0.53149 Accuracy: 0.96875\n",
      "Epoch:  76 Batch:   0 Loss: 0.52426 Accuracy: 0.96875\n",
      "Epoch:  77 Batch:   0 Loss: 0.51715 Accuracy: 0.96875\n",
      "Epoch:  78 Batch:   0 Loss: 0.51017 Accuracy: 0.96875\n",
      "Epoch:  79 Batch:   0 Loss: 0.50327 Accuracy: 0.96875\n",
      "Test Loss: 0.53259 Accuracy: 0.77879\n",
      "Epoch:  80 Batch:   0 Loss: 0.49648 Accuracy: 0.96875\n",
      "Epoch:  81 Batch:   0 Loss: 0.48977 Accuracy: 0.96875\n",
      "Epoch:  82 Batch:   0 Loss: 0.48316 Accuracy: 0.96875\n",
      "Epoch:  83 Batch:   0 Loss: 0.47666 Accuracy: 0.96875\n",
      "Epoch:  84 Batch:   0 Loss: 0.47025 Accuracy: 0.96875\n",
      "Epoch:  85 Batch:   0 Loss: 0.46397 Accuracy: 0.96875\n",
      "Epoch:  86 Batch:   0 Loss: 0.45779 Accuracy: 0.96875\n",
      "Epoch:  87 Batch:   0 Loss: 0.45170 Accuracy: 0.96875\n",
      "Epoch:  88 Batch:   0 Loss: 0.44573 Accuracy: 0.96875\n",
      "Epoch:  89 Batch:   0 Loss: 0.43984 Accuracy: 0.96875\n",
      "Test Loss: 0.51559 Accuracy: 0.79255\n",
      "Epoch:  90 Batch:   0 Loss: 0.43406 Accuracy: 0.96875\n",
      "Epoch:  91 Batch:   0 Loss: 0.42837 Accuracy: 0.96875\n",
      "Epoch:  92 Batch:   0 Loss: 0.42277 Accuracy: 0.96875\n",
      "Epoch:  93 Batch:   0 Loss: 0.41726 Accuracy: 0.96875\n",
      "Epoch:  94 Batch:   0 Loss: 0.41185 Accuracy: 0.96875\n",
      "Epoch:  95 Batch:   0 Loss: 0.40651 Accuracy: 0.96875\n",
      "Epoch:  96 Batch:   0 Loss: 0.40127 Accuracy: 0.96875\n",
      "Epoch:  97 Batch:   0 Loss: 0.39614 Accuracy: 0.96875\n",
      "Epoch:  98 Batch:   0 Loss: 0.39111 Accuracy: 0.96875\n",
      "Epoch:  99 Batch:   0 Loss: 0.38618 Accuracy: 0.96875\n",
      "Test Loss: 0.50222 Accuracy: 0.80040\n",
      "Epoch: 100 Batch:   0 Loss: 0.38133 Accuracy: 0.96875\n",
      "Epoch: 101 Batch:   0 Loss: 0.37658 Accuracy: 0.96875\n",
      "Epoch: 102 Batch:   0 Loss: 0.37191 Accuracy: 0.96875\n",
      "Epoch: 103 Batch:   0 Loss: 0.36733 Accuracy: 0.96875\n",
      "Epoch: 104 Batch:   0 Loss: 0.36283 Accuracy: 0.96875\n",
      "Epoch: 105 Batch:   0 Loss: 0.35841 Accuracy: 0.96875\n",
      "Epoch: 106 Batch:   0 Loss: 0.35407 Accuracy: 0.96875\n",
      "Epoch: 107 Batch:   0 Loss: 0.34984 Accuracy: 0.96875\n",
      "Epoch: 108 Batch:   0 Loss: 0.34568 Accuracy: 0.96875\n",
      "Epoch: 109 Batch:   0 Loss: 0.34159 Accuracy: 0.96875\n",
      "Test Loss: 0.49209 Accuracy: 0.80539\n",
      "Epoch: 110 Batch:   0 Loss: 0.33760 Accuracy: 0.96875\n",
      "Epoch: 111 Batch:   0 Loss: 0.33369 Accuracy: 0.96875\n",
      "Epoch: 112 Batch:   0 Loss: 0.32984 Accuracy: 0.96875\n",
      "Epoch: 113 Batch:   0 Loss: 0.32607 Accuracy: 0.96875\n",
      "Epoch: 114 Batch:   0 Loss: 0.32237 Accuracy: 0.96875\n",
      "Epoch: 115 Batch:   0 Loss: 0.31874 Accuracy: 0.96875\n",
      "Epoch: 116 Batch:   0 Loss: 0.31518 Accuracy: 0.96875\n",
      "Epoch: 117 Batch:   0 Loss: 0.31170 Accuracy: 0.96875\n",
      "Epoch: 118 Batch:   0 Loss: 0.30830 Accuracy: 0.96875\n",
      "Epoch: 119 Batch:   0 Loss: 0.30495 Accuracy: 0.96875\n",
      "Test Loss: 0.48475 Accuracy: 0.80960\n",
      "Epoch: 120 Batch:   0 Loss: 0.30167 Accuracy: 0.96875\n",
      "Epoch: 121 Batch:   0 Loss: 0.29847 Accuracy: 0.96875\n",
      "Epoch: 122 Batch:   0 Loss: 0.29533 Accuracy: 0.96875\n",
      "Epoch: 123 Batch:   0 Loss: 0.29226 Accuracy: 0.96875\n",
      "Epoch: 124 Batch:   0 Loss: 0.28924 Accuracy: 0.96875\n",
      "Epoch: 125 Batch:   0 Loss: 0.28629 Accuracy: 0.96875\n",
      "Epoch: 126 Batch:   0 Loss: 0.28340 Accuracy: 0.96875\n",
      "Epoch: 127 Batch:   0 Loss: 0.28056 Accuracy: 0.96875\n",
      "Epoch: 128 Batch:   0 Loss: 0.27779 Accuracy: 0.96875\n",
      "Epoch: 129 Batch:   0 Loss: 0.27508 Accuracy: 0.96875\n",
      "Test Loss: 0.47975 Accuracy: 0.81155\n",
      "Epoch: 130 Batch:   0 Loss: 0.27256 Accuracy: 0.96875\n",
      "Epoch: 131 Batch:   0 Loss: 0.26985 Accuracy: 0.96875\n",
      "Epoch: 132 Batch:   0 Loss: 0.26727 Accuracy: 0.96875\n",
      "Epoch: 133 Batch:   0 Loss: 0.26489 Accuracy: 0.96875\n",
      "Epoch: 134 Batch:   0 Loss: 0.26235 Accuracy: 0.96875\n",
      "Epoch: 135 Batch:   0 Loss: 0.25994 Accuracy: 0.96875\n",
      "Epoch: 136 Batch:   0 Loss: 0.25759 Accuracy: 0.96875\n",
      "Epoch: 137 Batch:   0 Loss: 0.25530 Accuracy: 0.96875\n",
      "Epoch: 138 Batch:   0 Loss: 0.25305 Accuracy: 0.96875\n",
      "Epoch: 139 Batch:   0 Loss: 0.25086 Accuracy: 0.96875\n",
      "Test Loss: 0.47647 Accuracy: 0.81298\n",
      "Epoch: 140 Batch:   0 Loss: 0.24872 Accuracy: 0.96875\n",
      "Epoch: 141 Batch:   0 Loss: 0.24662 Accuracy: 0.96875\n",
      "Epoch: 142 Batch:   0 Loss: 0.24455 Accuracy: 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 143 Batch:   0 Loss: 0.24254 Accuracy: 0.96875\n",
      "Epoch: 144 Batch:   0 Loss: 0.24057 Accuracy: 0.96875\n",
      "Epoch: 145 Batch:   0 Loss: 0.23864 Accuracy: 0.96875\n",
      "Epoch: 146 Batch:   0 Loss: 0.23675 Accuracy: 0.96875\n",
      "Epoch: 147 Batch:   0 Loss: 0.23491 Accuracy: 0.96875\n",
      "Epoch: 148 Batch:   0 Loss: 0.23311 Accuracy: 0.96875\n",
      "Epoch: 149 Batch:   0 Loss: 0.23134 Accuracy: 0.96875\n",
      "Test Loss: 0.47466 Accuracy: 0.81230\n",
      "Epoch: 150 Batch:   0 Loss: 0.22961 Accuracy: 0.96875\n",
      "Epoch: 151 Batch:   0 Loss: 0.22792 Accuracy: 0.96875\n",
      "Epoch: 152 Batch:   0 Loss: 0.22627 Accuracy: 0.96875\n",
      "Epoch: 153 Batch:   0 Loss: 0.22465 Accuracy: 0.96875\n",
      "Epoch: 154 Batch:   0 Loss: 0.22307 Accuracy: 0.96875\n",
      "Epoch: 155 Batch:   0 Loss: 0.22152 Accuracy: 0.96875\n",
      "Epoch: 156 Batch:   0 Loss: 0.22000 Accuracy: 0.96875\n",
      "Epoch: 157 Batch:   0 Loss: 0.21852 Accuracy: 0.96875\n",
      "Epoch: 158 Batch:   0 Loss: 0.21707 Accuracy: 0.96875\n",
      "Epoch: 159 Batch:   0 Loss: 0.21566 Accuracy: 0.96875\n",
      "Test Loss: 0.47392 Accuracy: 0.81475\n",
      "Epoch: 160 Batch:   0 Loss: 0.21427 Accuracy: 0.96875\n",
      "Epoch: 161 Batch:   0 Loss: 0.21291 Accuracy: 0.96875\n",
      "Epoch: 162 Batch:   0 Loss: 0.21159 Accuracy: 0.96875\n",
      "Epoch: 163 Batch:   0 Loss: 0.21029 Accuracy: 0.96875\n",
      "Epoch: 164 Batch:   0 Loss: 0.20903 Accuracy: 0.96875\n",
      "Epoch: 165 Batch:   0 Loss: 0.20779 Accuracy: 0.96875\n",
      "Epoch: 166 Batch:   0 Loss: 0.20657 Accuracy: 0.96875\n",
      "Epoch: 167 Batch:   0 Loss: 0.20538 Accuracy: 0.96875\n",
      "Epoch: 168 Batch:   0 Loss: 0.20422 Accuracy: 0.96875\n",
      "Epoch: 169 Batch:   0 Loss: 0.20309 Accuracy: 0.96875\n",
      "Test Loss: 0.47397 Accuracy: 0.81492\n",
      "Epoch: 170 Batch:   0 Loss: 0.20197 Accuracy: 0.96875\n",
      "Epoch: 171 Batch:   0 Loss: 0.20088 Accuracy: 0.96875\n",
      "Epoch: 172 Batch:   0 Loss: 0.19982 Accuracy: 0.96875\n",
      "Epoch: 173 Batch:   0 Loss: 0.19878 Accuracy: 0.96875\n",
      "Epoch: 174 Batch:   0 Loss: 0.19776 Accuracy: 0.96875\n",
      "Epoch: 175 Batch:   0 Loss: 0.19676 Accuracy: 0.96875\n",
      "Epoch: 176 Batch:   0 Loss: 0.19579 Accuracy: 0.96875\n",
      "Epoch: 177 Batch:   0 Loss: 0.19483 Accuracy: 0.96875\n",
      "Epoch: 178 Batch:   0 Loss: 0.19389 Accuracy: 0.96875\n",
      "Epoch: 179 Batch:   0 Loss: 0.19298 Accuracy: 0.96875\n",
      "Test Loss: 0.47468 Accuracy: 0.81560\n",
      "Epoch: 180 Batch:   0 Loss: 0.19208 Accuracy: 0.96875\n",
      "Epoch: 181 Batch:   0 Loss: 0.19120 Accuracy: 0.96875\n",
      "Epoch: 182 Batch:   0 Loss: 0.19034 Accuracy: 0.96875\n",
      "Epoch: 183 Batch:   0 Loss: 0.18949 Accuracy: 0.96875\n",
      "Epoch: 184 Batch:   0 Loss: 0.18866 Accuracy: 0.96875\n",
      "Epoch: 185 Batch:   0 Loss: 0.18785 Accuracy: 0.96875\n",
      "Epoch: 186 Batch:   0 Loss: 0.18706 Accuracy: 0.96875\n",
      "Epoch: 187 Batch:   0 Loss: 0.18628 Accuracy: 0.96875\n",
      "Epoch: 188 Batch:   0 Loss: 0.18551 Accuracy: 0.96875\n",
      "Epoch: 189 Batch:   0 Loss: 0.18477 Accuracy: 0.96875\n",
      "Test Loss: 0.47581 Accuracy: 0.81551\n",
      "Epoch: 190 Batch:   0 Loss: 0.18403 Accuracy: 0.96875\n",
      "Epoch: 191 Batch:   0 Loss: 0.18331 Accuracy: 0.96875\n",
      "Epoch: 192 Batch:   0 Loss: 0.18261 Accuracy: 0.96875\n",
      "Epoch: 193 Batch:   0 Loss: 0.18191 Accuracy: 0.96875\n",
      "Epoch: 194 Batch:   0 Loss: 0.18123 Accuracy: 0.96875\n",
      "Epoch: 195 Batch:   0 Loss: 0.18057 Accuracy: 0.96875\n",
      "Epoch: 196 Batch:   0 Loss: 0.17991 Accuracy: 0.96875\n",
      "Epoch: 197 Batch:   0 Loss: 0.17928 Accuracy: 0.96875\n",
      "Epoch: 198 Batch:   0 Loss: 0.17865 Accuracy: 0.96875\n",
      "Epoch: 199 Batch:   0 Loss: 0.17802 Accuracy: 0.96875\n",
      "Test Loss: 0.47723 Accuracy: 0.81543\n",
      "Epoch: 200 Batch:   0 Loss: 0.17742 Accuracy: 0.96875\n",
      "Epoch: 201 Batch:   0 Loss: 0.17682 Accuracy: 0.96875\n",
      "Epoch: 202 Batch:   0 Loss: 0.17623 Accuracy: 0.96875\n",
      "Epoch: 203 Batch:   0 Loss: 0.17565 Accuracy: 0.96875\n",
      "Epoch: 204 Batch:   0 Loss: 0.17509 Accuracy: 0.96875\n",
      "Epoch: 205 Batch:   0 Loss: 0.17453 Accuracy: 0.96875\n",
      "Epoch: 206 Batch:   0 Loss: 0.17399 Accuracy: 0.96875\n",
      "Epoch: 207 Batch:   0 Loss: 0.17345 Accuracy: 0.96875\n",
      "Epoch: 208 Batch:   0 Loss: 0.17293 Accuracy: 0.96875\n",
      "Epoch: 209 Batch:   0 Loss: 0.17241 Accuracy: 0.96875\n",
      "Test Loss: 0.47880 Accuracy: 0.81542\n",
      "Epoch: 210 Batch:   0 Loss: 0.17191 Accuracy: 0.96875\n",
      "Epoch: 211 Batch:   0 Loss: 0.17141 Accuracy: 0.96875\n",
      "Epoch: 212 Batch:   0 Loss: 0.17092 Accuracy: 0.96875\n",
      "Epoch: 213 Batch:   0 Loss: 0.17044 Accuracy: 0.96875\n",
      "Epoch: 214 Batch:   0 Loss: 0.16997 Accuracy: 0.96875\n",
      "Epoch: 215 Batch:   0 Loss: 0.16950 Accuracy: 0.96875\n",
      "Epoch: 216 Batch:   0 Loss: 0.16904 Accuracy: 0.96875\n",
      "Epoch: 217 Batch:   0 Loss: 0.16859 Accuracy: 0.96875\n",
      "Epoch: 218 Batch:   0 Loss: 0.16814 Accuracy: 0.96875\n",
      "Epoch: 219 Batch:   0 Loss: 0.16771 Accuracy: 0.96875\n",
      "Test Loss: 0.48048 Accuracy: 0.81610\n",
      "Epoch: 220 Batch:   0 Loss: 0.16728 Accuracy: 0.96875\n",
      "Epoch: 221 Batch:   0 Loss: 0.16687 Accuracy: 0.96875\n",
      "Epoch: 222 Batch:   0 Loss: 0.16646 Accuracy: 0.96875\n",
      "Epoch: 223 Batch:   0 Loss: 0.16605 Accuracy: 0.96875\n",
      "Epoch: 224 Batch:   0 Loss: 0.16566 Accuracy: 0.96875\n",
      "Epoch: 225 Batch:   0 Loss: 0.16527 Accuracy: 0.96875\n",
      "Epoch: 226 Batch:   0 Loss: 0.16489 Accuracy: 0.96875\n",
      "Epoch: 227 Batch:   0 Loss: 0.16451 Accuracy: 0.96875\n",
      "Epoch: 228 Batch:   0 Loss: 0.16414 Accuracy: 0.96875\n",
      "Epoch: 229 Batch:   0 Loss: 0.16378 Accuracy: 0.96875\n",
      "Test Loss: 0.48225 Accuracy: 0.81534\n",
      "Epoch: 230 Batch:   0 Loss: 0.16342 Accuracy: 0.96875\n",
      "Epoch: 231 Batch:   0 Loss: 0.16307 Accuracy: 0.96875\n",
      "Epoch: 232 Batch:   0 Loss: 0.16272 Accuracy: 0.96875\n",
      "Epoch: 233 Batch:   0 Loss: 0.16237 Accuracy: 0.96875\n",
      "Epoch: 234 Batch:   0 Loss: 0.16204 Accuracy: 0.96875\n",
      "Epoch: 235 Batch:   0 Loss: 0.16170 Accuracy: 0.96875\n",
      "Epoch: 236 Batch:   0 Loss: 0.16137 Accuracy: 0.96875\n",
      "Epoch: 237 Batch:   0 Loss: 0.16104 Accuracy: 0.96875\n",
      "Epoch: 238 Batch:   0 Loss: 0.16072 Accuracy: 0.96875\n",
      "Epoch: 239 Batch:   0 Loss: 0.16040 Accuracy: 0.96875\n",
      "Test Loss: 0.48399 Accuracy: 0.81466\n",
      "Epoch: 240 Batch:   0 Loss: 0.16009 Accuracy: 0.96875\n",
      "Epoch: 241 Batch:   0 Loss: 0.15979 Accuracy: 0.96875\n",
      "Epoch: 242 Batch:   0 Loss: 0.15949 Accuracy: 0.96875\n",
      "Epoch: 243 Batch:   0 Loss: 0.15920 Accuracy: 0.96875\n",
      "Epoch: 244 Batch:   0 Loss: 0.15891 Accuracy: 0.96875\n",
      "Epoch: 245 Batch:   0 Loss: 0.15862 Accuracy: 0.96875\n",
      "Epoch: 246 Batch:   0 Loss: 0.15834 Accuracy: 0.96875\n",
      "Epoch: 247 Batch:   0 Loss: 0.15807 Accuracy: 0.96875\n",
      "Epoch: 248 Batch:   0 Loss: 0.15779 Accuracy: 0.96875\n",
      "Epoch: 249 Batch:   0 Loss: 0.15753 Accuracy: 0.96875\n",
      "Test Loss: 0.48567 Accuracy: 0.81390\n",
      "Epoch: 250 Batch:   0 Loss: 0.15726 Accuracy: 0.96875\n",
      "Epoch: 251 Batch:   0 Loss: 0.15700 Accuracy: 0.96875\n",
      "Epoch: 252 Batch:   0 Loss: 0.15674 Accuracy: 0.96875\n",
      "Epoch: 253 Batch:   0 Loss: 0.15649 Accuracy: 0.96875\n",
      "Epoch: 254 Batch:   0 Loss: 0.15623 Accuracy: 0.96875\n",
      "Epoch: 255 Batch:   0 Loss: 0.15598 Accuracy: 0.96875\n",
      "Epoch: 256 Batch:   0 Loss: 0.15574 Accuracy: 0.96875\n",
      "Epoch: 257 Batch:   0 Loss: 0.15550 Accuracy: 0.96875\n",
      "Epoch: 258 Batch:   0 Loss: 0.15526 Accuracy: 0.96875\n",
      "Epoch: 259 Batch:   0 Loss: 0.15502 Accuracy: 0.96875\n",
      "Test Loss: 0.48732 Accuracy: 0.81416\n",
      "Epoch: 260 Batch:   0 Loss: 0.15478 Accuracy: 0.96875\n",
      "Epoch: 261 Batch:   0 Loss: 0.15455 Accuracy: 0.96875\n",
      "Epoch: 262 Batch:   0 Loss: 0.15432 Accuracy: 0.96875\n",
      "Epoch: 263 Batch:   0 Loss: 0.15409 Accuracy: 0.96875\n",
      "Epoch: 264 Batch:   0 Loss: 0.15387 Accuracy: 0.96875\n",
      "Epoch: 265 Batch:   0 Loss: 0.15365 Accuracy: 0.96875\n",
      "Epoch: 266 Batch:   0 Loss: 0.15343 Accuracy: 0.96875\n",
      "Epoch: 267 Batch:   0 Loss: 0.15321 Accuracy: 0.96875\n",
      "Epoch: 268 Batch:   0 Loss: 0.15299 Accuracy: 0.96875\n",
      "Epoch: 269 Batch:   0 Loss: 0.15277 Accuracy: 0.96875\n",
      "Test Loss: 0.48890 Accuracy: 0.81356\n",
      "Epoch: 270 Batch:   0 Loss: 0.15256 Accuracy: 0.96875\n",
      "Epoch: 271 Batch:   0 Loss: 0.15236 Accuracy: 0.96875\n",
      "Epoch: 272 Batch:   0 Loss: 0.15215 Accuracy: 0.96875\n",
      "Epoch: 273 Batch:   0 Loss: 0.15195 Accuracy: 0.96875\n",
      "Epoch: 274 Batch:   0 Loss: 0.15175 Accuracy: 0.96875\n",
      "Epoch: 275 Batch:   0 Loss: 0.15155 Accuracy: 0.96875\n",
      "Epoch: 276 Batch:   0 Loss: 0.15135 Accuracy: 0.96875\n",
      "Epoch: 277 Batch:   0 Loss: 0.15115 Accuracy: 0.96875\n",
      "Epoch: 278 Batch:   0 Loss: 0.15096 Accuracy: 0.96875\n",
      "Epoch: 279 Batch:   0 Loss: 0.15077 Accuracy: 0.96875\n",
      "Test Loss: 0.49045 Accuracy: 0.81339\n",
      "Epoch: 280 Batch:   0 Loss: 0.15058 Accuracy: 0.96875\n",
      "Epoch: 281 Batch:   0 Loss: 0.15039 Accuracy: 0.96875\n",
      "Epoch: 282 Batch:   0 Loss: 0.15020 Accuracy: 0.96875\n",
      "Epoch: 283 Batch:   0 Loss: 0.15002 Accuracy: 0.96875\n",
      "Epoch: 284 Batch:   0 Loss: 0.14984 Accuracy: 0.96875\n",
      "Epoch: 285 Batch:   0 Loss: 0.14966 Accuracy: 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286 Batch:   0 Loss: 0.14948 Accuracy: 0.96875\n",
      "Epoch: 287 Batch:   0 Loss: 0.14931 Accuracy: 0.96875\n",
      "Epoch: 288 Batch:   0 Loss: 0.14914 Accuracy: 0.96875\n",
      "Epoch: 289 Batch:   0 Loss: 0.14897 Accuracy: 0.96875\n",
      "Test Loss: 0.49194 Accuracy: 0.81272\n",
      "Epoch: 290 Batch:   0 Loss: 0.14880 Accuracy: 0.96875\n",
      "Epoch: 291 Batch:   0 Loss: 0.14863 Accuracy: 0.96875\n",
      "Epoch: 292 Batch:   0 Loss: 0.14847 Accuracy: 0.96875\n",
      "Epoch: 293 Batch:   0 Loss: 0.14830 Accuracy: 0.96875\n",
      "Epoch: 294 Batch:   0 Loss: 0.14814 Accuracy: 0.96875\n",
      "Epoch: 295 Batch:   0 Loss: 0.14797 Accuracy: 0.96875\n",
      "Epoch: 296 Batch:   0 Loss: 0.14781 Accuracy: 0.96875\n",
      "Epoch: 297 Batch:   0 Loss: 0.14765 Accuracy: 0.96875\n",
      "Epoch: 298 Batch:   0 Loss: 0.14750 Accuracy: 0.96875\n",
      "Epoch: 299 Batch:   0 Loss: 0.14734 Accuracy: 0.96875\n",
      "Test Loss: 0.49339 Accuracy: 0.81094\n",
      "Epoch: 300 Batch:   0 Loss: 0.14719 Accuracy: 0.96875\n",
      "Epoch: 301 Batch:   0 Loss: 0.14703 Accuracy: 0.96875\n",
      "Epoch: 302 Batch:   0 Loss: 0.14688 Accuracy: 0.96875\n",
      "Epoch: 303 Batch:   0 Loss: 0.14673 Accuracy: 0.96875\n",
      "Epoch: 304 Batch:   0 Loss: 0.14659 Accuracy: 0.96875\n",
      "Epoch: 305 Batch:   0 Loss: 0.14644 Accuracy: 0.96875\n",
      "Epoch: 306 Batch:   0 Loss: 0.14629 Accuracy: 0.96875\n",
      "Epoch: 307 Batch:   0 Loss: 0.14614 Accuracy: 0.96875\n",
      "Epoch: 308 Batch:   0 Loss: 0.14600 Accuracy: 0.96875\n",
      "Epoch: 309 Batch:   0 Loss: 0.14585 Accuracy: 0.96875\n",
      "Test Loss: 0.49477 Accuracy: 0.80993\n",
      "Epoch: 310 Batch:   0 Loss: 0.14571 Accuracy: 0.96875\n",
      "Epoch: 311 Batch:   0 Loss: 0.14557 Accuracy: 0.96875\n",
      "Epoch: 312 Batch:   0 Loss: 0.14543 Accuracy: 0.96875\n",
      "Epoch: 313 Batch:   0 Loss: 0.14529 Accuracy: 0.96875\n",
      "Epoch: 314 Batch:   0 Loss: 0.14515 Accuracy: 0.96875\n",
      "Epoch: 315 Batch:   0 Loss: 0.14501 Accuracy: 0.96875\n",
      "Epoch: 316 Batch:   0 Loss: 0.14487 Accuracy: 0.96875\n",
      "Epoch: 317 Batch:   0 Loss: 0.14474 Accuracy: 0.96875\n",
      "Epoch: 318 Batch:   0 Loss: 0.14460 Accuracy: 0.96875\n",
      "Epoch: 319 Batch:   0 Loss: 0.14447 Accuracy: 0.96875\n",
      "Test Loss: 0.49608 Accuracy: 0.81018\n",
      "Epoch: 320 Batch:   0 Loss: 0.14434 Accuracy: 0.96875\n",
      "Epoch: 321 Batch:   0 Loss: 0.14421 Accuracy: 0.96875\n",
      "Epoch: 322 Batch:   0 Loss: 0.14408 Accuracy: 0.96875\n",
      "Epoch: 323 Batch:   0 Loss: 0.14395 Accuracy: 0.96875\n",
      "Epoch: 324 Batch:   0 Loss: 0.14382 Accuracy: 0.96875\n",
      "Epoch: 325 Batch:   0 Loss: 0.14369 Accuracy: 0.96875\n",
      "Epoch: 326 Batch:   0 Loss: 0.14357 Accuracy: 0.96875\n",
      "Epoch: 327 Batch:   0 Loss: 0.14344 Accuracy: 0.96875\n",
      "Epoch: 328 Batch:   0 Loss: 0.14332 Accuracy: 0.96875\n",
      "Epoch: 329 Batch:   0 Loss: 0.14319 Accuracy: 0.96875\n",
      "Test Loss: 0.49736 Accuracy: 0.80984\n",
      "Epoch: 330 Batch:   0 Loss: 0.14307 Accuracy: 0.96875\n",
      "Epoch: 331 Batch:   0 Loss: 0.14295 Accuracy: 0.96875\n",
      "Epoch: 332 Batch:   0 Loss: 0.14283 Accuracy: 0.96875\n",
      "Epoch: 333 Batch:   0 Loss: 0.14271 Accuracy: 0.96875\n",
      "Epoch: 334 Batch:   0 Loss: 0.14259 Accuracy: 0.96875\n",
      "Epoch: 335 Batch:   0 Loss: 0.14247 Accuracy: 0.96875\n",
      "Epoch: 336 Batch:   0 Loss: 0.14235 Accuracy: 0.96875\n",
      "Epoch: 337 Batch:   0 Loss: 0.14224 Accuracy: 0.96875\n",
      "Epoch: 338 Batch:   0 Loss: 0.14212 Accuracy: 0.96875\n",
      "Epoch: 339 Batch:   0 Loss: 0.14200 Accuracy: 0.96875\n",
      "Test Loss: 0.49861 Accuracy: 0.80967\n",
      "Epoch: 340 Batch:   0 Loss: 0.14188 Accuracy: 0.96875\n",
      "Epoch: 341 Batch:   0 Loss: 0.14177 Accuracy: 0.96875\n",
      "Epoch: 342 Batch:   0 Loss: 0.14165 Accuracy: 0.96875\n",
      "Epoch: 343 Batch:   0 Loss: 0.14154 Accuracy: 0.96875\n",
      "Epoch: 344 Batch:   0 Loss: 0.14142 Accuracy: 0.96875\n",
      "Epoch: 345 Batch:   0 Loss: 0.14130 Accuracy: 0.96875\n",
      "Epoch: 346 Batch:   0 Loss: 0.14119 Accuracy: 0.96875\n",
      "Epoch: 347 Batch:   0 Loss: 0.14108 Accuracy: 0.96875\n",
      "Epoch: 348 Batch:   0 Loss: 0.14097 Accuracy: 0.96875\n",
      "Epoch: 349 Batch:   0 Loss: 0.14085 Accuracy: 0.96875\n",
      "Test Loss: 0.49982 Accuracy: 0.80984\n",
      "Epoch: 350 Batch:   0 Loss: 0.14074 Accuracy: 0.96875\n",
      "Epoch: 351 Batch:   0 Loss: 0.14063 Accuracy: 0.96875\n",
      "Epoch: 352 Batch:   0 Loss: 0.14052 Accuracy: 0.96875\n",
      "Epoch: 353 Batch:   0 Loss: 0.14042 Accuracy: 0.96875\n",
      "Epoch: 354 Batch:   0 Loss: 0.14031 Accuracy: 0.96875\n",
      "Epoch: 355 Batch:   0 Loss: 0.14020 Accuracy: 0.96875\n",
      "Epoch: 356 Batch:   0 Loss: 0.14010 Accuracy: 0.96875\n",
      "Epoch: 357 Batch:   0 Loss: 0.13999 Accuracy: 0.96875\n",
      "Epoch: 358 Batch:   0 Loss: 0.13989 Accuracy: 0.96875\n",
      "Epoch: 359 Batch:   0 Loss: 0.13979 Accuracy: 0.96875\n",
      "Test Loss: 0.50101 Accuracy: 0.80924\n",
      "Epoch: 360 Batch:   0 Loss: 0.13969 Accuracy: 0.96875\n",
      "Epoch: 361 Batch:   0 Loss: 0.13959 Accuracy: 0.96875\n",
      "Epoch: 362 Batch:   0 Loss: 0.13949 Accuracy: 0.96875\n",
      "Epoch: 363 Batch:   0 Loss: 0.13939 Accuracy: 0.96875\n",
      "Epoch: 364 Batch:   0 Loss: 0.13929 Accuracy: 0.96875\n",
      "Epoch: 365 Batch:   0 Loss: 0.13919 Accuracy: 0.96875\n",
      "Epoch: 366 Batch:   0 Loss: 0.13909 Accuracy: 0.96875\n",
      "Epoch: 367 Batch:   0 Loss: 0.13899 Accuracy: 0.96875\n",
      "Epoch: 368 Batch:   0 Loss: 0.13889 Accuracy: 0.96875\n",
      "Epoch: 369 Batch:   0 Loss: 0.13879 Accuracy: 0.96875\n",
      "Test Loss: 0.50218 Accuracy: 0.80925\n",
      "Epoch: 370 Batch:   0 Loss: 0.13869 Accuracy: 0.96875\n",
      "Epoch: 371 Batch:   0 Loss: 0.13858 Accuracy: 0.96875\n",
      "Epoch: 372 Batch:   0 Loss: 0.13843 Accuracy: 0.96875\n",
      "Epoch: 373 Batch:   0 Loss: 0.13831 Accuracy: 0.96875\n",
      "Epoch: 374 Batch:   0 Loss: 0.13822 Accuracy: 0.96875\n",
      "Epoch: 375 Batch:   0 Loss: 0.13812 Accuracy: 0.96875\n",
      "Epoch: 376 Batch:   0 Loss: 0.13803 Accuracy: 0.96875\n",
      "Epoch: 377 Batch:   0 Loss: 0.13793 Accuracy: 0.96875\n",
      "Epoch: 378 Batch:   0 Loss: 0.13784 Accuracy: 0.96875\n",
      "Epoch: 379 Batch:   0 Loss: 0.13774 Accuracy: 0.96875\n",
      "Test Loss: 0.50330 Accuracy: 0.80975\n",
      "Epoch: 380 Batch:   0 Loss: 0.13765 Accuracy: 0.96875\n",
      "Epoch: 381 Batch:   0 Loss: 0.13755 Accuracy: 0.96875\n",
      "Epoch: 382 Batch:   0 Loss: 0.13746 Accuracy: 0.96875\n",
      "Epoch: 383 Batch:   0 Loss: 0.13736 Accuracy: 0.96875\n",
      "Epoch: 384 Batch:   0 Loss: 0.13727 Accuracy: 0.96875\n",
      "Epoch: 385 Batch:   0 Loss: 0.13717 Accuracy: 0.96875\n",
      "Epoch: 386 Batch:   0 Loss: 0.13708 Accuracy: 0.96875\n",
      "Epoch: 387 Batch:   0 Loss: 0.13699 Accuracy: 0.96875\n",
      "Epoch: 388 Batch:   0 Loss: 0.13690 Accuracy: 0.96875\n",
      "Epoch: 389 Batch:   0 Loss: 0.13681 Accuracy: 0.96875\n",
      "Test Loss: 0.50443 Accuracy: 0.80882\n",
      "Epoch: 390 Batch:   0 Loss: 0.13672 Accuracy: 0.96875\n",
      "Epoch: 391 Batch:   0 Loss: 0.13663 Accuracy: 0.96875\n",
      "Epoch: 392 Batch:   0 Loss: 0.13654 Accuracy: 0.96875\n",
      "Epoch: 393 Batch:   0 Loss: 0.13645 Accuracy: 0.96875\n",
      "Epoch: 394 Batch:   0 Loss: 0.13636 Accuracy: 0.96875\n",
      "Epoch: 395 Batch:   0 Loss: 0.13627 Accuracy: 0.96875\n",
      "Epoch: 396 Batch:   0 Loss: 0.13618 Accuracy: 0.96875\n",
      "Epoch: 397 Batch:   0 Loss: 0.13609 Accuracy: 0.96875\n",
      "Epoch: 398 Batch:   0 Loss: 0.13601 Accuracy: 0.96875\n",
      "Epoch: 399 Batch:   0 Loss: 0.13592 Accuracy: 0.96875\n",
      "Test Loss: 0.50552 Accuracy: 0.80832\n",
      "Epoch: 400 Batch:   0 Loss: 0.13584 Accuracy: 0.96875\n",
      "Epoch: 401 Batch:   0 Loss: 0.13575 Accuracy: 0.96875\n",
      "Epoch: 402 Batch:   0 Loss: 0.13567 Accuracy: 0.96875\n",
      "Epoch: 403 Batch:   0 Loss: 0.13558 Accuracy: 0.96875\n",
      "Epoch: 404 Batch:   0 Loss: 0.13549 Accuracy: 0.96875\n",
      "Epoch: 405 Batch:   0 Loss: 0.13541 Accuracy: 0.96875\n",
      "Epoch: 406 Batch:   0 Loss: 0.13532 Accuracy: 0.96875\n",
      "Epoch: 407 Batch:   0 Loss: 0.13524 Accuracy: 0.96875\n",
      "Epoch: 408 Batch:   0 Loss: 0.13515 Accuracy: 0.96875\n",
      "Epoch: 409 Batch:   0 Loss: 0.13506 Accuracy: 0.96875\n",
      "Test Loss: 0.50660 Accuracy: 0.80857\n",
      "Epoch: 410 Batch:   0 Loss: 0.13498 Accuracy: 0.96875\n",
      "Epoch: 411 Batch:   0 Loss: 0.13490 Accuracy: 0.96875\n",
      "Epoch: 412 Batch:   0 Loss: 0.13481 Accuracy: 0.96875\n",
      "Epoch: 413 Batch:   0 Loss: 0.13473 Accuracy: 0.96875\n",
      "Epoch: 414 Batch:   0 Loss: 0.13465 Accuracy: 0.96875\n",
      "Epoch: 415 Batch:   0 Loss: 0.13456 Accuracy: 0.96875\n",
      "Epoch: 416 Batch:   0 Loss: 0.13448 Accuracy: 0.96875\n",
      "Epoch: 417 Batch:   0 Loss: 0.13440 Accuracy: 0.96875\n",
      "Epoch: 418 Batch:   0 Loss: 0.13432 Accuracy: 0.96875\n",
      "Epoch: 419 Batch:   0 Loss: 0.13424 Accuracy: 0.96875\n",
      "Test Loss: 0.50764 Accuracy: 0.80781\n",
      "Epoch: 420 Batch:   0 Loss: 0.13416 Accuracy: 0.96875\n",
      "Epoch: 421 Batch:   0 Loss: 0.13408 Accuracy: 0.96875\n",
      "Epoch: 422 Batch:   0 Loss: 0.13400 Accuracy: 0.96875\n",
      "Epoch: 423 Batch:   0 Loss: 0.13392 Accuracy: 0.96875\n",
      "Epoch: 424 Batch:   0 Loss: 0.13384 Accuracy: 0.96875\n",
      "Epoch: 425 Batch:   0 Loss: 0.13376 Accuracy: 0.96875\n",
      "Epoch: 426 Batch:   0 Loss: 0.13369 Accuracy: 0.96875\n",
      "Epoch: 427 Batch:   0 Loss: 0.13361 Accuracy: 0.96875\n",
      "Epoch: 428 Batch:   0 Loss: 0.13353 Accuracy: 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 429 Batch:   0 Loss: 0.13345 Accuracy: 0.96875\n",
      "Test Loss: 0.50861 Accuracy: 0.80739\n",
      "Epoch: 430 Batch:   0 Loss: 0.13337 Accuracy: 0.96875\n",
      "Epoch: 431 Batch:   0 Loss: 0.13330 Accuracy: 0.96875\n",
      "Epoch: 432 Batch:   0 Loss: 0.13322 Accuracy: 0.96875\n",
      "Epoch: 433 Batch:   0 Loss: 0.13314 Accuracy: 0.96875\n",
      "Epoch: 434 Batch:   0 Loss: 0.13307 Accuracy: 0.96875\n",
      "Epoch: 435 Batch:   0 Loss: 0.13299 Accuracy: 0.96875\n",
      "Epoch: 436 Batch:   0 Loss: 0.13292 Accuracy: 0.96875\n",
      "Epoch: 437 Batch:   0 Loss: 0.13284 Accuracy: 0.96875\n",
      "Epoch: 438 Batch:   0 Loss: 0.13276 Accuracy: 0.96875\n",
      "Epoch: 439 Batch:   0 Loss: 0.13269 Accuracy: 0.96875\n",
      "Test Loss: 0.50955 Accuracy: 0.80670\n",
      "Epoch: 440 Batch:   0 Loss: 0.13261 Accuracy: 0.96875\n",
      "Epoch: 441 Batch:   0 Loss: 0.13254 Accuracy: 0.96875\n",
      "Epoch: 442 Batch:   0 Loss: 0.13246 Accuracy: 0.96875\n",
      "Epoch: 443 Batch:   0 Loss: 0.13239 Accuracy: 0.96875\n",
      "Epoch: 444 Batch:   0 Loss: 0.13231 Accuracy: 0.96875\n",
      "Epoch: 445 Batch:   0 Loss: 0.13224 Accuracy: 0.96875\n",
      "Epoch: 446 Batch:   0 Loss: 0.13216 Accuracy: 0.96875\n",
      "Epoch: 447 Batch:   0 Loss: 0.13209 Accuracy: 0.96875\n",
      "Epoch: 448 Batch:   0 Loss: 0.13202 Accuracy: 0.96875\n",
      "Epoch: 449 Batch:   0 Loss: 0.13194 Accuracy: 0.96875\n",
      "Test Loss: 0.51047 Accuracy: 0.80654\n",
      "Epoch: 450 Batch:   0 Loss: 0.13187 Accuracy: 0.96875\n",
      "Epoch: 451 Batch:   0 Loss: 0.13180 Accuracy: 0.96875\n",
      "Epoch: 452 Batch:   0 Loss: 0.13172 Accuracy: 0.96875\n",
      "Epoch: 453 Batch:   0 Loss: 0.13165 Accuracy: 0.96875\n",
      "Epoch: 454 Batch:   0 Loss: 0.13158 Accuracy: 0.96875\n",
      "Epoch: 455 Batch:   0 Loss: 0.13151 Accuracy: 0.96875\n",
      "Epoch: 456 Batch:   0 Loss: 0.13143 Accuracy: 0.96875\n",
      "Epoch: 457 Batch:   0 Loss: 0.13136 Accuracy: 0.96875\n",
      "Epoch: 458 Batch:   0 Loss: 0.13129 Accuracy: 0.96875\n",
      "Epoch: 459 Batch:   0 Loss: 0.13121 Accuracy: 0.96875\n",
      "Test Loss: 0.51135 Accuracy: 0.80670\n",
      "Epoch: 460 Batch:   0 Loss: 0.13114 Accuracy: 0.96875\n",
      "Epoch: 461 Batch:   0 Loss: 0.13107 Accuracy: 0.96875\n",
      "Epoch: 462 Batch:   0 Loss: 0.13099 Accuracy: 0.96875\n",
      "Epoch: 463 Batch:   0 Loss: 0.13092 Accuracy: 0.96875\n",
      "Epoch: 464 Batch:   0 Loss: 0.13085 Accuracy: 0.96875\n",
      "Epoch: 465 Batch:   0 Loss: 0.13077 Accuracy: 0.96875\n",
      "Epoch: 466 Batch:   0 Loss: 0.13070 Accuracy: 0.96875\n",
      "Epoch: 467 Batch:   0 Loss: 0.13063 Accuracy: 0.96875\n",
      "Epoch: 468 Batch:   0 Loss: 0.13056 Accuracy: 0.96875\n",
      "Epoch: 469 Batch:   0 Loss: 0.13049 Accuracy: 0.96875\n",
      "Test Loss: 0.51221 Accuracy: 0.80654\n",
      "Epoch: 470 Batch:   0 Loss: 0.13042 Accuracy: 0.96875\n",
      "Epoch: 471 Batch:   0 Loss: 0.13035 Accuracy: 0.96875\n",
      "Epoch: 472 Batch:   0 Loss: 0.13028 Accuracy: 0.96875\n",
      "Epoch: 473 Batch:   0 Loss: 0.13021 Accuracy: 0.96875\n",
      "Epoch: 474 Batch:   0 Loss: 0.13014 Accuracy: 0.96875\n",
      "Epoch: 475 Batch:   0 Loss: 0.13007 Accuracy: 0.96875\n",
      "Epoch: 476 Batch:   0 Loss: 0.13001 Accuracy: 0.96875\n",
      "Epoch: 477 Batch:   0 Loss: 0.12994 Accuracy: 0.96875\n",
      "Epoch: 478 Batch:   0 Loss: 0.12987 Accuracy: 0.96875\n",
      "Epoch: 479 Batch:   0 Loss: 0.12980 Accuracy: 0.96875\n",
      "Test Loss: 0.51304 Accuracy: 0.80670\n",
      "Epoch: 480 Batch:   0 Loss: 0.12974 Accuracy: 0.96875\n",
      "Epoch: 481 Batch:   0 Loss: 0.12967 Accuracy: 0.96875\n",
      "Epoch: 482 Batch:   0 Loss: 0.12960 Accuracy: 0.96875\n",
      "Epoch: 483 Batch:   0 Loss: 0.12953 Accuracy: 0.96875\n",
      "Epoch: 484 Batch:   0 Loss: 0.12946 Accuracy: 0.96875\n",
      "Epoch: 485 Batch:   0 Loss: 0.12940 Accuracy: 0.96875\n",
      "Epoch: 486 Batch:   0 Loss: 0.12933 Accuracy: 0.96875\n",
      "Epoch: 487 Batch:   0 Loss: 0.12926 Accuracy: 0.96875\n",
      "Epoch: 488 Batch:   0 Loss: 0.12919 Accuracy: 0.96875\n",
      "Epoch: 489 Batch:   0 Loss: 0.12913 Accuracy: 0.96875\n",
      "Test Loss: 0.51389 Accuracy: 0.80696\n",
      "Epoch: 490 Batch:   0 Loss: 0.12906 Accuracy: 0.96875\n",
      "Epoch: 491 Batch:   0 Loss: 0.12899 Accuracy: 0.96875\n",
      "Epoch: 492 Batch:   0 Loss: 0.12893 Accuracy: 0.96875\n",
      "Epoch: 493 Batch:   0 Loss: 0.12886 Accuracy: 0.96875\n",
      "Epoch: 494 Batch:   0 Loss: 0.12879 Accuracy: 0.96875\n",
      "Epoch: 495 Batch:   0 Loss: 0.12873 Accuracy: 0.96875\n",
      "Epoch: 496 Batch:   0 Loss: 0.12866 Accuracy: 0.96875\n",
      "Epoch: 497 Batch:   0 Loss: 0.12859 Accuracy: 0.96875\n",
      "Epoch: 498 Batch:   0 Loss: 0.12853 Accuracy: 0.96875\n",
      "Epoch: 499 Batch:   0 Loss: 0.12846 Accuracy: 0.96875\n",
      "Test Loss: 0.51472 Accuracy: 0.80654\n",
      "Epoch: 500 Batch:   0 Loss: 0.12839 Accuracy: 0.96875\n",
      "Epoch: 501 Batch:   0 Loss: 0.12833 Accuracy: 0.96875\n",
      "Epoch: 502 Batch:   0 Loss: 0.12826 Accuracy: 0.96875\n",
      "Epoch: 503 Batch:   0 Loss: 0.12820 Accuracy: 0.96875\n",
      "Epoch: 504 Batch:   0 Loss: 0.12813 Accuracy: 0.96875\n",
      "Epoch: 505 Batch:   0 Loss: 0.12807 Accuracy: 0.96875\n",
      "Epoch: 506 Batch:   0 Loss: 0.12800 Accuracy: 0.96875\n",
      "Epoch: 507 Batch:   0 Loss: 0.12794 Accuracy: 0.96875\n",
      "Epoch: 508 Batch:   0 Loss: 0.12787 Accuracy: 0.96875\n",
      "Epoch: 509 Batch:   0 Loss: 0.12781 Accuracy: 0.96875\n",
      "Test Loss: 0.51554 Accuracy: 0.80628\n",
      "Epoch: 510 Batch:   0 Loss: 0.12775 Accuracy: 0.96875\n",
      "Epoch: 511 Batch:   0 Loss: 0.12768 Accuracy: 0.96875\n",
      "Epoch: 512 Batch:   0 Loss: 0.12761 Accuracy: 0.96875\n",
      "Epoch: 513 Batch:   0 Loss: 0.12755 Accuracy: 0.96875\n",
      "Epoch: 514 Batch:   0 Loss: 0.12748 Accuracy: 0.96875\n",
      "Epoch: 515 Batch:   0 Loss: 0.12742 Accuracy: 0.96875\n",
      "Epoch: 516 Batch:   0 Loss: 0.12735 Accuracy: 0.96875\n",
      "Epoch: 517 Batch:   0 Loss: 0.12729 Accuracy: 0.96875\n",
      "Epoch: 518 Batch:   0 Loss: 0.12722 Accuracy: 0.96875\n",
      "Epoch: 519 Batch:   0 Loss: 0.12715 Accuracy: 0.96875\n",
      "Test Loss: 0.51631 Accuracy: 0.80594\n",
      "Epoch: 520 Batch:   0 Loss: 0.12709 Accuracy: 0.96875\n",
      "Epoch: 521 Batch:   0 Loss: 0.12702 Accuracy: 0.96875\n",
      "Epoch: 522 Batch:   0 Loss: 0.12696 Accuracy: 0.96875\n",
      "Epoch: 523 Batch:   0 Loss: 0.12689 Accuracy: 0.96875\n",
      "Epoch: 524 Batch:   0 Loss: 0.12683 Accuracy: 0.96875\n",
      "Epoch: 525 Batch:   0 Loss: 0.12676 Accuracy: 0.96875\n",
      "Epoch: 526 Batch:   0 Loss: 0.12670 Accuracy: 0.96875\n",
      "Epoch: 527 Batch:   0 Loss: 0.12664 Accuracy: 0.96875\n",
      "Epoch: 528 Batch:   0 Loss: 0.12657 Accuracy: 0.96875\n",
      "Epoch: 529 Batch:   0 Loss: 0.12651 Accuracy: 0.96875\n",
      "Test Loss: 0.51705 Accuracy: 0.80636\n",
      "Epoch: 530 Batch:   0 Loss: 0.12645 Accuracy: 0.96875\n",
      "Epoch: 531 Batch:   0 Loss: 0.12639 Accuracy: 0.96875\n",
      "Epoch: 532 Batch:   0 Loss: 0.12633 Accuracy: 0.96875\n",
      "Epoch: 533 Batch:   0 Loss: 0.12627 Accuracy: 0.96875\n",
      "Epoch: 534 Batch:   0 Loss: 0.12621 Accuracy: 0.96875\n",
      "Epoch: 535 Batch:   0 Loss: 0.12614 Accuracy: 0.96875\n",
      "Epoch: 536 Batch:   0 Loss: 0.12608 Accuracy: 0.96875\n",
      "Epoch: 537 Batch:   0 Loss: 0.12602 Accuracy: 0.96875\n",
      "Epoch: 538 Batch:   0 Loss: 0.12596 Accuracy: 0.96875\n",
      "Epoch: 539 Batch:   0 Loss: 0.12590 Accuracy: 0.96875\n",
      "Test Loss: 0.51776 Accuracy: 0.80510\n",
      "Epoch: 540 Batch:   0 Loss: 0.12584 Accuracy: 0.96875\n",
      "Epoch: 541 Batch:   0 Loss: 0.12578 Accuracy: 0.96875\n",
      "Epoch: 542 Batch:   0 Loss: 0.12572 Accuracy: 0.96875\n",
      "Epoch: 543 Batch:   0 Loss: 0.12567 Accuracy: 0.96875\n",
      "Epoch: 544 Batch:   0 Loss: 0.12561 Accuracy: 0.96875\n",
      "Epoch: 545 Batch:   0 Loss: 0.12555 Accuracy: 0.96875\n",
      "Epoch: 546 Batch:   0 Loss: 0.12549 Accuracy: 0.96875\n",
      "Epoch: 547 Batch:   0 Loss: 0.12543 Accuracy: 0.96875\n",
      "Epoch: 548 Batch:   0 Loss: 0.12537 Accuracy: 0.96875\n",
      "Epoch: 549 Batch:   0 Loss: 0.12531 Accuracy: 0.96875\n",
      "Test Loss: 0.51845 Accuracy: 0.80450\n",
      "Epoch: 550 Batch:   0 Loss: 0.12525 Accuracy: 0.96875\n",
      "Epoch: 551 Batch:   0 Loss: 0.12519 Accuracy: 0.96875\n",
      "Epoch: 552 Batch:   0 Loss: 0.12513 Accuracy: 0.96875\n",
      "Epoch: 553 Batch:   0 Loss: 0.12507 Accuracy: 0.96875\n",
      "Epoch: 554 Batch:   0 Loss: 0.12502 Accuracy: 0.96875\n",
      "Epoch: 555 Batch:   0 Loss: 0.12496 Accuracy: 0.96875\n",
      "Epoch: 556 Batch:   0 Loss: 0.12490 Accuracy: 0.96875\n",
      "Epoch: 557 Batch:   0 Loss: 0.12484 Accuracy: 0.96875\n",
      "Epoch: 558 Batch:   0 Loss: 0.12478 Accuracy: 0.96875\n",
      "Epoch: 559 Batch:   0 Loss: 0.12473 Accuracy: 0.96875\n",
      "Test Loss: 0.51911 Accuracy: 0.80391\n",
      "Epoch: 560 Batch:   0 Loss: 0.12467 Accuracy: 0.96875\n",
      "Epoch: 561 Batch:   0 Loss: 0.12461 Accuracy: 0.96875\n",
      "Epoch: 562 Batch:   0 Loss: 0.12455 Accuracy: 0.96875\n",
      "Epoch: 563 Batch:   0 Loss: 0.12449 Accuracy: 0.96875\n",
      "Epoch: 564 Batch:   0 Loss: 0.12443 Accuracy: 0.96875\n",
      "Epoch: 565 Batch:   0 Loss: 0.12437 Accuracy: 0.96875\n",
      "Epoch: 566 Batch:   0 Loss: 0.12432 Accuracy: 0.96875\n",
      "Epoch: 567 Batch:   0 Loss: 0.12426 Accuracy: 0.96875\n",
      "Epoch: 568 Batch:   0 Loss: 0.12420 Accuracy: 0.96875\n",
      "Epoch: 569 Batch:   0 Loss: 0.12414 Accuracy: 0.96875\n",
      "Test Loss: 0.51975 Accuracy: 0.80442\n",
      "Epoch: 570 Batch:   0 Loss: 0.12408 Accuracy: 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 571 Batch:   0 Loss: 0.12402 Accuracy: 0.96875\n",
      "Epoch: 572 Batch:   0 Loss: 0.12397 Accuracy: 0.96875\n",
      "Epoch: 573 Batch:   0 Loss: 0.12391 Accuracy: 0.96875\n",
      "Epoch: 574 Batch:   0 Loss: 0.12385 Accuracy: 0.96875\n",
      "Epoch: 575 Batch:   0 Loss: 0.12379 Accuracy: 0.96875\n",
      "Epoch: 576 Batch:   0 Loss: 0.12374 Accuracy: 0.96875\n",
      "Epoch: 577 Batch:   0 Loss: 0.12368 Accuracy: 0.96875\n",
      "Epoch: 578 Batch:   0 Loss: 0.12362 Accuracy: 0.96875\n",
      "Epoch: 579 Batch:   0 Loss: 0.12357 Accuracy: 0.96875\n",
      "Test Loss: 0.52038 Accuracy: 0.80349\n",
      "Epoch: 580 Batch:   0 Loss: 0.12351 Accuracy: 0.96875\n",
      "Epoch: 581 Batch:   0 Loss: 0.12346 Accuracy: 0.96875\n",
      "Epoch: 582 Batch:   0 Loss: 0.12340 Accuracy: 0.96875\n",
      "Epoch: 583 Batch:   0 Loss: 0.12334 Accuracy: 0.96875\n",
      "Epoch: 584 Batch:   0 Loss: 0.12329 Accuracy: 0.96875\n",
      "Epoch: 585 Batch:   0 Loss: 0.12323 Accuracy: 0.96875\n",
      "Epoch: 586 Batch:   0 Loss: 0.12318 Accuracy: 0.96875\n",
      "Epoch: 587 Batch:   0 Loss: 0.12312 Accuracy: 0.96875\n",
      "Epoch: 588 Batch:   0 Loss: 0.12307 Accuracy: 0.96875\n",
      "Epoch: 589 Batch:   0 Loss: 0.12301 Accuracy: 0.96875\n",
      "Test Loss: 0.52098 Accuracy: 0.80315\n",
      "Epoch: 590 Batch:   0 Loss: 0.12295 Accuracy: 0.96875\n",
      "Epoch: 591 Batch:   0 Loss: 0.12290 Accuracy: 0.96875\n",
      "Epoch: 592 Batch:   0 Loss: 0.12284 Accuracy: 0.96875\n",
      "Epoch: 593 Batch:   0 Loss: 0.12279 Accuracy: 0.96875\n",
      "Epoch: 594 Batch:   0 Loss: 0.12273 Accuracy: 0.96875\n",
      "Epoch: 595 Batch:   0 Loss: 0.12268 Accuracy: 0.96875\n",
      "Epoch: 596 Batch:   0 Loss: 0.12263 Accuracy: 0.96875\n",
      "Epoch: 597 Batch:   0 Loss: 0.12257 Accuracy: 0.96875\n",
      "Epoch: 598 Batch:   0 Loss: 0.12252 Accuracy: 0.96875\n",
      "Epoch: 599 Batch:   0 Loss: 0.12246 Accuracy: 0.96875\n",
      "Test Loss: 0.52156 Accuracy: 0.80375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup input and train protoNN\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, dataDimension], name='X')\n",
    "Y = tf.placeholder(tf.float32, [None, numClasses], name='Y')\n",
    "protoNN = ProtoNN(dataDimension, PROJECTION_DIM,\n",
    "                  NUM_PROTOTYPES, numClasses,\n",
    "                  gamma, W=W, B=B)\n",
    "trainer = ProtoNNTrainer(protoNN, REG_W, REG_B, REG_Z,\n",
    "                         SPAR_W, SPAR_B, SPAR_Z,\n",
    "                         LEARNING_RATE, X, Y, lossType='xentropy')\n",
    "sess = tf.Session()\n",
    "\n",
    "trainer.train(BATCH_SIZE, NUM_EPOCHS, sess, x_train, x_test, y_train, y_test,\n",
    "              printStep=600, valStep=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uypXmz1QJ2h"
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-15T13:07:22.671507Z",
     "start_time": "2018-08-15T13:07:22.645050Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 96103,
     "status": "ok",
     "timestamp": 1567154584845,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "VYxefAD3QJ2j",
    "outputId": "f8b711a1-bae4-4bf5-9a62-935838844601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy 0.80373436\n",
      "Model size constraint (Bytes):  9580\n",
      "Number of non-zeros:  2395\n"
     ]
    }
   ],
   "source": [
    "acc = sess.run(protoNN.accuracy, feed_dict={X: x_test, Y: y_test})\n",
    "pred = sess.run(protoNN.predictions, feed_dict={X: x_test, Y: y_test})\n",
    "# W, B, Z are tensorflow graph nodes\n",
    "W, B, Z, _ = protoNN.getModelMatrices()\n",
    "matrixList = sess.run([W, B, Z])\n",
    "sparcityList = [SPAR_W, SPAR_B, SPAR_Z]                       \n",
    "nnz, size, sparse = getModelSize(matrixList, sparcityList)\n",
    "print(\"Final test accuracy\", acc)\n",
    "print(\"Model size constraint (Bytes): \", size)\n",
    "print(\"Number of non-zeros: \", nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91542,
     "status": "ok",
     "timestamp": 1567154584848,
     "user": {
      "displayName": "Gokul Hari",
      "photoUrl": "",
      "userId": "16159457985484250305"
     },
     "user_tz": -330
    },
    "id": "yJM9puhcQJ2t",
    "outputId": "d8e743e0-1068-4681-e3bc-3b650ab66a3c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4483 1435]\n",
      " [ 888 5030]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.83467   0.75752   0.79422      5918\n",
      "           1    0.77804   0.84995   0.81240      5918\n",
      "\n",
      "    accuracy                        0.80373     11836\n",
      "   macro avg    0.80635   0.80373   0.80331     11836\n",
      "weighted avg    0.80635   0.80373   0.80331     11836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "y_test = np.argmax(y_test,axis=1)\n",
    "print (confusion_matrix(y_test,pred))\n",
    "print (classification_report(y_test,pred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8499493071983778"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitivity = confusion_matrix(y_test,pred)[1][1]/(confusion_matrix(y_test,pred)[1][1] + confusion_matrix(y_test,pred)[1][0])\n",
    "sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7575194322406218"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specificity = confusion_matrix(y_test,pred)[0][0]/(confusion_matrix(y_test,pred)[0][0] + confusion_matrix(y_test,pred)[0][1])\n",
    "specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = (TP + TN) / (TP + TN + FP + FN) \n",
    "# Precision = TP / (TP + FP)\n",
    "# Recall = TP / (TP + FN)\n",
    "# sensitivity = TP / (TP + FN) \n",
    "# specificity = TN / (TN + FP) \n",
    "# positive predictive value (PPV) = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "protoNN_example.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ProtoNN",
   "language": "python",
   "name": "protonn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
